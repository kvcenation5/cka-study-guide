{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Kubernetes CKA Study Notes","text":"<p>Welcome to my personal study notes for the Certified Kubernetes Administrator (CKA) exam.</p>"},{"location":"#documentation-sections","title":"\ud83d\udcc2 Documentation Sections","text":""},{"location":"#fundamentals","title":"\ud83e\udde0 Fundamentals","text":"<ul> <li>Labels &amp; Selectors</li> <li>Annotations</li> <li>ReplicaSets vs. Deployments</li> <li>Service Types</li> <li>Pod Scheduling<ul> <li>Taints and Tolerations</li> </ul> </li> </ul>"},{"location":"#cluster-architecture","title":"\ud83c\udfd7\ufe0f Cluster Architecture","text":"<ul> <li>Component Management</li> <li>ETCD Deep Dive</li> <li>Kube-Proxy</li> </ul>"},{"location":"#setup-guides","title":"\ud83d\udee0\ufe0f Setup Guides","text":"<ul> <li>kubeadm on Mac (Multipass)</li> <li>Kubernetes on AWS (kOps)</li> </ul>"},{"location":"#exam-overview","title":"\ud83c\udf93 Exam Overview","text":"<ul> <li>CKA Curriculum &amp; Weightage (2025)</li> </ul>"},{"location":"#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":"<ul> <li>Troubleshooting Flowchart</li> <li>Summary of Issues</li> </ul> <p>[!TIP] Use the sidebar or the links above to navigate through different sections of the documentation.</p>"},{"location":"BETTER_ARCHITECTURE/","title":"Network Policy Architecture","text":"<p>Here is a clear breakdown of the Network Policy set up in <code>05-networkpolicy</code>.</p> <pre><code>graph TD\n    %% Nodes\n    subgraph Cluster[\"Kubernetes Cluster\"]\n        direction TB\n        subgraph NS[\"Namespace: default\"]\n\n            %% Policy Definition\n            NP(\"\ud83d\udee1\ufe0f NetworkPolicy: redis-network-policy\")\n\n            %% Pods\n            TargetPod(\"\ud83d\udd34 Redis Pod (Target)&lt;br/&gt;[app: redis]\")\n            BlockedPod(\"\u26d4 Nginx/Hack (Blocked)&lt;br/&gt;[app: nginx]\")\n            AllowedPod(\"\u2705 Client Pod (Allowed)&lt;br/&gt;[role: known-redis-member]\")\n\n            %% Data Flow\n            BlockedPod -- \"\u274c TCP 6379 (Blocked)\" --&gt; TargetPod\n            AllowedPod -- \"\u2705 TCP 6379 (Allowed)\" --&gt; TargetPod\n\n        end\n    end\n\n    %% Styles\n    classDef target fill:#ffdede,stroke:#ff0000,stroke-width:2px,color:black;\n    classDef blocked fill:#f0f0f0,stroke:#666666,stroke-width:2px,color:#666666,stroke-dasharray: 5 5;\n    classDef allowed fill:#e6fffa,stroke:#00b894,stroke-width:2px,color:black;\n    classDef policy fill:#fff,stroke:#333,stroke-width:1px,stroke-dasharray: 5 5,color:black;\n\n    class TargetPod target;\n    class BlockedPod blocked;\n    class AllowedPod allowed;\n    class NP policy;\n\n    %% Connections\n    NP -.- TargetPod\n    linkStyle 0 stroke:red,stroke-width:2px;\n    linkStyle 1 stroke:green,stroke-width:2px;\n</code></pre>"},{"location":"BETTER_ARCHITECTURE/#breakdown","title":"Breakdown","text":"<ul> <li>Target: The Policy protects pods showing <code>app: redis</code>.</li> <li>Rule: It only accepts Ingress traffic on TCP port 6379.</li> <li>Source: Traffic is only allowed from pods labeled <code>role: known-redis-member</code>.</li> <li>Result: Even if the Nginx pod is in the same namespace, it cannot access Redis because it lacks the specific role label.</li> </ul>"},{"location":"KNOW_YOUR_COMPONENTS/","title":"How to Identify Kubernetes Components","text":"<p>(Is it a DaemonSet, Deployment, or Service?)</p>"},{"location":"KNOW_YOUR_COMPONENTS/#1-the-magic-command","title":"1. The \"Magic\" Command","text":"<p>The easiest way to see everything and its type is to run:</p> <pre><code>kubectl get all -n kube-system\n</code></pre> <p>Look at the PREFIX of the output lines: - <code>pod/coredns-...</code> \u2192 Pod (Actual container running) - <code>service/kube-dns</code> \u2192 Service (Network endpoint/IP) - <code>daemonset.apps/kube-proxy</code> \u2192 DaemonSet - <code>deployment.apps/coredns</code> \u2192 Deployment</p>"},{"location":"KNOW_YOUR_COMPONENTS/#2-common-system-components-cheat-sheet","title":"2. Common System Components (Cheat Sheet)","text":""},{"location":"KNOW_YOUR_COMPONENTS/#daemonsets-runs-on-every-node","title":"\u2705 DaemonSets (Runs on EVERY Node)","text":"<p>Agents that act as the \"glue\" for the cluster. - <code>kube-proxy</code>: Manages network rules on every node. - CNI Plugins (e.g., <code>calico-node</code>, <code>flannel</code>, <code>aws-node</code>): provides Pod networking. - Log Collectors (e.g., <code>fluentd</code>, <code>filebeat</code>): Scrapes logs from every node.</p>"},{"location":"KNOW_YOUR_COMPONENTS/#deployments-runs-anywhere","title":"\u2705 Deployments (Runs ANYWHERE)","text":"<p>Scalable applications that don't need to be on every single machine. - <code>coredns</code>: DNS Server (usually 2 replicas for redundancy). - <code>metrics-server</code>: Aggregates CPU/Memory stats. - <code>kubernetes-dashboard</code>: Valid Web UI.</p>"},{"location":"KNOW_YOUR_COMPONENTS/#static-pods-control-plane-only","title":"\u2705 Static Pods (Control Plane Only)","text":"<p>These run directly on the Master Node manifest files. They often don't have a Deployment or DaemonSet controlling them. - <code>kube-apiserver</code>: The Brain. - <code>etcd</code>: The Database. - <code>kube-scheduler</code>: The Decision Maker. - <code>kube-controller-manager</code>: The Loop Master.</p>"},{"location":"KNOW_YOUR_COMPONENTS/#3-heuristic-rule-of-thumb","title":"3. Heuristic / Rule of Thumb","text":"Question to Ask It probably is a... \"Does this provide a steady IP address?\" Service \"Does this need to handle networking/logs on hardware?\" DaemonSet \"Is this a standard app (web server, dns)?\" Deployment \"Does this hold unique data (database)?\" StatefulSet"},{"location":"KNOW_YOUR_COMPONENTS/#4-default-cluster-inventory-what-to-expect-in-a-fresh-cluster","title":"4. Default Cluster Inventory (What to expect in a fresh cluster)","text":"<p>When you run <code>kubectl get all -n kube-system</code> on a brand new cluster (e.g., Kubeadm, Minikube, or EKS), here is the standard checklist of what you will see.</p>"},{"location":"KNOW_YOUR_COMPONENTS/#the-control-plane-the-brains","title":"\ud83e\udde0 The Control Plane (The Brains)","text":"<p>Usually run as Static Pods on the Master Node(s). You won't see Deployments for these in managed clouds (EKS/GKE).</p> <ol> <li><code>etcd</code> (Database)<ul> <li>What is it? The single source of truth. Stores all YAMLs, secrets, and cluster state.</li> <li>Type: Static Pod.</li> </ul> </li> <li><code>kube-apiserver</code> (Front Door)<ul> <li>What is it? The only component you talk to (via <code>kubectl</code>). It validates valid requests and updates <code>etcd</code>.</li> <li>Type: Static Pod.</li> </ul> </li> <li><code>kube-controller-manager</code> (Automation)<ul> <li>What is it? A loop that fixes things. If a pod dies, this process notices and creates a new one.</li> <li>Type: Static Pod.</li> </ul> </li> <li><code>kube-scheduler</code> (Placement)<ul> <li>What is it? Decides which node a new pod should go to (based on CPU, RAM, Taints).</li> <li>Type: Static Pod.</li> </ul> </li> </ol>"},{"location":"KNOW_YOUR_COMPONENTS/#networking-the-plumbing","title":"\ud83d\udd0c Networking (The Plumbing)","text":"<p>These make sure Pod A can talk to Pod B.</p> <ol> <li><code>coredns</code> (Phonebook)<ul> <li>What is it? Allows you to reach services by name (e.g., <code>db-service</code>) instead of IP (<code>10.96.0.1</code>).</li> <li>Type: Deployment (usually 2 replicas).</li> <li>Service: <code>kube-dns</code> (ClusterIP).</li> </ul> </li> <li><code>kube-proxy</code> (Traffic Cop)<ul> <li>What is it? Maintains network rules (iptables/IPVS) on nodes to route traffic.</li> <li>Type: DaemonSet (Must run on every node).</li> </ul> </li> <li>CNI Plugin (The Cables - e.g., Flannel, Calico, Weave, AWS-VPC-CNI)<ul> <li>What is it? Assigns IP addresses to Pods. Without this, nodes are \"NotReady\".</li> <li>Type: DaemonSet (Must run on every node).</li> </ul> </li> </ol>"},{"location":"KNOW_YOUR_COMPONENTS/#optional-but-common","title":"\ud83d\udee1\ufe0f Optional but Common","text":"<ol> <li><code>metrics-server</code> (Stats)<ul> <li>What is it? Enables <code>kubectl top nodes</code> and Horizontal Pod Autoscaling.</li> <li>Type: Deployment.</li> </ul> </li> </ol>"},{"location":"SUMMARY/","title":"Kubernetes Troubleshooting Zero to Hero - Complete Summary","text":""},{"location":"SUMMARY/#01-imagepullbackoff","title":"01-ImagePullBackOff","text":"<p>Problem: Container image cannot be pulled from registry</p> <p>Common Causes: - Invalid Image Name   - Typo in image name or tag   - Non-existent image   - Wrong registry path - Private Registry Without Credentials   - Missing imagePullSecret for private Docker Hub, ECR, or other registries   - Expired or incorrect credentials</p> <p>Solution: - Verify image name and tag are correct - Create docker-registry secret for private images:   <pre><code>kubectl create secret docker-registry &lt;name&gt; \\\n  --docker-server=&lt;registry&gt; \\\n  --docker-username=&lt;user&gt; \\\n  --docker-password=&lt;password&gt;\n</code></pre> - Reference secret in pod spec using <code>imagePullSecrets</code></p> <p>Diagnosis: <code>kubectl describe pod &lt;name&gt;</code> - check Events section</p>"},{"location":"SUMMARY/#02-crashloopbackoff","title":"02-CrashLoopBackOff","text":"<p>Problem: Container starts, crashes, and enters restart loop</p> <p>Common Causes: - Wrong Command/Misconfiguration   - Invalid command-line arguments   - Missing environment variables   - Wrong config file paths - Liveness Probe Failures   - Probe checking non-existent files or endpoints   - initialDelaySeconds too short (app not ready yet)   - Wrong URL/port in HTTP probes - Out of Memory (OOMKilled)   - Memory limits set too low for application needs   - Exit code 137 indicates OOMKilled - Application Bugs   - Unhandled exceptions   - Segmentation faults   - Code errors causing immediate exit</p> <p>Solution: - Check logs: <code>kubectl logs &lt;pod&gt; --previous</code> - Fix liveness probe configuration (increase initialDelaySeconds, fix paths) - Increase memory limits in resource specifications - Debug application code for bugs</p> <p>Diagnosis: - <code>kubectl describe pod &lt;name&gt;</code> - check restart count and exit codes - <code>kubectl logs &lt;pod&gt; --previous</code> - view logs from crashed container</p>"},{"location":"SUMMARY/#03-pods-not-schedulable","title":"03-Pods-Not-Schedulable","text":"<p>Problem: Pods remain in Pending state, unable to be scheduled to nodes</p> <p>Common Causes: - Node Selector Mismatch   - Pod requires node labels that don't exist   - Simple key-value label matching - Node Affinity Rules   - Required affinity rules cannot be satisfied   - More expressive than nodeSelector with operators (In, NotIn, Exists)   - Two types: requiredDuringScheduling and preferredDuringScheduling - Taints on Nodes   - Nodes tainted to repel pods   - Pod lacks matching tolerations   - Effects: NoSchedule, PreferNoSchedule, NoExecute - Resource Constraints   - Insufficient CPU/memory available on nodes   - Pod resource requests exceed node capacity</p> <p>Solution: - NodeSelector: Add required labels to nodes or fix pod nodeSelector   <pre><code>nodeSelector:\n  disktype: ssd\n</code></pre> - Node Affinity: Use for complex scheduling rules   <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: disktype\n          operator: In\n          values: [ssd]\n</code></pre> - Tolerations: Add to pod spec to tolerate node taints   <pre><code>tolerations:\n- key: disktype\n  operator: Equal\n  value: ssd\n  effect: NoSchedule\n</code></pre></p> <p>Diagnosis: - <code>kubectl describe pod &lt;name&gt;</code> - check Events for scheduling failures - <code>kubectl get nodes --show-labels</code> - verify node labels - <code>kubectl describe node &lt;name&gt;</code> - check taints and available resources</p>"},{"location":"SUMMARY/#04-statefulset-pv","title":"04-StatefulSet-PV","text":"<p>Problem: StatefulSet pods failing due to Persistent Volume issues</p> <p>Key Concepts: - StatefulSets require stable, persistent storage - volumeClaimTemplates automatically create PVCs for each pod replica - Each pod gets its own unique PVC (e.g., www-web-0, www-web-1, www-web-2) - PVCs persist even if pods are deleted</p> <p>Common Issues: - No Available PersistentVolumes   - PVC remains in Pending state   - No PV matches PVC requirements (size, access mode, storage class) - Storage Class Not Found   - Referenced storageClassName doesn't exist   - No default storage class configured - Access Mode Mismatch   - PV has ReadWriteOnce but PVC requests ReadWriteMany - Volume Still Attached   - Previous pod termination didn't cleanly detach volume   - Pod stuck in terminating state</p> <p>Solution: - Create PersistentVolumes with matching specifications - Configure default StorageClass or specify storageClassName - Ensure access modes match between PV and PVC - Force delete stuck pods if necessary: <code>kubectl delete pod &lt;name&gt; --grace-period=0 --force</code></p> <p>Diagnosis: - <code>kubectl get pvc</code> - check PVC status - <code>kubectl describe pvc &lt;name&gt;</code> - see binding issues - <code>kubectl get pv</code> - check available PersistentVolumes - <code>kubectl get storageclass</code> - verify storage classes exist</p>"},{"location":"SUMMARY/#05-networkpolicy","title":"05-NetworkPolicy","text":"<p>Problem: Pod network connectivity issues due to NetworkPolicy restrictions</p> <p>Key Concepts: - NetworkPolicies act as firewall rules for pods - By default, pods accept traffic from any source - Once a NetworkPolicy selects a pod, it becomes isolated - Must explicitly allow desired traffic (whitelist approach)</p> <p>Common Issues: - Pod Cannot Receive Traffic   - NetworkPolicy blocks ingress but doesn't allow required sources   - Missing podSelector or namespaceSelector in ingress rules - Pod Cannot Send Traffic   - Egress rules too restrictive   - DNS traffic blocked (needs port 53 UDP/TCP allowed) - Wrong Label Selectors   - podSelector doesn't match intended pods   - Source pods lack required labels in ingress rules</p> <p>Solution: - Allow Ingress Traffic: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-specific-pods\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: backend\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> - Allow Egress (including DNS): <pre><code>policyTypes:\n- Egress\negress:\n- to:\n  - namespaceSelector:\n      matchLabels:\n        name: kube-system\n  ports:\n  - protocol: UDP\n    port: 53\n</code></pre></p> <p>Diagnosis: - <code>kubectl get networkpolicy</code> - list policies in namespace - <code>kubectl describe networkpolicy &lt;name&gt;</code> - see policy rules - <code>kubectl get pods --show-labels</code> - verify pod labels match selectors - Test connectivity: <code>kubectl exec &lt;pod&gt; -- curl &lt;target&gt;</code> - Check if NetworkPolicy controller is running (Calico, Cilium, etc.)</p>"},{"location":"SUMMARY/#quick-diagnosis-checklist","title":"Quick Diagnosis Checklist","text":"<ol> <li>Pod Status: <code>kubectl get pods -o wide</code></li> <li>Detailed Info: <code>kubectl describe pod &lt;name&gt;</code></li> <li>Logs: <code>kubectl logs &lt;name&gt; [--previous]</code></li> <li>Events: <code>kubectl get events --sort-by='.lastTimestamp'</code></li> <li>Node Info: <code>kubectl describe node &lt;name&gt;</code></li> <li>Resource Usage: <code>kubectl top pods</code> / <code>kubectl top nodes</code></li> </ol>"},{"location":"SUMMARY/#real-world-troubleshooting-examples-statefulset-pv","title":"Real-World Troubleshooting Examples: StatefulSet &amp; PV","text":"<p>Here are 3 common scenarios where storage fails in production.</p>"},{"location":"SUMMARY/#1-the-wrong-zone-trap-cloudawsgcp","title":"1. The \"Wrong Zone\" Trap (Cloud/AWS/GCP)","text":"<p>Scenario: You request a 100GB EBS volume. The Pod is scheduled in <code>us-east-1a</code>, but the Volume is created in <code>us-east-1b</code>. Symptoms: Pod stays in <code>ContainerCreating</code> forever. Events: <code>FailedAttachVolume: Volume vol-123 is in us-east-1b, but node is in us-east-1a</code>. Fix: - Use StorageClasses with <code>volumeBindingMode: WaitForFirstConsumer</code>. This forces Kubernetes to wait until the Pod is scheduled (node selected) before creating the physical disk.</p>"},{"location":"SUMMARY/#2-the-readwriteonce-lockout","title":"2. The \"ReadWriteOnce\" Lockout","text":"<p>Scenario: You have a Deployment with <code>replicas: 2</code> trying to share a single PVC (like a shared filestore). Symptoms: Pod #1 starts fine. Pod #2 stays in <code>ContainerCreating</code>. Events: <code>Multi-Attach error for volume \"pvc-x\": Volume is already exclusively attached to one node and cannot be attached to another</code>. Fix: - Short term: Change <code>accessModes</code> to <code>ReadWriteMany</code> (requires NFS/EFS, standard block storage doesn't support this). - Long term: Use a StatefulSet so each pod gets its own unique volume.</p>"},{"location":"SUMMARY/#3-the-ghost-volume-stuck-terminating","title":"3. The \"Ghost Volume\" (Stuck Terminating)","text":"<p>Scenario: You delete a pod, but it gets stuck in <code>Terminating</code>. You check the node and see the volume is still mounted by a zombie process. Symptoms: New pod cannot start because the volume is \"in use\". Fix: - Force Delete Pod: <code>kubectl delete pod &lt;pod-name&gt; --grace-period=0 --force</code> - Manual Cleanup: Log into the node and manually unmount the path (<code>umount /var/lib/kubelet/...</code>).</p>"},{"location":"SUMMARY/#real-world-troubleshooting-examples-networkpolicy","title":"Real-World Troubleshooting Examples: NetworkPolicy","text":"<p>Here are 3 scenarios where invisible firewall rules break apps.</p>"},{"location":"SUMMARY/#1-the-total-silence-implicit-default-deny","title":"1. The \"Total Silence\" (Implicit Default Deny)","text":"<p>Scenario: You add a NetworkPolicy to secure your Database. Suddenly, the API cannot talk to anything (DNS fails, external APIs fail). Why? If you create a policy with <code>policyTypes: [Egress]</code> but don't list any rules, it defaults to Blocking Everything. Fix: Always allow essential system traffic (like DNS): <pre><code>egress:\n- to:\n  - namespaceSelector:\n      matchLabels:\n        kubernetes.io/metadata.name: kube-system\n  ports:\n  - port: 53\n    protocol: UDP\n</code></pre></p>"},{"location":"SUMMARY/#2-the-missing-label-typos","title":"2. The \"Missing Label\" (Typos)","text":"<p>Scenario: You create a rule allowing traffic from <code>role: api</code>. You label your pods <code>app: api</code>. Symptoms: Connection Timeout. No logs in the app (traffic never reaches it). Fix: - Double-check labels: <code>kubectl get pods --show-labels</code>. - Remember: <code>podSelector</code> is an exact match. It does not warn you if 0 pods match.</p>"},{"location":"SUMMARY/#3-the-one-way-street-ingress-vs-egress","title":"3. The \"One-Way Street\" (Ingress vs Egress)","text":"<p>Scenario: You allow the API to talk to Redis (Egress Allow). But Redis still blocks the connection. Why? NetworkPolicies are one-way. Allowing the API to send (<code>Egress</code>) doesn't automatically mean Redis allows the receipt (<code>Ingress</code>). Fix: You need two policies (or a matching rule on both sides): 1. API Pod: Needs <code>Egress</code> allows -&gt; Redis. 2. Redis Pod: Needs <code>Ingress</code> allows &lt;- API.</p>"},{"location":"coredns-guide/","title":"CoreDNS &amp; Kubernetes Networking: Complete Guide with Real-World Examples","text":""},{"location":"coredns-guide/#what-is-coredns","title":"What is CoreDNS?","text":"<p>CoreDNS is the DNS server that runs inside your Kubernetes cluster. It acts as a \"phone book\" that translates service names into IP addresses so pods can find each other.</p> <p>Real-world analogy: Instead of remembering that your database is at <code>10.244.5.23:5432</code>, you just call it <code>postgres-service</code> and CoreDNS handles the translation.</p>"},{"location":"coredns-guide/#how-coredns-works","title":"How CoreDNS Works","text":""},{"location":"coredns-guide/#architecture-overview","title":"Architecture Overview","text":"<pre><code>Pod wants to connect to \"my-api-service\"\n         \u2193\nPod queries CoreDNS at 10.96.0.10:53\n         \u2193\nCoreDNS checks Kubernetes API for service info\n         \u2193\nReturns IP: 10.96.5.100\n         \u2193\nPod connects to 10.96.5.100\n         \u2193\nkube-proxy routes to actual pod at 10.244.2.15\n</code></pre>"},{"location":"coredns-guide/#key-components","title":"Key Components","text":"Component Purpose Port CoreDNS Deployment Runs 2+ DNS server pods for HA - kube-dns Service ClusterIP that pods use for DNS 53 CoreDNS ConfigMap Configuration (Corefile) - ServiceAccount/RBAC Permissions to read Services/Endpoints -"},{"location":"coredns-guide/#coredns-configuration-corefile","title":"CoreDNS Configuration (Corefile)","text":"<pre><code># Handles internal cluster domains\nkubernetes cluster.local in-addr.arpa ip6.arpa {\n   pods insecure              # Allow pod DNS records\n   fallthrough in-addr.arpa   # Pass reverse lookups through\n   ttl 30                     # Cache for 30 seconds\n}\n\n# Forward external queries\nforward . /etc/resolv.conf    # Use node's DNS for google.com, etc.\n\n# Other plugins\nprometheus :9153              # Metrics endpoint\nready :8181                   # Health check endpoint\ncache 30                      # Cache responses for 30s\nloop                          # Detect DNS loops\nreload                        # Auto-reload on config changes\nloadbalance                   # Round-robin between endpoints\n</code></pre>"},{"location":"coredns-guide/#common-coredns-errors-real-world-scenarios","title":"Common CoreDNS Errors &amp; Real-World Scenarios","text":""},{"location":"coredns-guide/#error-1-dns-loop-detection","title":"Error 1: DNS Loop Detection","text":""},{"location":"coredns-guide/#what-you-see","title":"What You See:","text":"<pre><code>$ kubectl get pods -n kube-system\nNAME                      READY   STATUS             RESTARTS   AGE\ncoredns-xxx               0/1     CrashLoopBackOff   5          3m\n\n$ kubectl logs coredns-xxx -n kube-system\n[FATAL] plugin/loop: Loop (127.0.0.1:56162 -&gt; :53) detected for zone \".\"\nQuery: \"HINFO 7087784449798295848.7359092265978106814.\"\n</code></pre>"},{"location":"coredns-guide/#real-world-scenario","title":"Real-World Scenario:","text":"<p>Company: E-commerce startup deploying on Ubuntu 22.04 servers</p> <p>What happened: - DevOps engineer set up a new Kubernetes cluster - Used Ubuntu's default DNS setup (systemd-resolved) - Node's <code>/etc/resolv.conf</code> pointed to <code>127.0.0.53</code> (localhost) - CoreDNS forwarded external queries to <code>127.0.0.53</code> - systemd-resolved forwarded back to CoreDNS \u2192 LOOP!</p>"},{"location":"coredns-guide/#root-cause","title":"Root Cause:","text":"<pre><code># On the node:\n$ cat /etc/resolv.conf\nnameserver 127.0.0.53  # \u2190 Problem: localhost reference\noptions edns0 trust-ad\nsearch .\n</code></pre> <p>CoreDNS uses this file to forward queries for <code>google.com</code>, but <code>127.0.0.53</code> creates a circular reference.</p>"},{"location":"coredns-guide/#the-fix","title":"The Fix:","text":"<pre><code># Edit kubelet config on each node\nsudo vi /var/lib/kubelet/config.yaml\n\n# Add this line:\nresolvConf: /run/systemd/resolve/resolv.conf\n\n# Restart kubelet\nsudo systemctl restart kubelet\n\n# Restart CoreDNS\nkubectl rollout restart deployment coredns -n kube-system\n</code></pre>"},{"location":"coredns-guide/#alternative-quick-fix-for-testing","title":"Alternative Quick Fix (for testing):","text":"<pre><code>kubectl edit configmap coredns -n kube-system\n\n# Change:\nforward . /etc/resolv.conf\n# To:\nforward . 8.8.8.8 1.1.1.1\n</code></pre> <p>Impact: All pods lost DNS resolution for 15 minutes during troubleshooting. API calls to external services failed.</p>"},{"location":"coredns-guide/#error-2-cannot-reach-kubernetes-api","title":"Error 2: Cannot Reach Kubernetes API","text":""},{"location":"coredns-guide/#what-you-see_1","title":"What You See:","text":"<pre><code>$ kubectl logs coredns-xxx -n kube-system\n[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server\n[INFO] plugin/ready: Still waiting on: \"kubernetes\"\n[ERROR] plugin/kubernetes: Unhandled Error\nfailed to list *v1.Service: Get \"https://10.96.0.1:443/api/v1/services\": \ndial tcp 10.96.0.1:443: i/o timeout\n</code></pre>"},{"location":"coredns-guide/#real-world-scenario_1","title":"Real-World Scenario:","text":"<p>Company: FinTech company migrating from EKS to bare-metal Kubernetes</p> <p>What happened: - Infrastructure team installed Kubernetes with <code>kubeadm</code> - Forgot to install a CNI plugin (Calico/Flannel) - Pods had IPs but couldn't communicate across nodes - CoreDNS couldn't reach the API server's Service IP</p>"},{"location":"coredns-guide/#root-cause_1","title":"Root Cause:","text":"<pre><code># No CNI plugin installed\n$ kubectl get pods -n kube-system | grep -E 'calico|flannel|weave'\n# (nothing shows up)\n\n$ kubectl get nodes\nNAME     STATUS     ROLES           AGE   VERSION\nnode-1   NotReady   control-plane   10m   v1.28.0\nnode-2   NotReady   &lt;none&gt;          10m   v1.28.0\n</code></pre> <p>Nodes show <code>NotReady</code> because there's no network overlay.</p>"},{"location":"coredns-guide/#the-fix_1","title":"The Fix:","text":"<pre><code># Install Calico\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml\n\n# Wait for CNI pods to start\nkubectl get pods -n kube-system -w\n\n# Nodes become Ready\n$ kubectl get nodes\nNAME     STATUS   ROLES           AGE   VERSION\nnode-1   Ready    control-plane   15m   v1.28.0\nnode-2   Ready    &lt;none&gt;          15m   v1.28.0\n</code></pre> <p>Impact: Entire cluster was non-functional for 2 hours. No service-to-service communication worked.</p>"},{"location":"coredns-guide/#error-3-crashloopbackoff-selinux","title":"Error 3: CrashLoopBackOff (SELinux)","text":""},{"location":"coredns-guide/#what-you-see_2","title":"What You See:","text":"<pre><code>$ kubectl get pods -n kube-system\nNAME                      READY   STATUS             RESTARTS   AGE\ncoredns-xxx               0/1     CrashLoopBackOff   8          10m\n\n$ kubectl logs coredns-xxx -n kube-system\nError: open /etc/coredns/Corefile: permission denied\n</code></pre>"},{"location":"coredns-guide/#real-world-scenario_2","title":"Real-World Scenario:","text":"<p>Company: Government contractor on RHEL 8 with SELinux enforcing</p> <p>What happened: - Security requirements mandated SELinux in enforcing mode - CoreDNS couldn't read its config file due to SELinux policies - Running older Docker version that didn't handle SELinux labels properly</p>"},{"location":"coredns-guide/#root-cause_2","title":"Root Cause:","text":"<pre><code># Check SELinux status\n$ getenforce\nEnforcing\n\n# CoreDNS security context conflicts with SELinux\n$ kubectl get pod coredns-xxx -n kube-system -o yaml | grep -A5 securityContext\nsecurityContext:\n  allowPrivilegeEscalation: false  # \u2190 Too restrictive for SELinux\n  capabilities:\n    add:\n    - NET_BIND_SERVICE\n</code></pre>"},{"location":"coredns-guide/#the-fix_2","title":"The Fix:","text":"<pre><code># Option 1: Allow privilege escalation\nkubectl -n kube-system get deployment coredns -o yaml | \\\n  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\\n  kubectl apply -f -\n\n# Option 2: Update Docker (recommended)\nsudo yum update docker-ce\nsudo systemctl restart docker kubelet\n</code></pre> <p>Impact: DNS outage during business hours. Incident report filed with customer.</p>"},{"location":"coredns-guide/#error-4-no-endpoints-for-kube-dns-service","title":"Error 4: No Endpoints for kube-dns Service","text":""},{"location":"coredns-guide/#what-you-see_3","title":"What You See:","text":"<pre><code>$ kubectl get svc kube-dns -n kube-system\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE\nkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP   5d\n\n$ kubectl get endpoints kube-dns -n kube-system\nNAME       ENDPOINTS   AGE\nkube-dns   &lt;none&gt;      5d  # \u2190 No endpoints!\n\n# DNS queries fail\n$ kubectl run test --image=busybox --rm -it -- nslookup kubernetes.default\n;; connection timed out; no servers could be reached\n</code></pre>"},{"location":"coredns-guide/#real-world-scenario_3","title":"Real-World Scenario:","text":"<p>Company: SaaS platform doing blue-green deployment</p> <p>What happened: - DevOps engineer modified CoreDNS deployment selectors during upgrade - Changed <code>k8s-app: kube-dns</code> to <code>k8s-app: coredns</code> - kube-dns Service still looked for old label - Service couldn't find any pods \u2192 no endpoints</p>"},{"location":"coredns-guide/#root-cause_3","title":"Root Cause:","text":"<pre><code># Check service selector\n$ kubectl get svc kube-dns -n kube-system -o yaml | grep -A2 selector\nselector:\n  k8s-app: kube-dns  # Looking for this label\n\n# Check actual pod labels\n$ kubectl get pods -n kube-system -l k8s-app=kube-dns\nNo resources found.  # \u2190 No pods match!\n\n$ kubectl get pods -n kube-system -l k8s-app=coredns\nNAME                      READY   STATUS    RESTARTS   AGE\ncoredns-xxx               1/1     Running   0          2m  # Found with different label\n</code></pre>"},{"location":"coredns-guide/#the-fix_3","title":"The Fix:","text":"<pre><code># Edit the CoreDNS deployment to restore correct labels\nkubectl edit deployment coredns -n kube-system\n\n# Ensure these labels exist:\nmetadata:\n  labels:\n    k8s-app: kube-dns  # \u2190 Critical!\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n\n# Verify endpoints appear\n$ kubectl get ep kube-dns -n kube-system\nNAME       ENDPOINTS                     AGE\nkube-dns   10.244.0.5:53,10.244.0.6:53   5d\n</code></pre> <p>Impact: 45-minute DNS outage affecting production traffic. Customer complaints about \"service unavailable\" errors.</p>"},{"location":"coredns-guide/#error-5-transient-startup-timeouts","title":"Error 5: Transient Startup Timeouts","text":""},{"location":"coredns-guide/#what-you-see_4","title":"What You See:","text":"<pre><code>$ kubectl logs coredns-xxx -n kube-system\n[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server\n[INFO] plugin/ready: Still waiting on: \"kubernetes\"\n[ERROR] plugin/kubernetes: dial tcp 10.96.0.1:443: i/o timeout\n[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API\n.:53\nCoreDNS-1.12.1\n[INFO] plugin/ready: Still waiting on: \"kubernetes\"\n# ... then errors stop and it works fine\n</code></pre>"},{"location":"coredns-guide/#real-world-scenario_4","title":"Real-World Scenario:","text":"<p>Company: Developer's minikube cluster</p> <p>What happened: - Minikube cluster restarted (after Docker Desktop update) - During pod startup, brief race condition:   1. CoreDNS pod starts   2. CNI hasn't finished configuring network routes   3. CoreDNS can't reach API for ~30 seconds   4. Network stabilizes, everything works</p>"},{"location":"coredns-guide/#root-cause_4","title":"Root Cause:","text":"<p>This is actually NORMAL startup behavior!</p> <pre><code># Check pod status - it's running fine now\n$ kubectl get pods -n kube-system\nNAME                      READY   STATUS    RESTARTS      AGE\ncoredns-xxx               1/1     Running   4 (19h ago)   83d\n                          \u2191 Running fine!   \u2191 Old restarts\n</code></pre> <p>The errors were logged during the restart event 19 hours ago, not currently happening.</p>"},{"location":"coredns-guide/#verification","title":"Verification:","text":"<pre><code># Check for RECENT errors (none = it's fine)\n$ kubectl logs coredns-xxx -n kube-system --since=1h | grep ERROR\n# (no output = no recent errors)\n\n# Test DNS works\n$ kubectl run test-dns --image=busybox:1.28 --rm -it -- nslookup kubernetes.default\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\nName:      kubernetes.default\nAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n# \u2705 Works perfectly!\n</code></pre>"},{"location":"coredns-guide/#when-to-worry","title":"When to Worry:","text":"<ul> <li>\u274c Errors persist for &gt; 2 minutes after pod start</li> <li>\u274c Pod never reaches <code>Running</code> state</li> <li>\u274c DNS queries actually fail</li> </ul>"},{"location":"coredns-guide/#when-not-to-worry","title":"When NOT to Worry:","text":"<ul> <li>\u2705 Errors only during pod startup (first 30-60 seconds)</li> <li>\u2705 Pod is <code>Running</code> with <code>1/1</code> ready</li> <li>\u2705 DNS queries work fine</li> </ul> <p>Impact: None! This is expected behavior. The logs just look scary.</p>"},{"location":"coredns-guide/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":""},{"location":"coredns-guide/#step-1-check-coredns-pod-status","title":"Step 1: Check CoreDNS Pod Status","text":"<pre><code>kubectl get pods -n kube-system -l k8s-app=kube-dns\n\n# Possible states:\n# Running (1/1)     \u2192 \u2705 Healthy\n# Pending           \u2192 CNI not installed\n# CrashLoopBackOff  \u2192 DNS loop or SELinux issue\n# ImagePullBackOff  \u2192 Registry problem\n</code></pre>"},{"location":"coredns-guide/#step-2-check-recent-logs","title":"Step 2: Check Recent Logs","text":"<pre><code># Don't look at ALL logs - only recent ones\nkubectl logs -n kube-system -l k8s-app=kube-dns --tail=50 --since=10m\n\n# Look for:\n# [FATAL] plugin/loop     \u2192 DNS loop\n# [ERROR] dial tcp timeout \u2192 API connectivity issue\n# permission denied       \u2192 SELinux/RBAC issue\n</code></pre>"},{"location":"coredns-guide/#step-3-test-dns-resolution","title":"Step 3: Test DNS Resolution","text":"<pre><code># Test internal DNS\nkubectl run test-dns --image=busybox:1.28 --rm -it -- nslookup kubernetes.default\n\n# Test external DNS\nkubectl run test-dns --image=busybox:1.28 --rm -it -- nslookup google.com\n\n# If internal works but external fails \u2192 check forward config\n# If both fail \u2192 CoreDNS is broken\n</code></pre>"},{"location":"coredns-guide/#step-4-check-service-endpoints","title":"Step 4: Check Service &amp; Endpoints","text":"<pre><code># Verify service exists\nkubectl get svc kube-dns -n kube-system\n\n# Check it has endpoints\nkubectl get ep kube-dns -n kube-system\n\n# Should show CoreDNS pod IPs:\n# ENDPOINTS: 10.244.0.5:53,10.244.0.6:53\n</code></pre>"},{"location":"coredns-guide/#step-5-verify-network-plugin","title":"Step 5: Verify Network Plugin","text":"<pre><code># Check CNI is running\nkubectl get pods -n kube-system | grep -E 'calico|flannel|weave|cilium'\n\n# Check nodes are Ready\nkubectl get nodes\n\n# On node, verify CNI config\nls /etc/cni/net.d/\n# Should have .conf or .conflist files\n</code></pre>"},{"location":"coredns-guide/#step-6-check-kube-proxy","title":"Step 6: Check kube-proxy","text":"<pre><code># CoreDNS needs kube-proxy to route traffic\nkubectl get pods -n kube-system | grep kube-proxy\n\n# Check logs for errors\nkubectl logs -n kube-system kube-proxy-xxx\n</code></pre>"},{"location":"coredns-guide/#prevention-best-practices","title":"Prevention Best Practices","text":""},{"location":"coredns-guide/#1-always-install-cni-first","title":"1. Always Install CNI First","text":"<pre><code># Correct order when building cluster:\nkubeadm init                    # 1. Initialize control plane\nkubectl apply -f calico.yaml    # 2. Install CNI immediately\nkubectl join...                 # 3. Then join nodes\n</code></pre>"},{"location":"coredns-guide/#2-configure-kubelet-before-deploying","title":"2. Configure kubelet Before Deploying","text":"<pre><code># Set this BEFORE first boot\n# /var/lib/kubelet/config.yaml\nresolvConf: /run/systemd/resolve/resolv.conf\n</code></pre>"},{"location":"coredns-guide/#3-monitor-coredns-health","title":"3. Monitor CoreDNS Health","text":"<pre><code># Prometheus metrics\ncurl http://&lt;coredns-pod-ip&gt;:9153/metrics\n\n# Key metrics:\n# coredns_dns_request_count_total\n# coredns_dns_request_duration_seconds\n# coredns_forward_request_count_total\n</code></pre>"},{"location":"coredns-guide/#4-set-resource-limits","title":"4. Set Resource Limits","text":"<pre><code># CoreDNS can OOM under heavy load\nresources:\n  requests:\n    memory: \"170Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"512Mi\"  # Increase for large clusters\n    cpu: \"500m\"\n</code></pre>"},{"location":"coredns-guide/#5-use-multiple-replicas","title":"5. Use Multiple Replicas","text":"<pre><code># For high availability\nkubectl scale deployment coredns -n kube-system --replicas=3\n</code></pre>"},{"location":"coredns-guide/#quick-reference-error-solution","title":"Quick Reference: Error \u2192 Solution","text":"Error Message Root Cause Fix <code>[FATAL] plugin/loop</code> DNS loop detected Configure kubelet <code>resolvConf</code> or use <code>forward . 8.8.8.8</code> <code>dial tcp timeout</code> Can't reach API Install CNI plugin <code>CrashLoopBackOff</code> SELinux or permissions Set <code>allowPrivilegeEscalation: true</code> <code>No endpoints</code> Label mismatch Fix deployment labels to match service selector <code>Waiting for Kubernetes API</code> (brief) Startup race condition Normal - wait 60 seconds <code>permission denied</code> RBAC or SELinux Check ServiceAccount and SELinux mode"},{"location":"coredns-guide/#real-world-lesson-always-check-timestamps","title":"Real-World Lesson: Always Check Timestamps!","text":"<p>The most important takeaway:</p> <pre><code># This looks scary:\n[ERROR] plugin/kubernetes: Unhandled Error\n\n# But check when it happened:\nRESTARTS: 4 (19h ago)\n         \u2191 These errors are from 19 hours ago!\n\n# Current status:\nREADY: 1/1, STATUS: Running\n       \u2191 It's working fine NOW\n</code></pre> <p>Key insight: Old errors in logs don't mean current problems. Always: 1. Check pod status first (<code>1/1 Running</code> = probably fine) 2. Use <code>--since=1h</code> to see only recent logs 3. Actually test DNS before assuming it's broken</p>"},{"location":"coredns-guide/#kube-proxy-overview","title":"kube-proxy Overview","text":""},{"location":"coredns-guide/#what-is-kube-proxy","title":"What is kube-proxy?","text":"<p>kube-proxy is a network proxy that runs on every node (as a DaemonSet). It maintains network rules that allow traffic to Services to be routed to the correct pods.</p>"},{"location":"coredns-guide/#how-it-works","title":"How it Works","text":"<pre><code>Client connects to Service IP (10.96.5.100:80)\n         \u2193\nkube-proxy intercepts using iptables/ipvs rules\n         \u2193\nForwards to actual pod (10.244.2.15:8080)\n         \u2193\nPod processes request\n</code></pre>"},{"location":"coredns-guide/#key-components_1","title":"Key Components","text":"<pre><code># kube-proxy runs with this config:\n/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf\n\n# Config includes:\n# - clusterCIDR: Pod IP range\n# - mode: iptables, ipvs, or userspace\n# - bindAddress: What IP to listen on\n# - kubeconfig: How to authenticate to API\n</code></pre>"},{"location":"coredns-guide/#common-kube-proxy-issues","title":"Common kube-proxy Issues","text":""},{"location":"coredns-guide/#problem-1-services-not-accessible","title":"Problem 1: Services Not Accessible","text":"<pre><code># Check if kube-proxy is running\nkubectl get pods -n kube-system | grep kube-proxy\n\n# Check logs\nkubectl logs -n kube-system kube-proxy-xxx\n\n# Verify it's listening\nkubectl exec -n kube-system kube-proxy-xxx -- netstat -plan | grep kube-proxy\n# Should see ports like 10249 (metrics) and 10256 (healthz)\n</code></pre>"},{"location":"coredns-guide/#problem-2-wrong-configmap","title":"Problem 2: Wrong ConfigMap","text":"<pre><code># Check the config\nkubectl get cm kube-proxy -n kube-system -o yaml\n\n# Verify mode matches your setup\nmode: \"iptables\"  # or \"ipvs\"\n</code></pre>"},{"location":"coredns-guide/#problem-3-iptables-rules-missing","title":"Problem 3: iptables Rules Missing","text":"<pre><code># On a node, check if rules exist\nsudo iptables-save | grep KUBE-SERVICES\n\n# Should see many rules like:\n# -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m tcp --dport 53 -j KUBE-SVC-XXX\n</code></pre>"},{"location":"coredns-guide/#summary","title":"Summary","text":"<p>CoreDNS is critical infrastructure that requires: - \u2705 Working CNI plugin - \u2705 Proper <code>/etc/resolv.conf</code> configuration - \u2705 Correct RBAC permissions - \u2705 Network connectivity to API server - \u2705 Matching service selectors and labels</p> <p>kube-proxy is essential for: - \u2705 Service IP to Pod IP translation - \u2705 Load balancing across pod replicas - \u2705 Network rule management</p> <p>When troubleshooting: 1. Don't panic at old logs 2. Check current pod status 3. Test actual DNS resolution 4. Verify network plugin is running 5. Check timestamps on errors</p> <p>Most \"errors\" are actually normal startup behavior - verify with actual DNS tests before declaring an emergency! \ud83c\udfaf</p>"},{"location":"etcd-deep-dive/","title":"ETCD Deep Dive &amp; Backup Guide","text":"<p>ETCD is the brain of your Kubernetes cluster. It is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p> <p>If you lose ETCD, you lose your cluster. That is why Backup &amp; Restore is a critical CKA competency.</p>"},{"location":"etcd-deep-dive/#1-exploring-etcd-the-hidden-database","title":"1. Exploring ETCD (The \"Hidden\" Database)","text":"<p>Kubernetes stores every resource (Pods, Services, Secrets) as a key-value pair in ETCD.</p>"},{"location":"etcd-deep-dive/#accessing-etcd","title":"Accessing ETCD","text":"<p>ETCD is secured by Mutual TLS (mTLS). You cannot just access it without the correct certificates.</p> <p>Common Certificate Locations (Kubeadm/Standard): - CA Cert: <code>/etc/kubernetes/pki/etcd/ca.crt</code> - Server Cert: <code>/etc/kubernetes/pki/etcd/server.crt</code> - Server Key: <code>/etc/kubernetes/pki/etcd/server.key</code></p> <p>Minikube Locations: - CA Cert: <code>/var/lib/minikube/certs/etcd/ca.crt</code> - Server Cert: <code>/var/lib/minikube/certs/etcd/server.crt</code> - Server Key: <code>/var/lib/minikube/certs/etcd/server.key</code></p>"},{"location":"etcd-deep-dive/#etcdctl-utility-versions-usage","title":"ETCDCTL Utility Versions &amp; Usage","text":"<p>The <code>etcdctl</code> CLI tool interacts with the ETCD Server using 2 API versions: Version 2 and Version 3.</p> <ul> <li>By default, it is often set to use Version 2.</li> <li>Each version has completely different commands.</li> </ul> <p>Version 2 Commands (Old/Default): *   <code>etcdctl backup</code> *   <code>etcdctl cluster-health</code> *   <code>etcdctl mk</code> *   <code>etcdctl mkdir</code> *   <code>etcdctl set</code></p> <p>Version 3 Commands (Used in Kubernetes): *   <code>etcdctl snapshot save</code> *   <code>etcdctl endpoint health</code> *   <code>etcdctl get</code> *   <code>etcdctl put</code></p> <p>How to set the right version: To use version 3 commands (which are required for Kubernetes/CKA), you MUST set the environment variable:</p> <pre><code>export ETCDCTL_API=3\n</code></pre> <p>Note: If the API version is not set, it defaults to version 2, and version 3 commands (like <code>snapshot save</code>) will fail. Conversely, if set to version 3, version 2 commands (like <code>mkdir</code>) will fail.</p>"},{"location":"etcd-deep-dive/#specifying-certificates-authentication","title":"Specifying Certificates (Authentication)","text":"<p>Apart from setting the API version, you must specify the path to certificate files so that <code>etcdctl</code> can authenticate to the ETCD API Server.</p> <p>Standard locations (kubeadm/master node): *   <code>--cacert /etc/kubernetes/pki/etcd/ca.crt</code> *   <code>--cert /etc/kubernetes/pki/etcd/server.crt</code> *   <code>--key /etc/kubernetes/pki/etcd/server.key</code></p>"},{"location":"etcd-deep-dive/#master-command-example","title":"Master Command Example","text":"<p>Here is the full command that combines API versioning, certificate paths, and execution inside a pod:</p> <pre><code>kubectl exec etcd-master -n kube-system -- sh -c \"ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key\"\n</code></pre>"},{"location":"etcd-deep-dive/#the-magic-command","title":"The \"Magic\" Command","text":"<p>To see what resides in your cluster's brain, run this from inside the etcd pod (or a node with <code>etcdctl</code> installed):</p> <pre><code># General Syntax\nETCDCTL_API=3 etcdctl \\\n  --cacert=&lt;path-to-ca&gt; \\\n  --cert=&lt;path-to-cert&gt; \\\n  --key=&lt;path-to-key&gt; \\\n  get / --prefix --keys-only\n</code></pre> <p>Example (Minikube): <pre><code>kubectl exec etcd-minikube -n kube-system -- etcdctl \\\n  --cacert=/var/lib/minikube/certs/etcd/ca.crt \\\n  --cert=/var/lib/minikube/certs/etcd/server.crt \\\n  --key=/var/lib/minikube/certs/etcd/server.key \\\n  get /registry/pods --prefix --keys-only\n</code></pre></p> <p>Sample Output: <pre><code>/registry/pods/default/my-web-app\n/registry/pods/kube-system/coredns-66bc5c9577-6xrr6\n/registry/pods/kube-system/etcd-minikube\n</code></pre></p>"},{"location":"etcd-deep-dive/#2-etcd-backup-cka-must-know","title":"2. ETCD Backup (CKA Must-Know)","text":"<p>You MUST know how to take a snapshot of the ETCD database.</p>"},{"location":"etcd-deep-dive/#step-1-log-in-to-the-control-plane-node","title":"Step 1: Log in to the Control Plane Node","text":"<p>In the exam, you will likely run this directly on the master node where <code>etcdctl</code> is installed.</p>"},{"location":"etcd-deep-dive/#step-2-run-the-snapshot-command","title":"Step 2: Run the Snapshot Command","text":"<p>Use the <code>snapshot save</code> command.</p> <pre><code>ETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  snapshot save /tmp/etcd-backup.db\n</code></pre> <p>Success Output: <pre><code>Snapshot saved at /tmp/etcd-backup.db\n</code></pre></p>"},{"location":"etcd-deep-dive/#step-3-verify-the-snapshot","title":"Step 3: Verify the Snapshot","text":"<p>Always verify your backup was successful.</p> <pre><code>ETCDCTL_API=3 etcdctl --write-out=table snapshot status /tmp/etcd-backup.db\n</code></pre>"},{"location":"etcd-deep-dive/#3-etcd-restore-disaster-recovery","title":"3. ETCD Restore (Disaster Recovery)","text":"<p>If your cluster data is corrupted or deleted, you restore from the snapshot.</p>"},{"location":"etcd-deep-dive/#criticial-concept-the-restore-process","title":"Criticial Concept: The Restore Process","text":"<p>Restoring doesn't just \"overwrite\" the running database. It creates a new data directory. You then have to tell the ETCD pod to use this new directory.</p>"},{"location":"etcd-deep-dive/#step-1-restore-the-snapshot","title":"Step 1: Restore the Snapshot","text":"<p>This command extracts the backup file into a new directory path.</p> <pre><code># directory where the new data will be extracted\nETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \\\n  --data-dir /var/lib/etcd-restore-new\n</code></pre>"},{"location":"etcd-deep-dive/#step-2-update-the-etcd-static-pod-manifest","title":"Step 2: Update the ETCD Static Pod Manifest","text":"<p>You need to point the running ETCD pod to this new directory.</p> <ol> <li>Edit the manifest: <code>/etc/kubernetes/manifests/etcd.yaml</code></li> <li>Update <code>hostPath</code>: look for the volume mount for <code>etcd-data</code>.</li> </ol> <p>Before: <pre><code>  volumes:\n  - hostPath:\n      path: /var/lib/etcd\n      type: DirectoryOrCreate\n    name: etcd-data\n</code></pre></p> <p>After: <pre><code>  volumes:\n  - hostPath:\n      path: /var/lib/etcd-restore-new  # &lt;--- CHANGED THIS\n      type: DirectoryOrCreate\n    name: etcd-data\n</code></pre></p>"},{"location":"etcd-deep-dive/#step-3-wait-for-restart","title":"Step 3: Wait for Restart","text":"<p>Kubernetes will detect the file change and restart the ETCD pod automatically. It might take a minute.</p>"},{"location":"etcd-deep-dive/#4-cheat-sheet-summary","title":"4. Cheat Sheet Summary","text":"Task Command / Flag API Version <code>export ETCDCTL_API=3</code> (Always set this!) Endpoint <code>--endpoints=https://127.0.0.1:2379</code> Cert Flags <code>--cacert</code>, <code>--cert</code>, <code>--key</code> Backup <code>snapshot save &lt;filename&gt;</code> Verify <code>snapshot status &lt;filename&gt;</code> Restore <code>snapshot restore &lt;filename&gt; --data-dir &lt;new-path&gt;</code>"},{"location":"network-policy-architecture/","title":"Network Policy Architecture","text":"<p>This diagram illustrates how NetworkPolicies control pod-to-pod communication in Kubernetes.</p> <p></p>"},{"location":"network-policy-architecture/#how-networkpolicies-work","title":"How NetworkPolicies Work","text":""},{"location":"network-policy-architecture/#default-behavior","title":"Default Behavior","text":"<ul> <li>By default, all pods can communicate with each other</li> <li>Once a NetworkPolicy selects a pod, that pod becomes isolated</li> <li>Must explicitly allow desired traffic (whitelist approach)</li> </ul>"},{"location":"network-policy-architecture/#policy-types","title":"Policy Types","text":""},{"location":"network-policy-architecture/#ingress-rules","title":"Ingress Rules","text":"<ul> <li>Control incoming traffic TO the selected pods</li> <li>Specify which sources can connect (pods, namespaces, IP blocks)</li> <li>Define allowed ports and protocols</li> </ul>"},{"location":"network-policy-architecture/#egress-rules","title":"Egress Rules","text":"<ul> <li>Control outgoing traffic FROM the selected pods</li> <li>Specify allowed destinations</li> <li>Critical: Must allow DNS (port 53) for name resolution</li> </ul>"},{"location":"network-policy-architecture/#label-selectors","title":"Label Selectors","text":""},{"location":"network-policy-architecture/#podselector","title":"podSelector","text":"<ul> <li>Selects which pods the policy applies to</li> <li>Uses label matching (exact match required)</li> <li>Empty podSelector = applies to all pods in namespace</li> </ul>"},{"location":"network-policy-architecture/#namespaceselector","title":"namespaceSelector","text":"<ul> <li>Selects pods from specific namespaces</li> <li>Useful for cross-namespace communication</li> <li>Can combine with podSelector for fine-grained control</li> </ul>"},{"location":"network-policy-architecture/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Implicit Deny: Empty egress rules block ALL outbound traffic (including DNS)</li> <li>Label Typos: Selector doesn't warn if 0 pods match</li> <li>One-Way Rules: Allowing egress doesn't automatically allow ingress on destination</li> <li>Missing DNS: Always allow port 53 UDP to kube-system namespace</li> </ol>"},{"location":"statefulset-architecture/","title":"StatefulSet &amp; PersistentVolume Architecture","text":"<p>This diagram shows how StatefulSets work with PersistentVolumes in Kubernetes.</p> <p></p>"},{"location":"statefulset-architecture/#key-components","title":"Key Components","text":""},{"location":"statefulset-architecture/#statefulset","title":"StatefulSet","text":"<ul> <li>Manages stateful applications with stable pod identities</li> <li>Each pod gets a unique, persistent identifier (e.g., web-0, web-1, web-2)</li> <li>Pods are created and deleted in order</li> </ul>"},{"location":"statefulset-architecture/#volumeclaimtemplates","title":"VolumeClaimTemplates","text":"<ul> <li>Automatically creates a PVC for each pod replica</li> <li>Each pod gets its own unique PVC</li> <li>PVCs persist even when pods are deleted</li> </ul>"},{"location":"statefulset-architecture/#persistentvolume-pv","title":"PersistentVolume (PV)","text":"<ul> <li>Actual storage resource in the cluster</li> <li>Can be provisioned statically or dynamically via StorageClass</li> <li>Independent lifecycle from pods</li> </ul>"},{"location":"statefulset-architecture/#access-modes","title":"Access Modes","text":"<ul> <li>ReadWriteOnce (RWO): Volume mounted by single node</li> <li>ReadWriteMany (RWX): Volume mounted by multiple nodes (requires NFS/EFS)</li> <li>ReadOnlyMany (ROX): Read-only by multiple nodes</li> </ul>"},{"location":"statefulset-pvc-explained/","title":"StatefulSets and PVCs: The Key Difference","text":"<p>Understanding why StatefulSets are special when it comes to storage defined by PVCs.</p>"},{"location":"statefulset-pvc-explained/#the-problem-with-standard-deployments","title":"The Problem with Standard Deployments","text":"<p>If you use a standard Deployment with a PVC: 1. You create 1 PVC. 2. You create a Deployment with <code>replicas: 3</code>. 3. ALL 3 Pods try to mount that SAME single PVC.    - If the PVC is <code>ReadWriteOnce</code> (like AWS EBS), Pod #1 wins and Pod #2 and #3 fail (crash).    - They cannot share the disk.</p>"},{"location":"statefulset-pvc-explained/#the-statefulset-solution","title":"The StatefulSet Solution","text":"<p>StatefulSets introduce a \"Cookie Cutter\" feature called <code>volumeClaimTemplates</code>.</p> <p>Instead of pointing to one existing PVC, you tell the StatefulSet:</p> <p>\"Here is a template. Every time you create a Pod, stamp out a BRAND NEW PVC just for that Pod.\"</p>"},{"location":"statefulset-pvc-explained/#how-it-works-visually","title":"How it works visually","text":"<p>StatefulSet Definition: - <code>replicas: 3</code> - <code>volumeClaimTemplate: name=data</code></p> <p>Kubernetes automatically creates:</p> Pod Name PVC Name Created (Pod Identity) Physical Volume (PV) <code>web-0</code> <code>data-web-0</code> Disk A (10GB) <code>web-1</code> <code>data-web-1</code> Disk B (10GB) <code>web-2</code> <code>data-web-2</code> Disk C (10GB)"},{"location":"statefulset-pvc-explained/#the-sticky-bond","title":"The \"Sticky\" Bond","text":"<p>This is the most critical part.</p> <ol> <li>Disaster Strikes: Pod <code>web-0</code> crashes or the node dies.</li> <li>Rescheduling: Kubernetes sees <code>web-0</code> is gone and starts a new one strictly named <code>web-0</code> on a different node.</li> <li>The Magic: Kubernetes knows: \"Ah, you are <code>web-0</code>. Your specific data is inside PVC <code>data-web-0</code>. I will attach THAT specific disk to you.\"</li> </ol> <p>Result: The database comes back up with all its data intact (users, transactions, etc), exactly as if nothing happened. A Deployment would not guarantee this mapping.</p>"},{"location":"troubleshooting-flowchart/","title":"Kubernetes Troubleshooting Flowchart","text":"<p>This flowchart provides a systematic approach to troubleshooting common Kubernetes issues.</p> <p></p>"},{"location":"troubleshooting-flowchart/#how-to-use-this-flowchart","title":"How to Use This Flowchart","text":"<ol> <li>Start with identifying the pod status</li> <li>Follow the decision tree based on the error symptoms</li> <li>Apply the recommended fixes for each scenario</li> <li>Verify the resolution with kubectl commands</li> </ol>"},{"location":"troubleshooting-flowchart/#quick-reference","title":"Quick Reference","text":"<ul> <li>ImagePullBackOff: Check image name, registry credentials</li> <li>CrashLoopBackOff: Check logs, liveness probes, resource limits</li> <li>Pending: Check node resources, taints, affinity rules</li> <li>ContainerCreating: Check PVC status, network plugins</li> </ul>"},{"location":"yaml-guide/","title":"YAML Guide for Kubernetes","text":"<p>Understanding YAML data structures is critical for the CKA exam. You will spend 50% of your time reading and editing YAML manifests.</p> <p>This guide breaks down the data types you will encounter in every Kubernetes resource.</p>"},{"location":"yaml-guide/#yaml-data-types-in-kubernetes","title":"YAML Data Types in Kubernetes","text":""},{"location":"yaml-guide/#1-strings-simple-text-values","title":"1. Strings (Simple Text Values)","text":"<p>These are plain text values. Quotes are optional in YAML (but allowed).</p> <pre><code>apiVersion: v1                    # String\nkind: Pod                         # String\nname: hostnames-74fbbfdf9f-b7ftl  # String\nnamespace: default                # String\nimage: registry.k8s.io/serve_hostname  # String\n</code></pre> <p>When to use quotes: *   If the value contains special characters: <code>name: \"my-app:v1.0\"</code> *   If it looks like a number but should be a string: <code>version: \"1.0\"</code></p>"},{"location":"yaml-guide/#2-dictionariesmaps-key-value-pairs","title":"2. Dictionaries/Maps (Key-Value Pairs)","text":"<p>These are nested objects. Indicated by a colon <code>:</code> followed by indentation.</p> <pre><code>metadata:              # This is a Dictionary\n  name: my-pod         # String inside the dictionary\n  labels:              # This is a Dictionary inside metadata\n    app: hostnames     # String inside labels\n    tier: frontend     # Another key-value pair\n</code></pre> <p>How to spot them: Look for a colon <code>:</code> followed by a newline and increased indentation.</p> <p>Common mistake: <pre><code># WRONG (missing indentation)\nmetadata:\nname: my-pod\n\n# CORRECT\nmetadata:\n  name: my-pod\n</code></pre></p>"},{"location":"yaml-guide/#3-listsarrays-multiple-items","title":"3. Lists/Arrays (Multiple Items)","text":"<p>These start with a dash <code>-</code>. Each item can be a string, number, or even a dictionary.</p> <p>Example 1: List of Dictionaries (Most Common) <pre><code>containers:                    # This is a List\n  - name: nginx                # First item (Dictionary)\n    image: nginx:1.21\n  - name: sidecar              # Second item (Dictionary)\n    image: busybox\n</code></pre></p> <p>Example 2: List of Strings <pre><code>args:                          # List of strings\n  - \"--config=/etc/app.conf\"\n  - \"--verbose\"\n</code></pre></p> <p>Example 3: Nested Lists <pre><code>tolerations:                   # List of Dictionaries\n  - effect: NoExecute          # First toleration\n    key: node.kubernetes.io/not-ready\n    tolerationSeconds: 300\n  - effect: NoExecute          # Second toleration\n    key: node.kubernetes.io/unreachable\n    tolerationSeconds: 300\n</code></pre></p>"},{"location":"yaml-guide/#4-booleans-truefalse","title":"4. Booleans (True/False)","text":"<pre><code>readOnly: true                 # Boolean\nblockOwnerDeletion: true       # Boolean\nenableServiceLinks: false      # Boolean\n</code></pre> <p>Valid values: <code>true</code>, <code>false</code>, <code>True</code>, <code>False</code>, <code>TRUE</code>, <code>FALSE</code> (case-insensitive).</p>"},{"location":"yaml-guide/#5-numbers-integers-and-floats","title":"5. Numbers (Integers and Floats)","text":"<pre><code>priority: 0                    # Integer\nterminationGracePeriodSeconds: 30  # Integer\nrestartCount: 1                # Integer\ncpu: 0.5                       # Float (500 millicores)\n</code></pre>"},{"location":"yaml-guide/#6-nullempty-values","title":"6. Null/Empty Values","text":"<pre><code>lastProbeTime: null            # Explicitly null\nresources: {}                  # Empty dictionary\nargs: []                       # Empty list\n</code></pre> <p>When you see <code>{}</code>: It means \"this field exists but has no data.\" When you see <code>null</code>: It means \"this field is intentionally empty.\"</p>"},{"location":"yaml-guide/#real-world-example-annotated-pod-yaml","title":"Real-World Example: Annotated Pod YAML","text":"<p>Let's break down a real Pod manifest with data type annotations:</p> <pre><code>apiVersion: v1                 # String\nkind: Pod                      # String\nmetadata:                      # Dictionary\n  name: my-app                 # String\n  namespace: default           # String\n  labels:                      # Dictionary\n    app: web                   # String\n    tier: frontend             # String\nspec:                          # Dictionary\n  containers:                  # List (of Dictionaries)\n  - name: nginx                # Dictionary item #1\n    image: nginx:1.21          # String\n    ports:                     # List (of Dictionaries)\n    - containerPort: 80        # Dictionary item\n      protocol: TCP            # String\n    resources:                 # Dictionary\n      requests:                # Dictionary\n        memory: \"64Mi\"         # String (with quotes because of unit)\n        cpu: \"250m\"            # String\n      limits:                  # Dictionary\n        memory: \"128Mi\"        # String\n        cpu: \"500m\"            # String\n    volumeMounts:              # List (of Dictionaries)\n    - name: config-volume      # Dictionary item\n      mountPath: /etc/config   # String\n      readOnly: true           # Boolean\n  restartPolicy: Always        # String\n  volumes:                     # List (of Dictionaries)\n  - name: config-volume        # Dictionary item\n    configMap:                 # Dictionary\n      name: app-config         # String\n</code></pre>"},{"location":"yaml-guide/#common-patterns-for-cka","title":"Common Patterns for CKA","text":""},{"location":"yaml-guide/#pattern-1-adding-a-container-to-a-pod","title":"Pattern 1: Adding a Container to a Pod","text":"<p>You are adding an item to the <code>containers</code> list.</p> <pre><code>spec:\n  containers:\n  - name: main-app             # Existing container\n    image: nginx\n  - name: sidecar              # NEW container (note the dash!)\n    image: busybox\n    command: [\"sh\", \"-c\", \"tail -f /var/log/app.log\"]\n</code></pre>"},{"location":"yaml-guide/#pattern-2-adding-a-label","title":"Pattern 2: Adding a Label","text":"<p>You are adding a key-value pair to the <code>labels</code> dictionary.</p> <pre><code>metadata:\n  labels:\n    app: web                   # Existing label\n    environment: production    # NEW label (no dash, just key: value)\n</code></pre>"},{"location":"yaml-guide/#pattern-3-adding-a-volume-mount","title":"Pattern 3: Adding a Volume Mount","text":"<p>You are adding an item to the <code>volumeMounts</code> list.</p> <pre><code>volumeMounts:\n  - name: data                 # Existing mount\n    mountPath: /data\n  - name: logs                 # NEW mount\n    mountPath: /var/log\n</code></pre>"},{"location":"yaml-guide/#jsonpath-quick-reference","title":"JSONPath Quick Reference","text":"<p>Understanding data types helps you write JSONPath queries (used in <code>kubectl get -o jsonpath</code>).</p> Data Type JSONPath Example What it Returns String <code>.metadata.name</code> <code>\"my-pod\"</code> Dictionary <code>.metadata.labels</code> <code>{\"app\":\"web\",\"tier\":\"frontend\"}</code> List <code>.spec.containers</code> <code>[{...}, {...}]</code> (array of containers) First item in list <code>.spec.containers[0].name</code> <code>\"nginx\"</code> All names in list <code>.spec.containers[*].name</code> <code>[\"nginx\", \"sidecar\"]</code>"},{"location":"yaml-guide/#debugging-yaml-errors","title":"Debugging YAML Errors","text":""},{"location":"yaml-guide/#error-1-mapping-values-are-not-allowed-here","title":"Error 1: \"mapping values are not allowed here\"","text":"<p>Cause: Missing indentation or extra space.</p> <pre><code># WRONG\nmetadata:\nname: my-pod\n\n# CORRECT\nmetadata:\n  name: my-pod\n</code></pre>"},{"location":"yaml-guide/#error-2-expected-a-sequence","title":"Error 2: \"expected a sequence\"","text":"<p>Cause: You provided a dictionary where a list was expected.</p> <pre><code># WRONG\ncontainers:\n  name: nginx\n\n# CORRECT\ncontainers:\n  - name: nginx\n</code></pre>"},{"location":"yaml-guide/#error-3-duplicate-key","title":"Error 3: \"duplicate key\"","text":"<p>Cause: You defined the same key twice in a dictionary.</p> <pre><code># WRONG\nmetadata:\n  name: pod-1\n  name: pod-2\n\n# CORRECT\nmetadata:\n  name: pod-1\n</code></pre>"},{"location":"yaml-guide/#summary-cheat-sheet","title":"Summary Cheat Sheet","text":"Type Syntax Example String <code>key: value</code> <code>name: my-pod</code> Dictionary <code>key:</code> + indented keys <code>metadata:</code> <code>name: pod</code> List <code>key:</code> + <code>- item</code> <code>containers:</code> <code>- name: nginx</code> Boolean <code>key: true/false</code> <code>readOnly: true</code> Number <code>key: 123</code> <code>replicas: 3</code> Null <code>key: null</code> or <code>key:</code> <code>lastProbeTime: null</code> Empty Dict <code>key: {}</code> <code>resources: {}</code> Empty List <code>key: []</code> <code>args: []</code>"},{"location":"yaml-guide/#practice-exercise","title":"Practice Exercise","text":"<p>Try to identify the data types in this snippet:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <p>Answers: *   <code>apiVersion</code>: String *   <code>metadata</code>: Dictionary *   <code>labels</code>: Dictionary *   <code>replicas</code>: Number *   <code>containers</code>: List *   <code>ports</code>: List *   <code>containerPort</code>: Number</p>"},{"location":"cluster-architecture/admission-controller-types/","title":"Types of Admission Controllers and Webhooks","text":"<p>Understanding the types of admission controllers and how they interconnect is crucial for mastering Kubernetes admission control.</p>"},{"location":"cluster-architecture/admission-controller-types/#1-the-complete-taxonomy","title":"1. The Complete Taxonomy","text":""},{"location":"cluster-architecture/admission-controller-types/#high-level-classification","title":"High-Level Classification","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    ADMISSION CONTROLLERS                            \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  BUILT-IN                 \u2502   \u2502  DYNAMIC (WEBHOOKS)           \u2502 \u2502\n\u2502  \u2502  (Compiled into apiserver)\u2502   \u2502  (External services)          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502              \u2193                                  \u2193                   \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502   MUTATING      \u2502                \u2502   MUTATING      \u2502          \u2502\n\u2502    \u2502   - ServiceAcct \u2502                \u2502   - Webhooks    \u2502          \u2502\n\u2502    \u2502   - DefaultSC   \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502              \u2502                                                      \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502   VALIDATING    \u2502                \u2502   VALIDATING    \u2502          \u2502\n\u2502    \u2502   - ResQuota    \u2502                \u2502   - Webhooks    \u2502          \u2502\n\u2502    \u2502   - LimitRange  \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#2-type-1-built-in-admission-controllers","title":"2. Type 1: Built-in Admission Controllers","text":"<p>Built-in admission controllers are compiled into the kube-apiserver binary.</p>"},{"location":"cluster-architecture/admission-controller-types/#characteristics","title":"Characteristics","text":"Aspect Details Location Inside kube-apiserver binary Configuration <code>--enable-admission-plugins</code> flag Language Written in Go Performance Fast (in-process) Flexibility Limited (fixed logic) Examples ServiceAccount, ResourceQuota, NamespaceLifecycle"},{"location":"cluster-architecture/admission-controller-types/#sub-types","title":"Sub-types","text":""},{"location":"cluster-architecture/admission-controller-types/#type-1a-built-in-mutating-controllers","title":"Type 1A: Built-in Mutating Controllers","text":"<p>Purpose: Modify requests before validation.</p> Controller What It Mutates ServiceAccount Injects SA token volumes DefaultStorageClass Sets default StorageClass on PVCs DefaultTolerationSeconds Adds node taint tolerations PodSecurityDefaults Sets pod security labels <p>Example Flow: <pre><code>User submits Pod with no serviceAccountName\n         \u2193\nServiceAccount Controller (Built-in Mutating)\n         \u2193\nAdds: serviceAccountName: default\nAdds: volume mount for token\n         \u2193\nModified Pod \u2192 Sent to Validating Controllers\n</code></pre></p>"},{"location":"cluster-architecture/admission-controller-types/#type-1b-built-in-validating-controllers","title":"Type 1B: Built-in Validating Controllers","text":"<p>Purpose: Approve or reject requests.</p> Controller What It Validates ResourceQuota Checks if within quota LimitRanger Checks if within limits NamespaceLifecycle Checks namespace existence PodSecurity Validates pod security standards NodeRestriction Validates kubelet permissions <p>Example Flow: <pre><code>Pod (already mutated) arrives\n         \u2193\nResourceQuota Controller (Built-in Validating)\n         \u2193\nCheck: Does this pod exceed namespace quota?\n         \u2193\nYES \u2192 \u274c REJECT (Error to user)\nNO  \u2192 \u2705 APPROVE (Continue to next validator)\n</code></pre></p>"},{"location":"cluster-architecture/admission-controller-types/#3-type-2-dynamic-admission-webhooks","title":"3. Type 2: Dynamic Admission Webhooks","text":"<p>Dynamic admission webhooks are external HTTP services that kube-apiserver calls.</p>"},{"location":"cluster-architecture/admission-controller-types/#characteristics_1","title":"Characteristics","text":"Aspect Details Location External service (pod, external server) Configuration <code>MutatingWebhookConfiguration</code> / <code>ValidatingWebhookConfiguration</code> Language Any (Python, Go, Node.js, etc.) Performance Slower (network call) Flexibility High (custom logic) Examples Istio sidecar injector, OPA Gatekeeper"},{"location":"cluster-architecture/admission-controller-types/#sub-types_1","title":"Sub-types","text":""},{"location":"cluster-architecture/admission-controller-types/#type-2a-mutatingadmissionwebhook","title":"Type 2A: MutatingAdmissionWebhook","text":"<p>Purpose: Call external service to mutate requests.</p> <p>How It Works: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Pod creation request arrives                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. kube-apiserver calls MutatingWebhook                   \u2502\n\u2502                                                            \u2502\n\u2502     POST https://webhook-service.default.svc/mutate        \u2502\n\u2502     {                                                      \u2502\n\u2502       \"request\": {                                         \u2502\n\u2502         \"object\": { /* Pod YAML */ }                       \u2502\n\u2502       }                                                    \u2502\n\u2502     }                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Webhook service processes request                      \u2502\n\u2502     - Runs custom logic (Go, Python, etc.)                 \u2502\n\u2502     - Decides what to add/modify                           \u2502\n\u2502     - Creates JSON Patch                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Webhook returns response                               \u2502\n\u2502     {                                                      \u2502\n\u2502       \"response\": {                                        \u2502\n\u2502         \"allowed\": true,                                   \u2502\n\u2502         \"patchType\": \"JSONPatch\",                          \u2502\n\u2502         \"patch\": \"W3sib3AiOiJhZGQi...\"                     \u2502\n\u2502       }                                                    \u2502\n\u2502     }                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  5. kube-apiserver applies patch                           \u2502\n\u2502     Original Pod + JSON Patch = Mutated Pod                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Configuration: <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: example-mutating-webhook\nwebhooks:\n- name: mutate.example.com\n  clientConfig:\n    service:\n      name: webhook-service\n      namespace: default\n      path: /mutate\n  rules:\n  - operations: [\"CREATE\", \"UPDATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n</code></pre></p>"},{"location":"cluster-architecture/admission-controller-types/#type-2b-validatingadmissionwebhook","title":"Type 2B: ValidatingAdmissionWebhook","text":"<p>Purpose: Call external service to validate requests.</p> <p>How It Works: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Pod (already mutated) arrives for validation           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. kube-apiserver calls ValidatingWebhook                 \u2502\n\u2502                                                            \u2502\n\u2502     POST https://validator.default.svc/validate            \u2502\n\u2502     {                                                      \u2502\n\u2502       \"request\": {                                         \u2502\n\u2502         \"object\": { /* Mutated Pod YAML */ }               \u2502\n\u2502       }                                                    \u2502\n\u2502     }                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Webhook service validates request                      \u2502\n\u2502     - Checks custom policies                               \u2502\n\u2502     - Validates against company rules                      \u2502\n\u2502     - Returns ALLOW or DENY                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Webhook returns response                               \u2502\n\u2502     {                                                      \u2502\n\u2502       \"response\": {                                        \u2502\n\u2502         \"allowed\": false,                                  \u2502\n\u2502         \"status\": {                                        \u2502\n\u2502           \"message\": \"Image not from approved registry\"    \u2502\n\u2502         }                                                  \u2502\n\u2502       }                                                    \u2502\n\u2502     }                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  5. kube-apiserver rejects or approves                     \u2502\n\u2502     DENY \u2192 User gets error message                         \u2502\n\u2502     ALLOW \u2192 Continue to next validator                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Configuration: <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: example-validating-webhook\nwebhooks:\n- name: validate.example.com\n  clientConfig:\n    service:\n      name: validator-service\n      namespace: default\n      path: /validate\n  rules:\n  - operations: [\"CREATE\", \"UPDATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n</code></pre></p>"},{"location":"cluster-architecture/admission-controller-types/#4-how-all-types-interconnect","title":"4. How All Types Interconnect","text":""},{"location":"cluster-architecture/admission-controller-types/#the-complete-admission-control-flow","title":"The Complete Admission Control Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      REQUEST ARRIVES AT API SERVER                       \u2502\n\u2502                           kubectl apply -f pod.yaml                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHENTICATION \u2705                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHORIZATION \u2705                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 1: MUTATING ADMISSION (Sequential Execution)                      \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 TYPE 1A: Built-in Mutating Controllers                            \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Controller 1: NamespaceLifecycle (check namespace exists)         \u2502 \u2502\n\u2502  \u2502      \u2193                                                             \u2502 \u2502\n\u2502  \u2502 Controller 2: ServiceAccount (inject SA token)                    \u2502 \u2502\n\u2502  \u2502      \u2193                                                             \u2502 \u2502\n\u2502  \u2502 Controller 3: DefaultStorageClass (set default SC)                \u2502 \u2502\n\u2502  \u2502      \u2193                                                             \u2502 \u2502\n\u2502  \u2502 Controller 4: DefaultTolerationSeconds (add tolerations)          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                   \u2193                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 TYPE 2A: MutatingAdmissionWebhook                                 \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Webhook 1: Istio Sidecar Injector                                 \u2502 \u2502\n\u2502  \u2502   POST https://istio-sidecar.istio-system.svc/inject              \u2502 \u2502\n\u2502  \u2502   Response: Add istio-proxy container                             \u2502 \u2502\n\u2502  \u2502      \u2193                                                             \u2502 \u2502\n\u2502  \u2502 Webhook 2: Vault Agent Injector                                   \u2502 \u2502\n\u2502  \u2502   POST https://vault-agent.vault.svc/mutate                       \u2502 \u2502\n\u2502  \u2502   Response: Add vault agent init container                        \u2502 \u2502\n\u2502  \u2502      \u2193                                                             \u2502 \u2502\n\u2502  \u2502 Webhook 3: Image Policy Enforcer                                  \u2502 \u2502\n\u2502  \u2502   POST https://image-policy.security.svc/mutate                   \u2502 \u2502\n\u2502  \u2502   Response: Prepend registry URL to image                         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                          \u2502\n\u2502  Result: Pod is now FULLY MUTATED                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 2: VALIDATING ADMISSION (Parallel Execution)                     \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 TYPE 1B: Built-in Validating Controllers                          \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Controller 1: LimitRanger           Controller 2: ResourceQuota   \u2502 \u2502\n\u2502  \u2502 Check: Within limits? \u2705             Check: Within quota? \u2705       \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Controller 3: PodSecurity            Controller 4: NodeRestriction\u2502 \u2502\n\u2502  \u2502 Check: Security policy? \u2705           Check: Node perms? \u2705         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                   \u2193                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 TYPE 2B: ValidatingAdmissionWebhook                               \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Webhook 1: OPA Gatekeeper          Webhook 2: Image Scanner       \u2502 \u2502\n\u2502  \u2502 POST /validate                     POST /validate-image            \u2502 \u2502\n\u2502  \u2502 Check: Policy compliant? \u2705         Check: No vulnerabilities? \u2705  \u2502 \u2502\n\u2502  \u2502                                                                    \u2502 \u2502\n\u2502  \u2502 Webhook 3: Custom Validator                                       \u2502 \u2502\n\u2502  \u2502 POST /custom-validate                                              \u2502 \u2502\n\u2502  \u2502 Check: Company rules? \u2705                                           \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                          \u2502\n\u2502  IF ANY VALIDATOR REJECTS \u2192 \u274c REQUEST REJECTED                          \u2502\n\u2502  IF ALL VALIDATORS APPROVE \u2192 \u2705 CONTINUE                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PERSIST TO ETCD \u2705                                                       \u2502\n\u2502  Pod is created with all mutations applied                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#5-built-in-vs-webhook-detailed-comparison","title":"5. Built-in vs Webhook: Detailed Comparison","text":""},{"location":"cluster-architecture/admission-controller-types/#architecture-differences","title":"Architecture Differences","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BUILT-IN ADMISSION CONTROLLERS                                     \u2502\n\u2502                                                                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502   \u2502                                                     \u2502          \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502          \u2502\n\u2502   \u2502   \u2502         kube-apiserver                   \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u2502  ServiceAccount Controller         \u2502  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u2502  (Compiled Go code)                \u2502  \u2502  \u2190\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502\n\u2502   \u2502   \u2502  \u2502                                    \u2502  \u2502     \u2502   Fast   \u2502\n\u2502   \u2502   \u2502  \u2502  ResourceQuota Controller          \u2502  \u2502     \u2502  (in-    \u2502\n\u2502   \u2502   \u2502  \u2502  (Compiled Go code)                \u2502  \u2502     \u2502  process)\u2502\n\u2502   \u2502   \u2502  \u2502                                    \u2502  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u2502  LimitRanger Controller            \u2502  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u2502  (Compiled Go code)                \u2502  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502     \u2502          \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502          \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  WEBHOOK ADMISSION CONTROLLERS                                      \u2502\n\u2502                                                                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502   \u2502         kube-apiserver                   \u2502                     \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                     \u2502\n\u2502   \u2502  \u2502  MutatingWebhook Plugin            \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2502  (Makes HTTP calls)                \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                     \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                      \u2502 HTTP POST                                   \u2502\n\u2502                      \u2502 (Network call)                              \u2502\n\u2502                      \u2193                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502   \u2502  Webhook Service Pod                     \u2502  \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502       Slower       \u2502\n\u2502   \u2502  \u2502  Your custom code                  \u2502  \u2502     (network       \u2502\n\u2502   \u2502  \u2502  (Python, Go, Node.js, etc.)       \u2502  \u2502      latency)      \u2502\n\u2502   \u2502  \u2502                                    \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2502  - Istio sidecar injector          \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2502  - Vault agent injector            \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2502  - OPA Gatekeeper                  \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2502  - Custom validators               \u2502  \u2502                     \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                     \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature Built-in Controllers Webhook Controllers Performance \u26a1 Very fast (in-process) \ud83d\udc0c Slower (HTTP call) Latency &lt;1ms 10-100ms+ Flexibility \u274c Fixed logic \u2705 Custom logic Language Go only Any language Updates \u274c Requires apiserver restart \u2705 Update anytime Configuration API server flags K8s resources (YAML) Debugging \ud83d\udd34 Harder (recompile) \ud83d\udfe2 Easier (logs, debug) Failure Impact \u274c API server crash \ud83d\udfe1 Request timeout/fail Examples ServiceAccount, ResourceQuota Istio, OPA, Vault"},{"location":"cluster-architecture/admission-controller-types/#6-execution-order-and-precedence","title":"6. Execution Order and Precedence","text":""},{"location":"cluster-architecture/admission-controller-types/#detailed-execution-sequence","title":"Detailed Execution Sequence","text":"<pre><code>REQUEST\n   \u2502\n   \u251c\u2500 Authentication\n   \u2502\n   \u251c\u2500 Authorization\n   \u2502\n   \u251c\u2500 MUTATING PHASE (Sequential)\n   \u2502   \u2502\n   \u2502   \u251c\u2500 Built-in Mutating Controllers (in configured order)\n   \u2502   \u2502   \u251c\u2500 NamespaceLifecycle\n   \u2502   \u2502   \u251c\u2500 LimitRanger (mutating part)\n   \u2502   \u2502   \u251c\u2500 ServiceAccount\n   \u2502   \u2502   \u251c\u2500 DefaultStorageClass\n   \u2502   \u2502   \u251c\u2500 DefaultTolerationSeconds\n   \u2502   \u2502   \u2514\u2500 RuntimeClass\n   \u2502   \u2502\n   \u2502   \u2514\u2500 MutatingAdmissionWebhooks (ordered by name, then creation time)\n   \u2502       \u251c\u2500 istio-sidecar-injector\n   \u2502       \u251c\u2500 vault-agent-injector\n   \u2502       \u2514\u2500 custom-mutator\n   \u2502\n   \u251c\u2500 VALIDATING PHASE (Parallel)\n   \u2502   \u2502\n   \u2502   \u251c\u2500 Built-in Validating Controllers (parallel)\n   \u2502   \u2502   \u251c\u2500 LimitRanger\n   \u2502   \u2502   \u251c\u2500 ResourceQuota\n   \u2502   \u2502   \u251c\u2500 PodSecurity\n   \u2502   \u2502   \u2514\u2500 NodeRestriction\n   \u2502   \u2502\n   \u2502   \u2514\u2500 ValidatingAdmissionWebhooks (parallel)\n   \u2502       \u251c\u2500 gatekeeper-validator\n   \u2502       \u251c\u2500 image-scanner\n   \u2502       \u2514\u2500 custom-validator\n   \u2502\n   \u2514\u2500 Persist to etcd\n</code></pre> <p>Key Points: 1. Mutating phase is SEQUENTIAL - order matters! 2. Validating phase is PARALLEL - all run simultaneously 3. Built-in before Webhooks within each phase 4. Any rejection stops the entire request</p>"},{"location":"cluster-architecture/admission-controller-types/#7-real-world-integration-examples","title":"7. Real-World Integration Examples","text":""},{"location":"cluster-architecture/admission-controller-types/#example-1-complete-pod-creation-with-all-types","title":"Example 1: Complete Pod Creation with All Types","text":"<p>Original Pod Submission: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: app\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>After All Mutations (Built-in + Webhooks): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\n    istio.io/rev: default                          # \u2190 Istio webhook\n  annotations:\n    vault.hashicorp.com/agent-inject: \"true\"       # \u2190 Vault webhook\n    prometheus.io/scrape: \"true\"                   # \u2190 Prometheus webhook\nspec:\n  serviceAccountName: default                      # \u2190 ServiceAccount (built-in)\n  automountServiceAccountToken: true\n  tolerations:                                     # \u2190 DefaultTolerationSeconds (built-in)\n  - key: node.kubernetes.io/not-ready\n    operator: Exists\n    effect: NoExecute\n    tolerationSeconds: 300\n  initContainers:                                  # \u2190 Vault webhook\n  - name: vault-agent-init\n    image: vault:1.12.1\n  containers:\n  - name: app\n    image: myregistry.company.com/nginx:latest     # \u2190 Image Policy webhook (modified!)\n    ports:\n    - containerPort: 80\n    volumeMounts:                                  # \u2190 ServiceAccount (built-in)\n    - name: kube-api-access-xxxxx\n      mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n  - name: istio-proxy                              # \u2190 Istio webhook (added!)\n    image: istio/proxyv2:1.17.1\n    ports:\n    - containerPort: 15001\n  volumes:\n  - name: kube-api-access-xxxxx                    # \u2190 ServiceAccount (built-in)\n    projected:\n      sources:\n      - serviceAccountToken: ...\n</code></pre></p>"},{"location":"cluster-architecture/admission-controller-types/#example-2-multi-cluster-service-mesh","title":"Example 2: Multi-Cluster Service Mesh","text":"<p>Scenario: Using both built-in and webhook types together.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Pod Creation Request                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Built-in: ServiceAccount       \u2502\n        \u2502 Adds: SA token volume          \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Webhook: Istio Sidecar         \u2502\n        \u2502 Adds: Envoy proxy container    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Webhook: Linkerd Injector      \u2502\n        \u2502 Adds: Linkerd proxy container  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Built-in: ResourceQuota        \u2502\n        \u2502 Validates: Within quota?       \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Webhook: OPA Gatekeeper        \u2502\n        \u2502 Validates: Policy compliant?   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n                   \u2705 APPROVED\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#8-configuration-patterns","title":"8. Configuration Patterns","text":""},{"location":"cluster-architecture/admission-controller-types/#pattern-1-only-built-in-controllers-simple","title":"Pattern 1: Only Built-in Controllers (Simple)","text":"<pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\n--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota\n</code></pre> <p>Use case: Simple clusters without custom policies</p>"},{"location":"cluster-architecture/admission-controller-types/#pattern-2-built-in-webhooks-production","title":"Pattern 2: Built-in + Webhooks (Production)","text":"<pre><code># API Server\n--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,MutatingAdmissionWebhook,ValidatingAdmissionWebhook\n</code></pre> <pre><code># MutatingWebhookConfiguration\napiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: istio-sidecar-injector\nwebhooks:\n- name: sidecar-injector.istio.io\n  clientConfig:\n    service:\n      name: istiod\n      namespace: istio-system\n      path: /inject\n  rules:\n  - operations: [\"CREATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n</code></pre> <p>Use case: Production clusters with service mesh, policy enforcement</p>"},{"location":"cluster-architecture/admission-controller-types/#9-troubleshooting-the-interconnections","title":"9. Troubleshooting the Interconnections","text":""},{"location":"cluster-architecture/admission-controller-types/#issue-my-pod-doesnt-have-the-injected-sidecar","title":"Issue: \"My pod doesn't have the injected sidecar\"","text":"<p>Check the chain:</p> <pre><code># 1. Is MutatingAdmissionWebhook controller enabled?\nkubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml | grep enable-admission-plugins\n\n# 2. Does the webhook configuration exist?\nkubectl get mutatingwebhookconfigurations\n\n# 3. Is the webhook service running?\nkubectl get svc -n istio-system istiod\n\n# 4. Check webhook logs\nkubectl logs -n istio-system deployment/istiod -f\n\n# 5. Test with dry-run to see mutations\nkubectl apply -f pod.yaml --dry-run=server -o yaml\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#issue-request-taking-too-long","title":"Issue: \"Request taking too long\"","text":"<p>Likely cause: Too many webhooks or slow webhook services.</p> <pre><code># Check all webhooks\nkubectl get mutatingwebhookconfigurations\nkubectl get validatingwebhookconfigurations\n\n# Check webhook timeouts\nkubectl get mutatingwebhookconfigurations &lt;name&gt; -o yaml | grep timeoutSeconds\n\n# Monitor API server latency\nkubectl get --raw /metrics | grep apiserver_admission_webhook_admission_duration_seconds\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#10-summary-the-complete-picture","title":"10. Summary: The Complete Picture","text":""},{"location":"cluster-architecture/admission-controller-types/#type-matrix","title":"Type Matrix","text":"Type Category Location Performance Flexibility Examples 1A Built-in Mutating kube-apiserver \u26a1 Very Fast \u274c Fixed ServiceAccount 1B Built-in Validating kube-apiserver \u26a1 Very Fast \u274c Fixed ResourceQuota 2A Webhook Mutating External Pod \ud83d\udc0c Slower \u2705 Custom Istio Injector 2B Webhook Validating External Pod \ud83d\udc0c Slower \u2705 Custom OPA Gatekeeper"},{"location":"cluster-architecture/admission-controller-types/#interconnection-map","title":"Interconnection Map","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    ADMISSION CONTROL SYSTEM                     \u2502\n\u2502                                                                 \u2502\n\u2502  Authentication \u2192 Authorization                                 \u2502\n\u2502         \u2193                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  MUTATING (Sequential)                      \u2502                \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                \u2502\n\u2502  \u2502  \u2502 Built-in      \u2502\u2192\u2192\u2192\u2502 Webhooks        \u2502   \u2502                \u2502\n\u2502  \u2502  \u2502 (Type 1A)     \u2502   \u2502 (Type 2A)       \u2502   \u2502                \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502         \u2193                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502  VALIDATING (Parallel)                      \u2502                \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                \u2502\n\u2502  \u2502  \u2502 Built-in      \u2502\u2551\u2551\u2551\u2502 Webhooks        \u2502   \u2502                \u2502\n\u2502  \u2502  \u2502 (Type 1B)     \u2502\u2551\u2551\u2551\u2502 (Type 2B)       \u2502   \u2502                \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502         \u2193                                                       \u2502\n\u2502  Persist to etcd                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controller-types/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Admission Controllers Reference   Complete list of all built-in controllers</p> </li> <li> <p>Dynamic Admission Control   Webhook configuration and implementation</p> </li> <li> <p>Admission Controller Metrics   Monitoring admission controller performance</p> </li> </ul>"},{"location":"cluster-architecture/admission-controllers/","title":"Admission Controllers in Kubernetes","text":"<p>Admission Controllers are plugins that intercept requests to the Kubernetes API server after authentication and authorization but before the object is persisted in etcd. They can modify (mutate) or reject requests.</p>"},{"location":"cluster-architecture/admission-controllers/#1-the-security-checkpoint-analogy","title":"1. The \"Security Checkpoint\" Analogy","text":"<p>Think of admission controllers like airport security checkpoints:</p> <pre><code>You (User) \u2192 Passport Check (Authentication) \u2192 Visa Check (Authorization) \u2192 Security Screening (Admission) \u2192 Gate (etcd)\n                    \u2705                              \u2705                         \ud83d\udd0d                    \u2705\n</code></pre> <p>What happens at each stage: 1. Authentication: \"Who are you?\" (Are you a valid user?) 2. Authorization: \"What can you do?\" (Do you have permission?) 3. Admission Control: \"Is this request safe and compliant?\" (Does it follow cluster policies?) 4. Persistence: Store the object in etcd</p>"},{"location":"cluster-architecture/admission-controllers/#2-what-are-admission-controllers","title":"2. What Are Admission Controllers?","text":"<p>Admission controllers are compiled into the kube-apiserver binary and can only be configured by administrators.</p>"},{"location":"cluster-architecture/admission-controllers/#key-characteristics","title":"Key Characteristics","text":"Feature Description Location Run inside kube-apiserver Timing After authentication/authorization, before persistence Types Mutating (modify requests) and Validating (approve/reject) Configuration Via <code>--enable-admission-plugins</code> flag Extensibility Custom webhooks (MutatingWebhookConfiguration, ValidatingWebhookConfiguration)"},{"location":"cluster-architecture/admission-controllers/#3-the-two-types-of-admission-controllers","title":"3. The Two Types of Admission Controllers","text":""},{"location":"cluster-architecture/admission-controllers/#mutating-admission-controllers","title":"Mutating Admission Controllers","text":"<p>Purpose: Modify the request before it's persisted.</p> <p>Example: Automatically inject a sidecar container into every pod.</p> <pre><code># Original request\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n\n# After MutatingAdmission\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  - name: sidecar-proxy    # \u2190 Injected by admission controller!\n    image: envoy:latest\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#validating-admission-controllers","title":"Validating Admission Controllers","text":"<p>Purpose: Validate the request and reject if it violates policies.</p> <p>Example: Reject pods that request more than 4 CPUs.</p> <pre><code># Request\nspec:\n  containers:\n  - name: app\n    resources:\n      requests:\n        cpu: \"8\"    # \u2190 Exceeds limit!\n\n# Response from ValidatingAdmission\nError: Pod cpu request exceeds cluster policy maximum of 4 cores\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#4-the-admission-control-flow","title":"4. The Admission Control Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    REQUEST TO API SERVER                        \u2502\n\u2502                     (kubectl apply -f pod.yaml)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 1: AUTHENTICATION                                        \u2502\n\u2502  Plugin: X509, ServiceAccount, OIDC, etc.                       \u2502\n\u2502  Question: \"Who are you?\"                                       \u2502\n\u2502  Result: \u2705 User identified as \"alice@example.com\"               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 2: AUTHORIZATION                                         \u2502\n\u2502  Plugin: RBAC, ABAC, Webhook, etc.                              \u2502\n\u2502  Question: \"Can you create pods in namespace 'prod'?\"           \u2502\n\u2502  Result: \u2705 Allowed (alice has pod:create permission)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 3A: MUTATING ADMISSION CONTROL                           \u2502\n\u2502                                                                 \u2502\n\u2502  Controllers run in sequence:                                   \u2502\n\u2502  1. NamespaceLifecycle    \u2705 (namespace exists)                 \u2502\n\u2502  2. ServiceAccount        \u2705 (inject SA token)                  \u2502\n\u2502  3. MutatingWebhook       \u2705 (add sidecar container)            \u2502\n\u2502                                                                 \u2502\n\u2502  Result: Pod object modified                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 3B: VALIDATING ADMISSION CONTROL                         \u2502\n\u2502                                                                 \u2502\n\u2502  Controllers run in parallel:                                   \u2502\n\u2502  1. ResourceQuota         \u2705 (within quota)                     \u2502\n\u2502  2. LimitRanger           \u2705 (within limits)                    \u2502\n\u2502  3. PodSecurityPolicy     \u2705 (meets security requirements)      \u2502\n\u2502  4. ValidatingWebhook     \u2705 (passes custom validation)         \u2502\n\u2502                                                                 \u2502\n\u2502  Result: Pod approved                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PHASE 4: PERSISTENCE                                           \u2502\n\u2502  Write object to etcd                                           \u2502\n\u2502  Result: \u2705 Pod created successfully                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#key-point-order-matters","title":"Key Point: Order Matters","text":"<p>Mutating \u2192 Validating</p> <p>Why? Because mutating controllers modify the object, validation must happen after all modifications are complete.</p>"},{"location":"cluster-architecture/admission-controllers/#5-built-in-admission-controllers","title":"5. Built-in Admission Controllers","text":"<p>Kubernetes comes with many built-in admission controllers. Here are the most important ones for CKA:</p>"},{"location":"cluster-architecture/admission-controllers/#essential-admission-controllers-usually-enabled","title":"Essential Admission Controllers (Usually Enabled)","text":"Name Type What It Does CKA Importance NamespaceLifecycle Validating Prevents creating resources in non-existent or terminating namespaces High LimitRanger Validating Enforces LimitRange constraints High ServiceAccount Mutating Injects default ServiceAccount if not specified High ResourceQuota Validating Enforces ResourceQuota limits High DefaultStorageClass Mutating Assigns default StorageClass to PVCs Medium DefaultTolerationSeconds Mutating Sets default toleration for taints Low MutatingAdmissionWebhook Mutating Calls external webhooks for custom mutation Medium ValidatingAdmissionWebhook Validating Calls external webhooks for custom validation Medium PodSecurityPolicy Validating (Deprecated) Enforces pod security standards Low (deprecated in 1.25) NodeRestriction Validating Limits what kubelets can modify Medium"},{"location":"cluster-architecture/admission-controllers/#other-important-ones","title":"Other Important Ones","text":"Name What It Does AlwaysPullImages Forces image pull even if already cached DenyEscalatingExec Prevents <code>exec</code> into privileged pods ImagePolicyWebhook Validates images against external policy PersistentVolumeClaimResize Allows PVC expansion Priority Sets pod priority based on PriorityClass StorageObjectInUseProtection Prevents deletion of in-use PVs/PVCs"},{"location":"cluster-architecture/admission-controllers/#namespacelifecycle-the-namespace-guardian","title":"NamespaceLifecycle: The Namespace Guardian","text":"<p>Deprecated Controllers</p> <p>NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and replaced by NamespaceLifecycle.</p> <p>NamespaceLifecycle is one of the most important admission controllers and performs two critical functions:</p>"},{"location":"cluster-architecture/admission-controllers/#function-1-reject-requests-to-non-existent-namespaces","title":"Function 1: Reject Requests to Non-Existent Namespaces","text":"<p>Ensures that any requests to create resources in a namespace that doesn't exist are rejected.</p> <p>Example: <pre><code># Try to create a pod in a non-existent namespace\nkubectl run nginx --image=nginx -n prod\n\n# Result: NamespaceLifecycle rejects the request\nError from server (NotFound): namespaces \"prod\" not found\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#function-2-protect-default-namespaces-from-deletion","title":"Function 2: Protect Default Namespaces from Deletion","text":"<p>Safeguards critical namespaces from being deleted: - <code>default</code> - <code>kube-system</code> - <code>kube-public</code> - <code>kube-node-lease</code></p> <p>Example: <pre><code># Try to delete kube-system namespace\nkubectl delete namespace kube-system\n\n# Result: NamespaceLifecycle rejects the request\nError from server (Forbidden): namespaces \"kube-system\" is forbidden: \nthis namespace may not be deleted\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#function-3-prevent-resource-creation-in-terminating-namespaces","title":"Function 3: Prevent Resource Creation in Terminating Namespaces","text":"<p>When a namespace is being deleted (in <code>Terminating</code> state), NamespaceLifecycle prevents new resources from being created in it.</p> <p>Flow Diagram: <pre><code>Namespace Deletion Started\n         \u2193\nNamespace Status: Terminating\n         \u2193\nUser tries to create Pod \u2500\u2500\u2192 NamespaceLifecycle Controller \u2500\u2500\u2192 \u274c REJECTED\n                                          \u2502\n                                          \u2193\n                            \"Namespace is terminating\"\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#deprecated-namespace-controllers","title":"Deprecated Namespace Controllers","text":"Controller Status Replaced By Notes NamespaceExists \u274c Deprecated NamespaceLifecycle Checked if namespace exists NamespaceAutoProvision \u274c Deprecated NamespaceLifecycle Auto-created namespaces (removed for security) <p>Why were they deprecated?</p> <p>NamespaceAutoProvision was removed because: - Security risk: Automatically creating namespaces could lead to unintended resource creation - Resource exhaustion: Malicious users could create unlimited namespaces - Better practice: Explicit namespace creation via RBAC policies</p> <p>NamespaceExists was merged into NamespaceLifecycle for simplicity.</p>"},{"location":"cluster-architecture/admission-controllers/#6-default-admission-controllers","title":"6. Default Admission Controllers","text":""},{"location":"cluster-architecture/admission-controllers/#enabled-by-default-kubernetes-128","title":"Enabled by Default (Kubernetes 1.28+)","text":"<p>These admission controllers are automatically enabled in most Kubernetes distributions:</p> Controller Name Type Why Enabled by Default NamespaceLifecycle Validating Prevents resources in non-existent/terminating namespaces LimitRanger Validating Enforces LimitRange constraints ServiceAccount Mutating Auto-injects ServiceAccount tokens DefaultStorageClass Mutating Assigns default StorageClass to PVCs DefaultTolerationSeconds Mutating Sets default toleration for node taints MutatingAdmissionWebhook Mutating Enables custom mutation webhooks ValidatingAdmissionWebhook Validating Enables custom validation webhooks ResourceQuota Validating Enforces ResourceQuota limits Priority Validating Handles pod priority scheduling TaintNodesByCondition Mutating Taints nodes based on conditions PodSecurity Validating Enforces Pod Security Standards (replaces PSP) StorageObjectInUseProtection Validating Prevents deletion of in-use PVs/PVCs PersistentVolumeClaimResize Validating Allows PVC expansion RuntimeClass Validating Validates RuntimeClass references CertificateApproval Validating Validates certificate signing requests CertificateSigning Validating Signs certificates CertificateSubjectRestriction Validating Restricts certificate subjects"},{"location":"cluster-architecture/admission-controllers/#not-enabled-by-default","title":"NOT Enabled by Default","text":"<p>These require manual enablement via <code>--enable-admission-plugins</code>:</p> Controller Name Type Why Not Default Use Case AlwaysPullImages Mutating Performance impact Multi-tenant clusters NodeRestriction Validating May break some setups Restrict kubelet permissions ImagePolicyWebhook Validating Requires external service Image scanning/validation PodSecurityPolicy Validating Deprecated in 1.25+ Use PodSecurity instead DenyEscalatingExec Validating May break debugging Prevent exec into privileged pods EventRateLimit Validating Requires configuration Prevent event flooding ExtendedResourceToleration Mutating Specific use case GPU/FPGA tolerations PodNodeSelector Validating Namespace-specific Force node selection per namespace"},{"location":"cluster-architecture/admission-controllers/#7-how-to-list-admission-controllers","title":"7. How to List Admission Controllers","text":""},{"location":"cluster-architecture/admission-controllers/#method-1-check-currently-enabled-controllers","title":"Method 1: Check Currently Enabled Controllers","text":"<pre><code># Get the API server pod name\nkubectl get pod -n kube-system | grep kube-apiserver\n\n# View enabled admission plugins\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml | grep enable-admission-plugins\n</code></pre> <p>Example Output: <pre><code>--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,MutatingAdmissionWebhook,ValidatingAdmissionWebhook\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#method-2-check-static-pod-manifest-control-plane","title":"Method 2: Check Static Pod Manifest (Control Plane)","text":"<pre><code># SSH to control plane node\nssh controlplane\n\n# View the manifest\ncat /etc/kubernetes/manifests/kube-apiserver.yaml | grep enable-admission-plugins\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#method-3-list-all-available-admission-plugins","title":"Method 3: List ALL Available Admission Plugins","text":"<pre><code># This shows all admission plugins compiled into kube-apiserver\nkube-apiserver -h | grep -A 30 enable-admission-plugins\n</code></pre> <p>Example Output: <pre><code>--enable-admission-plugins strings\n      admission plugins that should be enabled in addition to default \n      enabled ones (NamespaceLifecycle, LimitRanger, ServiceAccount...).\n      Comma-delimited list of admission plugins: ..., AlwaysPullImages,\n      CertificateApproval, CertificateSigning, ...\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#method-4-describe-api-server-pod","title":"Method 4: Describe API Server Pod","text":"<pre><code># More detailed view\nkubectl -n kube-system describe pod kube-apiserver-&lt;node-name&gt; | grep -i admission\n</code></pre> <p>Example Output: <pre><code>--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount...\n--disable-admission-plugins=\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#method-5-check-which-plugins-are-actually-running","title":"Method 5: Check Which Plugins Are Actually Running","text":"<pre><code># Extract just the enabled plugins list\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o jsonpath='{.spec.containers[0].command}' | grep -o 'enable-admission-plugins=[^\"]*' | cut -d= -f2\n</code></pre> <p>Example Output: <pre><code>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#method-6-check-both-enabled-and-disabled","title":"Method 6: Check Both Enabled and Disabled","text":"<pre><code># See what's enabled\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml | grep enable-admission-plugins\n\n# See what's explicitly disabled\nkubectl -n kube-system get pod kube-apiserver-&lt;node-name&gt; -o yaml | grep disable-admission-plugins\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#8-viewing-admission-controllers-complete-reference","title":"8. Viewing Admission Controllers - Complete Reference","text":""},{"location":"cluster-architecture/admission-controllers/#quick-commands-table","title":"Quick Commands Table","text":"Task Command List enabled plugins <code>kubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml \\| grep enable-admission-plugins</code> List disabled plugins <code>kubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml \\| grep disable-admission-plugins</code> View manifest file <code>cat /etc/kubernetes/manifests/kube-apiserver.yaml \\| grep admission</code> List ALL available <code>kube-apiserver -h \\| grep -A 30 enable-admission-plugins</code> Extract enabled list <code>kubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o jsonpath='{.spec.containers[0].command}' \\| tr ',' '\\n' \\| grep -A 50 enable-admission-plugins</code>"},{"location":"cluster-architecture/admission-controllers/#formatted-output-examples","title":"Formatted Output Examples","text":"<p>Clean list of enabled controllers: <pre><code>kubectl -n kube-system get pod kube-apiserver-controlplane -o yaml | \\\n  grep enable-admission-plugins | \\\n  sed 's/.*enable-admission-plugins=//' | \\\n  tr ',' '\\n'\n</code></pre></p> <p>Output: <pre><code>NamespaceLifecycle\nLimitRanger\nServiceAccount\nDefaultStorageClass\nResourceQuota\nMutatingAdmissionWebhook\nValidatingAdmissionWebhook\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#9-default-controllers-by-kubernetes-version","title":"9. Default Controllers by Kubernetes Version","text":""},{"location":"cluster-architecture/admission-controllers/#kubernetes-128-current","title":"Kubernetes 1.28+ (Current)","text":"<p>Default Enabled: <pre><code>NamespaceLifecycle, LimitRanger, ServiceAccount, \nDefaultStorageClass, DefaultTolerationSeconds, \nMutatingAdmissionWebhook, ValidatingAdmissionWebhook,\nResourceQuota, Priority, TaintNodesByCondition, \nPodSecurity, StorageObjectInUseProtection,\nPersistentVolumeClaimResize, RuntimeClass,\nCertificateApproval, CertificateSigning, \nCertificateSubjectRestriction\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#kubernetes-123-127","title":"Kubernetes 1.23-1.27","text":"<p>Default Enabled: <pre><code>NamespaceLifecycle, LimitRanger, ServiceAccount,\nTaintNodesByCondition, Priority, DefaultTolerationSeconds,\nDefaultStorageClass, StorageObjectInUseProtection,\nPersistentVolumeClaimResize, RuntimeClass, \nCertificateApproval, CertificateSigning, \nCertificateSubjectRestriction, DefaultIngressClass,\nMutatingAdmissionWebhook, ValidatingAdmissionWebhook,\nResourceQuota, PodSecurity\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#changes-from-older-versions","title":"Changes from Older Versions","text":"Version Change 1.25+ <code>PodSecurityPolicy</code> deprecated 1.23+ <code>PodSecurity</code> enabled by default (replaces PSP) 1.21+ <code>PersistentVolumeClaimResize</code> enabled by default"},{"location":"cluster-architecture/admission-controllers/#10-enablingdisabling-admission-controllers","title":"10. Enabling/Disabling Admission Controllers","text":""},{"location":"cluster-architecture/admission-controllers/#configuration-file-location","title":"Configuration File Location","text":"<p><code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> (on control plane)</p>"},{"location":"cluster-architecture/admission-controllers/#enable-additional-controllers","title":"Enable Additional Controllers","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    command:\n    - kube-apiserver\n    - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,PodSecurityPolicy\n    #                                                                                                  \u2191\n    #                                                                          Added PodSecurityPolicy\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#disable-controllers","title":"Disable Controllers","text":"<pre><code>- --disable-admission-plugins=ServiceAccount,DefaultStorageClass\n</code></pre> <p>After modification: The kubelet will automatically restart the API server pod.</p>"},{"location":"cluster-architecture/admission-controllers/#8-visual-examples-admission-controllers-in-action","title":"8. Visual Examples: Admission Controllers in Action","text":""},{"location":"cluster-architecture/admission-controllers/#example-1-serviceaccount-mutating-controller","title":"Example 1: ServiceAccount Mutating Controller","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  USER SUBMITS POD (kubectl apply -f pod.yaml)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  ORIGINAL REQUEST                                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 apiVersion: v1                                             \u2502     \u2502\n\u2502  \u2502 kind: Pod                                                  \u2502     \u2502\n\u2502  \u2502 metadata:                                                  \u2502     \u2502\n\u2502  \u2502   name: nginx                                              \u2502     \u2502\n\u2502  \u2502 spec:                                                      \u2502     \u2502\n\u2502  \u2502   containers:                                              \u2502     \u2502\n\u2502  \u2502   - name: nginx                                            \u2502     \u2502\n\u2502  \u2502     image: nginx                                           \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SERVICEACCOUNT ADMISSION CONTROLLER (MUTATING)                      \u2502\n\u2502                                                                      \u2502\n\u2502  Action: Inject default ServiceAccount and token volume             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MODIFIED REQUEST (sent to etcd)                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 apiVersion: v1                                             \u2502     \u2502\n\u2502  \u2502 kind: Pod                                                  \u2502     \u2502\n\u2502  \u2502 metadata:                                                  \u2502     \u2502\n\u2502  \u2502   name: nginx                                              \u2502     \u2502\n\u2502  \u2502 spec:                                                      \u2502     \u2502\n\u2502  \u2502   serviceAccountName: default       \u2190 INJECTED!            \u2502     \u2502\n\u2502  \u2502   containers:                                              \u2502     \u2502\n\u2502  \u2502   - name: nginx                                            \u2502     \u2502\n\u2502  \u2502     image: nginx                                           \u2502     \u2502\n\u2502  \u2502     volumeMounts:                    \u2190 INJECTED!           \u2502     \u2502\n\u2502  \u2502     - name: kube-api-access-xxxxx                          \u2502     \u2502\n\u2502  \u2502       mountPath: /var/run/secrets/kubernetes.io/serviceaccount  \u2502\n\u2502  \u2502   volumes:                           \u2190 INJECTED!           \u2502     \u2502\n\u2502  \u2502   - name: kube-api-access-xxxxx                            \u2502     \u2502\n\u2502  \u2502     projected:                                             \u2502     \u2502\n\u2502  \u2502       sources:                                             \u2502     \u2502\n\u2502  \u2502       - serviceAccountToken:                               \u2502     \u2502\n\u2502  \u2502           path: token                                      \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n                         \u2705 PERSISTED TO ETCD\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#example-2-resourcequota-validating-controller","title":"Example 2: ResourceQuota Validating Controller","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CLUSTER STATE: ResourceQuota in 'prod' namespace                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 hard:                                                      \u2502     \u2502\n\u2502  \u2502   requests.cpu: \"10\"                                       \u2502     \u2502\n\u2502  \u2502   requests.memory: \"20Gi\"                                  \u2502     \u2502\n\u2502  \u2502                                                            \u2502     \u2502\n\u2502  \u2502 used:                                                      \u2502     \u2502\n\u2502  \u2502   requests.cpu: \"8\"                                        \u2502     \u2502\n\u2502  \u2502   requests.memory: \"12Gi\"                                  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                              \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  USER TRIES TO CREATE POD                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 spec:                                                      \u2502     \u2502\n\u2502  \u2502   containers:                                              \u2502     \u2502\n\u2502  \u2502   - name: app                                              \u2502     \u2502\n\u2502  \u2502     resources:                                             \u2502     \u2502\n\u2502  \u2502       requests:                                            \u2502     \u2502\n\u2502  \u2502         cpu: \"4\"                                           \u2502     \u2502\n\u2502  \u2502         memory: \"8Gi\"                                      \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                              \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  RESOURCEQUOTA ADMISSION CONTROLLER (VALIDATING)                     \u2502\n\u2502                                                                      \u2502\n\u2502  Check: Will this pod exceed quota?                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Current CPU usage:  8 cores                                \u2502     \u2502\n\u2502  \u2502 Requested:          +4 cores                               \u2502     \u2502\n\u2502  \u2502 Total if approved:  12 cores                               \u2502     \u2502\n\u2502  \u2502 Quota limit:        10 cores                               \u2502     \u2502\n\u2502  \u2502                                                            \u2502     \u2502\n\u2502  \u2502 12 &gt; 10  \u274c EXCEEDS QUOTA!                                 \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                              \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u274c REQUEST REJECTED                                                 \u2502\n\u2502                                                                      \u2502\n\u2502  Error from server (Forbidden): pods \"app\" is forbidden:             \u2502\n\u2502  exceeded quota: mem-cpu-quota, requested: requests.cpu=4,           \u2502\n\u2502  used: requests.cpu=8, limited: requests.cpu=10                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#example-3-multiple-controllers-working-together","title":"Example 3: Multiple Controllers Working Together","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  POD CREATION REQUEST                                                    \u2502\n\u2502  namespace: prod                                                         \u2502\n\u2502  cpu: 2                                                                  \u2502\n\u2502  memory: 4Gi                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MUTATING ADMISSION PHASE (Sequential)                                  \u2502\n\u2502                                                                          \u2502\n\u2502  Controller 1: NamespaceLifecycle                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502 Check: Does namespace 'prod' exist?                \u2502                 \u2502\n\u2502  \u2502 Result: \u2705 Yes, continue                            \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                   \u2193                                      \u2502\n\u2502  Controller 2: ServiceAccount                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502 Action: Inject serviceAccountName: default         \u2502                 \u2502\n\u2502  \u2502 Action: Add volume mount for SA token              \u2502                 \u2502\n\u2502  \u2502 Result: \u2705 Pod modified                             \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                   \u2193                                      \u2502\n\u2502  Controller 3: DefaultStorageClass                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502 Check: Does pod use PVCs?                          \u2502                 \u2502\n\u2502  \u2502 Result: \u2705 No, skip                                 \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATING ADMISSION PHASE (Parallel)                                  \u2502\n\u2502                                                                          \u2502\n\u2502  Controller 1: LimitRanger          Controller 2: ResourceQuota          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Check: Within limits?      \u2502   \u2502 Check: Within quota?         \u2502     \u2502\n\u2502  \u2502 cpu: 2 \u2264 4 (max)    \u2705     \u2502   \u2502 Current: 8 cores             \u2502     \u2502\n\u2502  \u2502 mem: 4Gi \u2264 8Gi (max) \u2705    \u2502   \u2502 Request: +2 cores            \u2502     \u2502\n\u2502  \u2502 Result: APPROVE            \u2502   \u2502 Total: 10 cores \u2264 10 \u2705      \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 Result: APPROVE              \u2502     \u2502\n\u2502                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502            \u2193                                     \u2193                       \u2502\n\u2502  Controller 3: PodSecurity          Controller 4: NodeRestriction       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Check: Privileged?         \u2502   \u2502 Check: Valid node selector?  \u2502     \u2502\n\u2502  \u2502 privileged: false    \u2705     \u2502   \u2502 Result: \u2705 Yes               \u2502     \u2502\n\u2502  \u2502 Result: APPROVE            \u2502   \u2502                              \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                          \u2502\n\u2502  ALL VALIDATING CONTROLLERS: \u2705 APPROVED                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u2705 FINAL RESULT: POD PERSISTED TO ETCD                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#example-4-rejection-flow","title":"Example 4: Rejection Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  USER TRIES TO CREATE POD IN NON-EXISTENT NAMESPACE                 \u2502\n\u2502                                                                      \u2502\n\u2502  kubectl run nginx --image=nginx -n does-not-exist                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHENTICATION: \u2705 User authenticated                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHORIZATION: \u2705 User has pod:create permission                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MUTATING ADMISSION: NamespaceLifecycle                             \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Check: Does namespace 'does-not-exist' exist?             \u2502     \u2502\n\u2502  \u2502                                                            \u2502     \u2502\n\u2502  \u2502 Query etcd for namespace: does-not-exist                  \u2502     \u2502\n\u2502  \u2502 Result: \u274c NOT FOUND                                       \u2502     \u2502\n\u2502  \u2502                                                            \u2502     \u2502\n\u2502  \u2502 Action: REJECT REQUEST                                    \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u274c ERROR RETURNED TO USER                                           \u2502\n\u2502                                                                      \u2502\n\u2502  Error from server (NotFound): namespaces \"does-not-exist\" not found \u2502\n\u2502                                                                      \u2502\n\u2502  Pod was NEVER created. Request stopped at admission control.       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#decision-flow-how-admission-controllers-decide","title":"Decision Flow: How Admission Controllers Decide","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  For Each Admission Controller                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502  Is it enabled?          \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193           \u2193\n               NO \u2500\u2500\u2518           \u2514\u2500\u2500 YES\n                \u2502                    \u2502\n                \u2502                    \u2193\n                \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502      \u2502 Type: Mutating?         \u2502\n                \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502              \u2193           \u2193\n                \u2502         YES \u2500\u2500\u2518           \u2514\u2500\u2500 NO (Validating)\n                \u2502          \u2502                     \u2502\n                \u2502          \u2193                     \u2193\n                \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   \u2502  Modify      \u2502     \u2502  Validate    \u2502\n                \u2502   \u2502  Request     \u2502     \u2502  Request     \u2502\n                \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502          \u2502                     \u2502\n                \u2502          \u2193                     \u2193\n                \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   \u2502 Continue to  \u2502     \u2502  Pass/Fail?  \u2502\n                \u2502   \u2502 next mutator \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n                \u2502          \u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502            \u2193                \u2193\n                \u2502          \u2502         PASS             FAIL\n                \u2502          \u2502            \u2502                \u2502\n                \u2502          \u2502            \u2193                \u2193\n                \u2502          \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502    \u2502 Continue to  \u2502  \u2502  REJECT  \u2502\n                \u2502          \u2502    \u2502 next         \u2502  \u2502  REQUEST \u2502\n                \u2502          \u2502    \u2502 validator    \u2502  \u2502          \u2502\n                \u2502          \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502          \u2502            \u2502                \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n                           \u2502                             \u2502\n                           \u2193                             \u2193\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         User gets error message\n                  \u2502  All controllers \u2502\n                  \u2502  passed?         \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2193             \u2193\n                  YES            NO\n                    \u2502             \u2502\n                    \u2193             \u2193\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 PERSIST      \u2502  \u2502 REJECT \u2502\n            \u2502 TO ETCD      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u2193\n              \u2705 SUCCESS\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#9-real-world-examples","title":"9. Real-World Examples","text":""},{"location":"cluster-architecture/admission-controllers/#example-1-serviceaccount-injection","title":"Example 1: ServiceAccount Injection","text":"<p>What you submit: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre></p> <p>What gets stored (after ServiceAccount admission controller): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  serviceAccountName: default    # \u2190 Injected!\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: kube-api-access-xxxxx\n      mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n  volumes:\n  - name: kube-api-access-xxxxx\n    projected:\n      sources:\n      - serviceAccountToken:\n          path: token\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#example-2-resourcequota-validation","title":"Example 2: ResourceQuota Validation","text":"<p>Cluster has a quota: <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: mem-cpu-quota\n  namespace: prod\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"20Gi\"\n</code></pre></p> <p>You try to create a pod: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: big-app\n  namespace: prod\nspec:\n  containers:\n  - name: app\n    resources:\n      requests:\n        cpu: \"12\"     # \u2190 Exceeds quota!\n        memory: \"8Gi\"\n</code></pre></p> <p>Result: <pre><code>Error from server (Forbidden): pods \"big-app\" is forbidden: \nexceeded quota: mem-cpu-quota, requested: requests.cpu=12, \nused: requests.cpu=8, limited: requests.cpu=10\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#example-3-namespacelifecycle-protection","title":"Example 3: NamespaceLifecycle Protection","text":"<p>Try to create a pod in a non-existent namespace: <pre><code>kubectl run nginx --image=nginx -n does-not-exist\n</code></pre></p> <p>Result: <pre><code>Error from server (NotFound): namespaces \"does-not-exist\" not found\n</code></pre></p> <p>The NamespaceLifecycle admission controller rejected the request.</p>"},{"location":"cluster-architecture/admission-controllers/#9-custom-admission-controllers-webhooks","title":"9. Custom Admission Controllers (Webhooks)","text":"<p>For custom logic, use Admission Webhooks instead of modifying the API server.</p>"},{"location":"cluster-architecture/admission-controllers/#mutatingwebhookconfiguration","title":"MutatingWebhookConfiguration","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: inject-sidecar\nwebhooks:\n- name: sidecar.example.com\n  clientConfig:\n    service:\n      name: webhook-service\n      namespace: default\n      path: /mutate\n  rules:\n  - operations: [\"CREATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#validatingwebhookconfiguration","title":"ValidatingWebhookConfiguration","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: validate-images\nwebhooks:\n- name: images.example.com\n  clientConfig:\n    service:\n      name: webhook-service\n      namespace: default\n      path: /validate\n  rules:\n  - operations: [\"CREATE\", \"UPDATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#10-troubleshooting-admission-controller-issues","title":"10. Troubleshooting Admission Controller Issues","text":""},{"location":"cluster-architecture/admission-controllers/#issue-1-pods-not-being-created","title":"Issue 1: Pods Not Being Created","text":"<p>Symptom: <pre><code>kubectl run nginx --image=nginx\nError from server: admission webhook \"validate-pods\" denied the request\n</code></pre></p> <p>Debug: <pre><code># Check which admission controllers are enabled\nkubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml | grep enable-admission-plugins\n\n# Check webhook configurations\nkubectl get mutatingwebhookconfigurations\nkubectl get validatingwebhookconfigurations\n\n# Describe the webhook\nkubectl describe validatingwebhookconfiguration &lt;name&gt;\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#issue-2-serviceaccount-not-injected","title":"Issue 2: ServiceAccount Not Injected","text":"<p>Check if ServiceAccount admission controller is enabled: <pre><code>grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre></p> <p>Should include <code>ServiceAccount</code>.</p>"},{"location":"cluster-architecture/admission-controllers/#issue-3-resourcequota-not-enforced","title":"Issue 3: ResourceQuota Not Enforced","text":"<p>Check if ResourceQuota admission controller is enabled: <pre><code>grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre></p> <p>Should include <code>ResourceQuota</code>.</p>"},{"location":"cluster-architecture/admission-controllers/#11-cka-exam-tips","title":"11. CKA Exam Tips","text":""},{"location":"cluster-architecture/admission-controllers/#common-exam-tasks","title":"Common Exam Tasks","text":"<p>Task: \"Enable PodSecurityPolicy admission controller\"</p> <pre><code># SSH to control plane\nssh controlplane\n\n# Edit the API server manifest\nvi /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Find the line: <pre><code>--enable-admission-plugins=...\n</code></pre></p> <p>Add <code>PodSecurityPolicy</code>: <pre><code>--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,PodSecurityPolicy\n</code></pre></p> <p>Save and wait for API server to restart (~30 seconds).</p> <p>Task: \"Check which admission controllers are enabled\"</p> <pre><code>kubectl -n kube-system describe pod kube-apiserver-&lt;node-name&gt; | grep enable-admission-plugins\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#12-quick-reference","title":"12. Quick Reference","text":""},{"location":"cluster-architecture/admission-controllers/#check-enabled-controllers","title":"Check Enabled Controllers","text":"<pre><code># Method 1: Via kubectl\nkubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml | grep enable-admission-plugins\n\n# Method 2: Via static manifest\ncat /etc/kubernetes/manifests/kube-apiserver.yaml | grep enable-admission-plugins\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#enable-a-controller","title":"Enable a Controller","text":"<p>Edit <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>: <pre><code>--enable-admission-plugins=NamespaceLifecycle,ServiceAccount,NewController\n</code></pre></p>"},{"location":"cluster-architecture/admission-controllers/#disable-a-controller","title":"Disable a Controller","text":"<pre><code>--disable-admission-plugins=ServiceAccount\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#list-all-available-controllers","title":"List All Available Controllers","text":"<pre><code>kube-apiserver -h | grep admission-plugins\n</code></pre>"},{"location":"cluster-architecture/admission-controllers/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 Admission controllers run after authentication/authorization, before persistence \u2705 Mutating controllers modify requests, Validating controllers approve/reject \u2705 Mutating controllers run before validating controllers \u2705 Configured via <code>--enable-admission-plugins</code> flag in kube-apiserver \u2705 Built-in controllers are compiled into kube-apiserver \u2705 Custom logic uses Webhooks (MutatingWebhookConfiguration, ValidatingWebhookConfiguration) \u2705 Common controllers: ServiceAccount, ResourceQuota, LimitRanger, NamespaceLifecycle \u2705 Changes require editing <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> and waiting for restart  </p>"},{"location":"cluster-architecture/admission-controllers/#essential-controllers-for-cka","title":"Essential Controllers for CKA","text":"Controller Must Know? Why? NamespaceLifecycle \u2705 Prevents namespace errors ServiceAccount \u2705 Automatically injects SA tokens ResourceQuota \u2705 Enforces quotas (exam topic) LimitRanger \u2705 Enforces limits (exam topic) MutatingAdmissionWebhook \ud83d\udfe1 Know the concept ValidatingAdmissionWebhook \ud83d\udfe1 Know the concept"},{"location":"cluster-architecture/admission-controllers/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Official Kubernetes Documentation: Admission Controllers   Complete reference for all admission controllers</p> </li> <li> <p>A Guide to Kubernetes Admission Controllers   Official blog post explaining admission controller concepts</p> </li> <li> <p>Dynamic Admission Control   How to build custom admission webhooks</p> </li> </ul>"},{"location":"cluster-architecture/component-management/","title":"Component Management: Static Pods vs. Systemd","text":"<p>Understanding how Kubernetes components are run is the single most common stumbling block for CKA students.</p> <p>Kubernetes components are run in one of two ways: 1.  Systemd Services (Native Binary) 2.  Static Pods (Containerized)</p>"},{"location":"cluster-architecture/component-management/#1-the-kubelet-exception","title":"1. The \"Kubelet\" Exception","text":"<p>The Kubelet is unique. It is the only component that is ALWAYS run as a binary (Systemd Service).</p>"},{"location":"cluster-architecture/component-management/#why-the-captain-analogy","title":"Why? (The \"Captain\" Analogy)","text":"<p>Imagine a ship (the Node). *   The Containers (Static Pods) are the cargo. *   The Kubelet is the Captain.</p> <p>The Captain cannot be cargo. The Kubelet's job is to talk to the Container Runtime (Docker/Containerd) and say \"Start this container.\" If the Kubelet was a container itself, who would start it? It's a chicken-and-egg problem.</p> <p>Therefore, the Kubelet must always be installed on the Operating System <code>(apt-get install kubelet)</code> and managed by the OS init system <code>(systemctl start kubelet)</code>.</p>"},{"location":"cluster-architecture/component-management/#2-static-pods-the-mirror-concept","title":"2. Static Pods (The \"Mirror\" Concept)","text":"<p>Once the Kubelet (Captain) is running, it can start other components (API Server, ETCD, Scheduler) as Containers.</p>"},{"location":"cluster-architecture/component-management/#what-is-a-static-pod","title":"What is a Static Pod?","text":"<p>A Static Pod is a pod managed directly by the Kubelet on a specific node, without the API Server observing it initially.</p> <ul> <li>Normal Pod: API Server -&gt; Scheduler -&gt; Kubelet -&gt; Run Pod.</li> <li>Static Pod: Kubelet -&gt; Reads File -&gt; Run Pod.</li> </ul>"},{"location":"cluster-architecture/component-management/#how-it-works","title":"How it works","text":"<ol> <li>Configuration: The Kubelet config file (<code>/var/lib/kubelet/config.yaml</code>) has a setting: <code>staticPodPath:  /etc/kubernetes/manifests</code>.</li> <li>The Watch: The Kubelet watches that folder constantly.</li> <li>The Action:<ul> <li>If you put a file <code>etcd.yaml</code> there -&gt; Kubelet starts the pod.</li> <li>If you delete the file -&gt; Kubelet kills the pod.</li> </ul> </li> <li>The Mirror: The Kubelet creates a \"Mirror Pod\" on the API Server so you can see it with <code>kubectl get pods</code>, but you cannot edit it via kubectl. It is read-only in the API.</li> </ol> <p>Deep Dive</p> <p>For a complete guide on Static Pods (Creation, Use Cases, Master-Down scenarios), see the Static Pods Fundamentals guide.</p>"},{"location":"cluster-architecture/component-management/#3-topologies-summary-cka-cheat-sheet","title":"3. Topologies Summary (CKA Cheat Sheet)","text":"Component Run As Location of Config managed By Kubelet Systemd Service <code>/var/lib/kubelet/config.yaml</code> <code>systemctl</code> Etcd Static Pod <code>/etc/kubernetes/manifests/etcd.yaml</code> Kubelet (File edit) API Server Static Pod <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> Kubelet (File edit) Scheduler Static Pod <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code> Kubelet (File edit) Controller Mgr Static Pod <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> Kubelet (File edit) Kube-Proxy DaemonSet ConfigMap/API Kubernetes (API)"},{"location":"cluster-architecture/component-management/#4-the-kube-proxy-exception-daemonset","title":"4. The Kube-Proxy Exception (DaemonSet)","text":"<p>You might notice <code>kube-proxy</code> is missing from the Static Pod folder.</p> <p>It is NOT a Static Pod. *   Why? Static Pods are for the \"Brain\" (Control Plane) that creates the cluster. <code>kube-proxy</code> is a worker Process that runs on every node (even workers). *   How it runs: It is run as a DaemonSet.     *   This means the Scheduler schedules it.     *   You manage it via <code>kubectl edit daemonset kube-proxy -n kube-system</code>.     *   It reads its configuration from a ConfigMap (not a local file).</p> <p>Why this matters: If you try to find <code>/etc/kubernetes/manifests/kube-proxy.yaml</code>, it won't be there. You must use <code>kubectl</code> to configure it.</p>"},{"location":"cluster-architecture/etcd-data-model/","title":"ETCD Data Model &amp; Architecture","text":"<p>Understanding how ETCD differs from traditional databases is key to understanding Kubernetes performance and scalability constraints.</p>"},{"location":"cluster-architecture/etcd-data-model/#1-structured-vs-semi-structured-data","title":"1. Structured vs. Semi-Structured Data","text":"<p>Since ETCD is a key-value store, it is often categorized as a NoSQL database.</p>"},{"location":"cluster-architecture/etcd-data-model/#structured-data-the-excel-sheet-approach","title":"Structured Data (The \"Excel Sheet\" Approach)","text":"<p>Think of this like a rigid filing cabinet or an Excel spreadsheet. *   Format: Tables with fixed rows and columns. *   Rules: Every row must have the same columns. If you have a \"Users\" table, every user must have a field for \"Age\" even if it's empty. *   Examples: MySQL, PostgreSQL, Oracle. *   Analogy: A phone book. Every entry has exactly: Last Name, First Name, Phone Number.</p>"},{"location":"cluster-architecture/etcd-data-model/#semi-structured-data-the-sticky-note-approach","title":"Semi-Structured Data (The \"Sticky Note\" Approach)","text":"<p>Think of this like a flexible folder where you can throw in different documents. *   Format: Data that has some organizational properties (like tags or markers) but doesn't conform to a rigid table. *   Rules: Self-describing. One item might have \"Name\" and \"Email\", while the next item has \"Name\" and \"Twitter Handle\". It handles hierarchy and nesting well. *   Examples: JSON, XML, YAML, NoSQL databases (MongoDB, Cassandra, Redis). *   Analogy: A resume. Everyone's resume has \"Experience\" and \"Education\", but the format, length, and specific bullet points vary wildly.</p>"},{"location":"cluster-architecture/etcd-data-model/#how-this-applies-to-kubernetes-etcd","title":"How this applies to Kubernetes &amp; ETCD","text":"<p>ETCD is a Key-Value Store (Semi-Structured / NoSQL). It does not have tables or foreign keys. It's just a giant map of <code>Key</code> -&gt; <code>Value</code>.</p> <ol> <li>The Key: A simple string (like a file path).<ul> <li>Example: <code>/registry/pods/default/nginx</code></li> </ul> </li> <li>The Value: A blob of data.<ul> <li>ETCD doesn't care what is inside the value. It just stores bytes.</li> </ul> </li> </ol> <p>However... Kubernetes cheats. Even though ETCD is \"semi-structured\" (it lets you store anything), Kubernetes is extremely strict. It forces the data inside that value to be highly structured JSON or Protobuf.</p> <p>So, when you see a Kubernetes manifest, you are looking at Structured Data (strict schema) stored inside a Semi-Structured Database (ETCD).</p>"},{"location":"cluster-architecture/etcd-data-model/#summary","title":"Summary","text":"<p>In short, Kubernetes utilizes the best of both worlds: 1.  Reliability of ETCD: Uses ETCD's semi-structured key-value store to ensure high availability and consistency across the cluster. 2.  Strictness of API: Imposes strict structure (YAML/JSON schemas) on top of the raw data to ensure validity before it ever reaches the database.</p>"},{"location":"cluster-architecture/etcd-data-model/#2-why-etcd-is-not-used-as-a-regular-database","title":"2. Why ETCD is not used as a \"Regular\" Database","text":"<p>ETCD is used by specific distributed systems (OpenStack, Rook, Patroni) but rarely for standard web applications.</p>"},{"location":"cluster-architecture/etcd-data-model/#1-it-is-small-by-design","title":"1. It is Small (By Design)","text":"<ul> <li>Storage Limit: <code>etcd</code> has a default hard limit of 2 GB (max recommended is ~8 GB).</li> <li>Regular DB: Postgres or MongoDB can easily handle Terabytes of data.</li> <li>Why? <code>etcd</code> loads all keys into RAM for speed. It is built to store metadata (configuration), not data (user profiles, transaction history, images).</li> </ul>"},{"location":"cluster-architecture/etcd-data-model/#2-it-prioritizes-correctness-over-speed","title":"2. It Prioritizes \"Correctness\" over \"Speed\"","text":"<ul> <li>The Consensus Tax: <code>etcd</code> uses the Raft consensus algorithm.</li> <li>How it works: Every time you write a single key, <code>etcd</code> has to talk to the other nodes in the cluster, get a majority vote, and write it to disk on all of them before it tells you \"Success\".</li> <li>Result: This makes writes significantly slower than Redis or MySQL. It is great for keeping 3 servers in sync, but terrible for ingesting 10,000 user clicks per second.</li> </ul>"},{"location":"cluster-architecture/etcd-data-model/#3-no-query-language-no-sql","title":"3. No Query Language (No SQL)","text":"<ul> <li>Regular DB: <code>SELECT * FROM Users WHERE age &gt; 25 AND city = 'NY'</code></li> <li>ETCD: You can only ask: \"Give me key <code>/users/1</code>\" or \"Give me all keys starting with <code>/users/</code>\".</li> <li>Pain Point: If you want to filter data, you have to download everything to your app and filter it there. That is inefficient for large datasets.</li> </ul>"},{"location":"cluster-architecture/etcd-data-model/#4-comparison-summary","title":"4. Comparison Summary","text":"Feature Redis (Cache) Postgres (App DB) ETCD (Control Plane) Primary Goal Extreme Speed Complex Queries &amp; Relationships Consistency &amp; Reliability Max Size RAM Limit (GBs) Disk Limit (TBs) ~8 GB Write Speed Microseconds Milliseconds Slower (Raft Consistency) Querying Key-Value / Simple SQL (Joins, Filters) Key-Value / Watch"},{"location":"cluster-architecture/etcd-data-model/#when-should-you-use-etcd","title":"When should you use ETCD?","text":"<p>You use it when you have a distributed system and you need a \"Single Source of Truth\" that never lies. *   \"Which server is the master?\" (Leader Election) *   \"What is the current config?\" (Distributed Configuration) *   \"Who owns this job?\" (Distributed Locking)</p>"},{"location":"cluster-architecture/kube-proxy-deep-dive/","title":"Kube-Proxy Deep Dive","text":"<p>Kube-Proxy is the network \"plumber\" of your Kubernetes cluster. It runs on every single node and ensures that \"Services\" (like ClusterIP) actually work.</p> <p>Without Kube-Proxy, <code>Service</code> IPs (e.g., <code>10.96.0.10</code>) would just be dead IP addresses that route nowhere.</p>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#1-how-it-runs-the-architecture","title":"1. How it Runs (The Architecture)","text":""},{"location":"cluster-architecture/kube-proxy-deep-dive/#kubeadm-standard-clusters","title":"Kubeadm / Standard Clusters","text":"<p>In 99% of modern clusters (and the CKA exam), Kube-Proxy runs as a DaemonSet.</p> <ul> <li>Type: DaemonSet (Ensures exactly one pod runs on every node).</li> <li>Namespace: <code>kube-system</code></li> <li>Process: It talks to the API server to watch for new Services and Endpoints.</li> </ul> <p>To see it: <pre><code>kubectl get daemonset -n kube-system kube-proxy\n</code></pre></p>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#the-hard-way-legacy","title":"\"The Hard Way\" / Legacy","text":"<p>In manual binary installations, <code>kube-proxy</code> is installed as a Systemd Service (binary) directly on the worker node OS, just like the Kubelet.</p>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#2-configuration-where-is-the-config-file","title":"2. Configuration (Where is the config file?)","text":"<p>This is where Kube-Proxy differs from Static Pods.</p> <ul> <li>Static Pods (Scheduler/API): Config is a local file on the node (<code>/etc/kubernetes/...</code>).</li> <li>Kube-Proxy: Config is a ConfigMap inside Kubernetes.</li> </ul>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#finding-the-config","title":"Finding the Config","text":"<p>You cannot just SSH into a node and edit a file to change Kube-Proxy settings. You must edit the ConfigMap object.</p> <pre><code># View the config\nkubectl -n kube-system get cm kube-proxy -o yaml\n\n# Edit the config (e.g., to change mode from iptables to ipvs)\nkubectl -n kube-system edit cm kube-proxy\n</code></pre> <p>CRITICAL STEP: After editing the ConfigMap, the Pods will not update automatically. You must restart the DaemonSet pods to pick up the changes:</p> <pre><code>kubectl -n kube-system rollout restart daemonset kube-proxy\n</code></pre>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#3-operations-what-does-it-actually-do","title":"3. Operations: What does it actually do?","text":"<p>Kube-Proxy watches the API Server for Services and Endpoints. When you create a Service, Kube-Proxy wakes up and writes network rules on the Node's kernel.</p>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#the-modes-implementing-the-magic","title":"The Modes (Implementing the Magic)","text":"<ol> <li> <p>IPTables Mode (Default):</p> <ul> <li>Kube-Proxy writes standard Linux <code>iptables</code> rules.</li> <li>Traffic to the Service IP is intercepted by the kernel and DNAT'ed (Forwarded) to a random Pod IP.</li> <li>Pros: Universal, mature.</li> <li>Cons: Slow at massive scale (5,000+ services).</li> </ul> </li> <li> <p>IPVS Mode (High Performance):</p> <ul> <li>Uses the Linux Kernel's IP Virtual Server (L4 Load Balancer).</li> <li>Uses hash tables instead of linear lists.</li> <li>Pros: Much faster for large clusters.</li> <li>Cons: Requires extra kernel modules to be loaded on the OS.</li> </ul> </li> </ol>"},{"location":"cluster-architecture/kube-proxy-deep-dive/#4-cheat-sheet-summary","title":"4. Cheat Sheet Summary","text":"Feature Details Run As DaemonSet (Namespace: <code>kube-system</code>) Managed By Kubernetes Deployment Controller (DaemonSet controller) Configuration <code>ConfigMap: kube-proxy</code> Logs <code>kubectl logs -n kube-system -l k8s-app=kube-proxy</code> Core Job Translates Service VIPs -&gt; Pod IPs using iptables/ipvs"},{"location":"cluster-architecture/mutating-admission-controllers/","title":"Mutating Admission Controllers in Kubernetes","text":"<p>Mutating Admission Controllers are plugins that intercept and modify API requests before they are persisted to etcd. They can add, remove, or change fields in resource specifications.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#1-the-security-screening-with-baggage-tagging-analogy","title":"1. The \"Security Screening with Baggage Tagging\" Analogy","text":"<p>Think of mutating admission controllers like airport security that modifies your luggage:</p> <pre><code>You arrive at airport with a bag\n         \u2193\nSecurity Checkpoint (Authentication) \u2705\n         \u2193\nBoarding Pass Check (Authorization) \u2705\n         \u2193\nMutating Security:\n  - Adds a baggage tag with tracking number\n  - Inserts a security inspection notice\n  - Attaches a fragile sticker (if needed)\n         \u2193\nYour bag is now \"mutated\" with additional items\n         \u2193\nValidating Security:\n  - Checks if bag meets weight limit\n  - Verifies no prohibited items\n         \u2193\nBag is loaded onto plane (etcd)\n</code></pre> <p>Your pod goes through the same process! It gets \"tags\" (labels, annotations, volumes) added automatically.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#2-what-are-mutating-admission-controllers","title":"2. What Are Mutating Admission Controllers?","text":"<p>Mutating admission controllers intercept API requests and modify them before validation and persistence.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#key-characteristics","title":"Key Characteristics","text":"Aspect Description When They Run After authentication/authorization, BEFORE validating admission What They Do Add, remove, or modify fields in the request Execution Order Sequential (one after another) Can Reject? Yes, if mutation fails Examples ServiceAccount injection, DefaultStorageClass assignment"},{"location":"cluster-architecture/mutating-admission-controllers/#3-the-mutation-pipeline","title":"3. The Mutation Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CLIENT SUBMITS REQUEST                                         \u2502\n\u2502  kubectl apply -f pod.yaml                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHENTICATION \u2705                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AUTHORIZATION \u2705                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MUTATING ADMISSION CONTROLLERS (Sequential)                    \u2502\n\u2502                                                                 \u2502\n\u2502  Original Request:                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 apiVersion: v1                             \u2502                \u2502\n\u2502  \u2502 kind: Pod                                  \u2502                \u2502\n\u2502  \u2502 metadata:                                  \u2502                \u2502\n\u2502  \u2502   name: nginx                              \u2502                \u2502\n\u2502  \u2502 spec:                                      \u2502                \u2502\n\u2502  \u2502   containers:                              \u2502                \u2502\n\u2502  \u2502   - name: nginx                            \u2502                \u2502\n\u2502  \u2502     image: nginx                           \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                                 \u2502\n\u2502  Controller 1: ServiceAccount                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 ACTION: Add serviceAccountName             \u2502                \u2502\n\u2502  \u2502 ACTION: Add volume for SA token            \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                     \u2193                                           \u2502\n\u2502  Modified Request:                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 spec:                                      \u2502                \u2502\n\u2502  \u2502   serviceAccountName: default  \u2190 ADDED     \u2502                \u2502\n\u2502  \u2502   volumes:                                 \u2502                \u2502\n\u2502  \u2502   - name: sa-token             \u2190 ADDED     \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                                 \u2502\n\u2502  Controller 2: DefaultTolerationSeconds                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 ACTION: Add default tolerations            \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                     \u2193                                           \u2502\n\u2502  Modified Request:                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 spec:                                      \u2502                \u2502\n\u2502  \u2502   tolerations:                 \u2190 ADDED     \u2502                \u2502\n\u2502  \u2502   - key: node.kubernetes.io/not-ready      \u2502                \u2502\n\u2502  \u2502     operator: Exists                       \u2502                \u2502\n\u2502  \u2502     effect: NoExecute                      \u2502                \u2502\n\u2502  \u2502     tolerationSeconds: 300                 \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                                 \u2502\n\u2502  Controller 3: MutatingWebhook                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 ACTION: Call external webhook              \u2502                \u2502\n\u2502  \u2502 ACTION: Add sidecar container              \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                     \u2193                                           \u2502\n\u2502  Final Mutated Request:                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 spec:                                      \u2502                \u2502\n\u2502  \u2502   serviceAccountName: default              \u2502                \u2502\n\u2502  \u2502   containers:                              \u2502                \u2502\n\u2502  \u2502   - name: nginx                            \u2502                \u2502\n\u2502  \u2502     image: nginx                           \u2502                \u2502\n\u2502  \u2502   - name: istio-proxy      \u2190 ADDED         \u2502                \u2502\n\u2502  \u2502     image: istio/proxyv2                   \u2502                \u2502\n\u2502  \u2502   volumes: [...]                           \u2502                \u2502\n\u2502  \u2502   tolerations: [...]                       \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATING ADMISSION CONTROLLERS                               \u2502\n\u2502  (Validate the mutated request)                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PERSIST TO ETCD \u2705                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#4-built-in-mutating-admission-controllers","title":"4. Built-in Mutating Admission Controllers","text":""},{"location":"cluster-architecture/mutating-admission-controllers/#serviceaccount-most-common","title":"ServiceAccount (Most Common)","text":"<p>Purpose: Automatically injects ServiceAccount credentials into pods.</p> <p>What it adds: - <code>spec.serviceAccountName: default</code> (if not specified) - Volume mount for ServiceAccount token - Projected volume with token, CA cert, and namespace</p> <p>Before: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre></p> <p>After ServiceAccount mutation: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  serviceAccountName: default                    # \u2190 ADDED\n  automountServiceAccountToken: true             # \u2190 ADDED\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:                                # \u2190 ADDED\n    - name: kube-api-access-xxxxx\n      mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      readOnly: true\n  volumes:                                       # \u2190 ADDED\n  - name: kube-api-access-xxxxx\n    projected:\n      sources:\n      - serviceAccountToken:\n          path: token\n          expirationSeconds: 3607\n      - configMap:\n          name: kube-root-ca.crt\n          items:\n          - key: ca.crt\n            path: ca.crt\n      - downwardAPI:\n          items:\n          - path: namespace\n            fieldRef:\n              fieldPath: metadata.namespace\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#defaultstorageclass","title":"DefaultStorageClass","text":"<p>Purpose: Assigns the default StorageClass to PVCs that don't specify one.</p> <p>Before: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ReadWriteOnce]\n  resources:\n    requests:\n      storage: 1Gi\n  # No storageClassName specified\n</code></pre></p> <p>After DefaultStorageClass mutation: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ReadWriteOnce]\n  storageClassName: gp2             # \u2190 ADDED (default storage class)\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#defaulttolerationseconds","title":"DefaultTolerationSeconds","text":"<p>Purpose: Adds default tolerations for <code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code> taints.</p> <p>Before: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp\n</code></pre></p> <p>After DefaultTolerationSeconds mutation: <pre><code>spec:\n  tolerations:                                   # \u2190 ADDED\n  - key: node.kubernetes.io/not-ready\n    operator: Exists\n    effect: NoExecute\n    tolerationSeconds: 300\n  - key: node.kubernetes.io/unreachable\n    operator: Exists\n    effect: NoExecute\n    tolerationSeconds: 300\n  containers:\n  - name: app\n    image: myapp\n</code></pre></p> <p>Why? Gives pods 5 minutes (300 seconds) to be evicted when a node becomes not-ready.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#podpreset-deprecated","title":"PodPreset (Deprecated)","text":"<p>Purpose: Inject environment variables, volumes, and other config into pods matching a label selector.</p> <p>Status: \u274c Removed in Kubernetes 1.20+ (use MutatingWebhooks instead)</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#5-custom-mutating-admission-webhooks","title":"5. Custom Mutating Admission Webhooks","text":"<p>For custom mutation logic, use MutatingAdmissionWebhook.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. User creates a Pod                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. API Server calls MutatingWebhook                            \u2502\n\u2502     POST /mutate HTTP/1.1                                       \u2502\n\u2502     {                                                           \u2502\n\u2502       \"request\": {                                              \u2502\n\u2502         \"object\": { /* Pod YAML */ }                            \u2502\n\u2502       }                                                         \u2502\n\u2502     }                                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Your Webhook Service processes request                      \u2502\n\u2502     - Analyzes the pod spec                                     \u2502\n\u2502     - Decides what to add/modify                                \u2502\n\u2502     - Creates a JSON Patch                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Webhook returns AdmissionReview response                    \u2502\n\u2502     {                                                           \u2502\n\u2502       \"response\": {                                             \u2502\n\u2502         \"allowed\": true,                                        \u2502\n\u2502         \"patchType\": \"JSONPatch\",                               \u2502\n\u2502         \"patch\": \"base64-encoded-json-patch\"                    \u2502\n\u2502       }                                                         \u2502\n\u2502     }                                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  5. API Server applies the patch                                \u2502\n\u2502     Original Pod + JSON Patch = Mutated Pod                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  6. Continue to Validating Admission                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#example-sidecar-injector-webhook","title":"Example: Sidecar Injector Webhook","text":"<p>MutatingWebhookConfiguration:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: sidecar-injector\nwebhooks:\n- name: sidecar.example.com\n  clientConfig:\n    service:\n      name: sidecar-injector\n      namespace: default\n      path: /mutate\n    caBundle: LS0tLS1CRUdJTi... # Base64 encoded CA cert\n  rules:\n  - operations: [\"CREATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n  admissionReviewVersions: [\"v1\"]\n  sideEffects: None\n  failurePolicy: Fail  # Reject if webhook fails\n</code></pre> <p>Webhook Service Logic (Pseudocode):</p> <pre><code>@app.route('/mutate', methods=['POST'])\ndef mutate():\n    request = flask.request.json\n    pod = request['request']['object']\n\n    # Check if pod needs sidecar\n    if pod.get('metadata', {}).get('annotations', {}).get('inject-sidecar') == 'true':\n        # Create JSON Patch to add sidecar container\n        patch = [\n            {\n                \"op\": \"add\",\n                \"path\": \"/spec/containers/-\",\n                \"value\": {\n                    \"name\": \"sidecar\",\n                    \"image\": \"sidecar:v1\",\n                    \"ports\": [{\"containerPort\": 8080}]\n                }\n            }\n        ]\n\n        response = {\n            \"response\": {\n                \"uid\": request['request']['uid'],\n                \"allowed\": True,\n                \"patchType\": \"JSONPatch\",\n                \"patch\": base64.b64encode(json.dumps(patch).encode()).decode()\n            }\n        }\n    else:\n        # No mutation needed\n        response = {\n            \"response\": {\n                \"uid\": request['request']['uid'],\n                \"allowed\": True\n            }\n        }\n\n    return json.dumps(response)\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#6-real-world-use-cases","title":"6. Real-World Use Cases","text":""},{"location":"cluster-architecture/mutating-admission-controllers/#use-case-1-istio-service-mesh-sidecar-injection","title":"Use Case 1: Istio Service Mesh Sidecar Injection","text":"<p>Problem: Every pod in the service mesh needs an Envoy proxy sidecar.</p> <p>Solution: Mutating webhook automatically injects the <code>istio-proxy</code> container.</p> <p>Before: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp\n    image: myapp:latest\n</code></pre></p> <p>After Istio mutation: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\n  annotations:\n    sidecar.istio.io/status: '{\"version\":\"...\"}'  # \u2190 ADDED\nspec:\n  initContainers:                                 # \u2190 ADDED\n  - name: istio-init\n    image: istio/proxyv2\n  containers:\n  - name: myapp\n    image: myapp:latest\n  - name: istio-proxy                             # \u2190 ADDED\n    image: istio/proxyv2\n    ports:\n    - containerPort: 15001\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#use-case-2-vault-agent-injector","title":"Use Case 2: Vault Agent Injector","text":"<p>Problem: Applications need secrets from HashiCorp Vault.</p> <p>Solution: Mutating webhook injects Vault agent as init container and sidecar.</p> <p>Annotation: <pre><code>metadata:\n  annotations:\n    vault.hashicorp.com/agent-inject: \"true\"\n    vault.hashicorp.com/role: \"myapp\"\n    vault.hashicorp.com/agent-inject-secret-db: \"database/creds/myapp\"\n</code></pre></p> <p>Mutation adds: - Init container to fetch secrets - Shared volume for secrets - Sidecar to rotate secrets</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#use-case-3-image-policy-enforcement","title":"Use Case 3: Image Policy Enforcement","text":"<p>Problem: Ensure all images come from approved registries.</p> <p>Solution: Mutating webhook prepends registry URL.</p> <p>Before: <pre><code>spec:\n  containers:\n  - name: app\n    image: nginx:latest\n</code></pre></p> <p>After mutation: <pre><code>spec:\n  containers:\n  - name: app\n    image: myregistry.company.com/nginx:latest  # \u2190 MODIFIED\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#use-case-4-resource-limit-injection","title":"Use Case 4: Resource Limit Injection","text":"<p>Problem: Developers forget to set resource limits.</p> <p>Solution: Mutating webhook adds default limits.</p> <p>Before: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp\n</code></pre></p> <p>After mutation: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp\n    resources:                           # \u2190 ADDED\n      requests:\n        memory: \"128Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: \"200m\"\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#7-json-patch-operations","title":"7. JSON Patch Operations","text":"<p>Mutating webhooks use RFC 6902 JSON Patch format.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#supported-operations","title":"Supported Operations","text":"Operation Description Example add Add a new field Add a label remove Remove a field Remove an annotation replace Replace a field value Change image tag move Move a field Reorganize structure copy Copy a field Duplicate a value test Test a value Conditional patching"},{"location":"cluster-architecture/mutating-admission-controllers/#example-patches","title":"Example Patches","text":"<p>Add a Label: <pre><code>[\n  {\n    \"op\": \"add\",\n    \"path\": \"/metadata/labels/team\",\n    \"value\": \"backend\"\n  }\n]\n</code></pre></p> <p>Add a Sidecar Container: <pre><code>[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/containers/-\",\n    \"value\": {\n      \"name\": \"sidecar\",\n      \"image\": \"sidecar:v1\"\n    }\n  }\n]\n</code></pre></p> <p>Replace Image: <pre><code>[\n  {\n    \"op\": \"replace\",\n    \"path\": \"/spec/containers/0/image\",\n    \"value\": \"myregistry.com/nginx:v2\"\n  }\n]\n</code></pre></p> <p>Add Environment Variable: <pre><code>[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/containers/0/env/-\",\n    \"value\": {\n      \"name\": \"INJECTED_BY\",\n      \"value\": \"webhook\"\n    }\n  }\n]\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#8-debugging-mutating-webhooks","title":"8. Debugging Mutating Webhooks","text":""},{"location":"cluster-architecture/mutating-admission-controllers/#enable-audit-logging","title":"Enable Audit Logging","text":"<pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\n- --audit-log-path=/var/log/kubernetes/audit.log\n- --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n</code></pre> <p>Audit Policy: <pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: RequestResponse\n  verbs: [\"create\"]\n  resources:\n  - group: \"\"\n    resources: [\"pods\"]\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#check-webhook-logs","title":"Check Webhook Logs","text":"<pre><code># View webhook service logs\nkubectl logs -n default deployment/sidecar-injector -f\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#test-webhook-manually","title":"Test Webhook Manually","text":"<pre><code># Dry-run to see mutations\nkubectl apply -f pod.yaml --dry-run=server -o yaml\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#common-issues","title":"Common Issues","text":"<p>Issue 1: Webhook Timeout <pre><code>Error: context deadline exceeded calling webhook\n</code></pre></p> <p>Fix: Check if webhook service is running and accessible.</p> <p>Issue 2: Invalid Patch <pre><code>Error: invalid JSON patch\n</code></pre></p> <p>Fix: Validate JSON patch format and paths.</p> <p>Issue 3: Certificate Issues <pre><code>Error: x509: certificate signed by unknown authority\n</code></pre></p> <p>Fix: Ensure CA bundle in MutatingWebhookConfiguration matches webhook cert.</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#9-best-practices","title":"9. Best Practices","text":""},{"location":"cluster-architecture/mutating-admission-controllers/#dos","title":"DO's \u2705","text":"Practice Why Set failurePolicy: Fail Prevent unmodified resources if webhook is down Use namespaceSelector Target specific namespaces Set timeoutSeconds Prevent hanging (default: 10s) Validate patches Ensure valid JSON patch format Log all mutations Audit trail for debugging Version your webhooks Use versioned admission review"},{"location":"cluster-architecture/mutating-admission-controllers/#donts","title":"DON'Ts \u274c","text":"Practice Why Don't mutate kube-system Can break cluster Don't have side effects Webhooks should be idempotent Don't make external calls Webhook timeout issues Don't modify UID/name Immutable fields Don't chain too many Performance impact"},{"location":"cluster-architecture/mutating-admission-controllers/#10-cka-exam-relevance","title":"10. CKA Exam Relevance","text":""},{"location":"cluster-architecture/mutating-admission-controllers/#what-you-need-to-know","title":"What You Need to Know","text":"<p>For the CKA exam: 1. \u2705 Understand that mutating controllers modify requests 2. \u2705 Know they run before validating controllers 3. \u2705 Recognize common mutations (ServiceAccount injection) 4. \u2705 Know how to enable/disable mutating controllers 5. \ud83d\udfe1 Basic understanding of webhooks (concept level)</p> <p>Less likely to be tested: - \u274c Writing webhook code - \u274c JSON patch syntax details - \u274c Certificate management for webhooks</p>"},{"location":"cluster-architecture/mutating-admission-controllers/#exam-scenarios","title":"Exam Scenarios","text":"<p>Scenario: \"Why does my pod have extra containers?\"</p> <p>Answer: Check for mutating webhooks: <pre><code>kubectl get mutatingwebhookconfigurations\nkubectl describe mutatingwebhookconfiguration &lt;name&gt;\n</code></pre></p> <p>Scenario: \"How to disable ServiceAccount injection?\"</p> <p>Answer: <pre><code>spec:\n  automountServiceAccountToken: false\n</code></pre></p>"},{"location":"cluster-architecture/mutating-admission-controllers/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 Mutating controllers modify API requests before persistence \u2705 They run sequentially after authentication/authorization \u2705 Common mutations: ServiceAccount injection, DefaultStorageClass, tolerations \u2705 Custom mutations use MutatingAdmissionWebhook \u2705 Webhooks return JSON Patch to modify resources \u2705 Real-world uses: Istio sidecar, Vault agent, image policy \u2705 Always runs before validating admission controllers \u2705 Can reject requests if mutation fails  </p>"},{"location":"cluster-architecture/mutating-admission-controllers/#quick-commands","title":"Quick Commands","text":"<pre><code># List mutating webhooks\nkubectl get mutatingwebhookconfigurations\n\n# Describe a webhook\nkubectl describe mutatingwebhookconfiguration &lt;name&gt;\n\n# Test with dry-run (see mutations)\nkubectl apply -f pod.yaml --dry-run=server -o yaml\n\n# Check which mutating controllers are enabled\nkubectl -n kube-system get pod kube-apiserver-&lt;node&gt; -o yaml | grep enable-admission-plugins\n</code></pre>"},{"location":"cluster-architecture/mutating-admission-controllers/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Dynamic Admission Control   Official Kubernetes documentation on webhooks</p> </li> <li> <p>Admission Webhook Example   Reference implementation from Kubernetes project</p> </li> <li> <p>Istio Sidecar Injection   Real-world example of mutating webhooks in production</p> </li> </ul>"},{"location":"exam-overview/curriculum/","title":"CKA Exam Curriculum &amp; Weightage (2025/2026)","text":"<p>The Certified Kubernetes Administrator (CKA) exam is performance-based and tests your ability to solve multiple tasks in a Kubernetes environment.</p>"},{"location":"exam-overview/curriculum/#domain-weightage","title":"Domain Weightage","text":"Domain Weightage Questions (Out of 17) Troubleshooting 30% ~5 Cluster Architecture, Installation &amp; Configuration 25% ~4 Services &amp; Networking 20% ~3 Workloads &amp; Scheduling 15% ~3 Storage 10% ~2 <p>[!NOTE] In a typical 17-question exam scenario, each question is worth approximately 5.88% of the total score. This distribution ensures the domain weightages are met accurately.</p>"},{"location":"exam-overview/curriculum/#detailed-curriculum","title":"Detailed Curriculum","text":""},{"location":"exam-overview/curriculum/#1-troubleshooting-30","title":"1. Troubleshooting (30%)","text":"<ul> <li>Troubleshoot clusters and nodes: Identify and resolve issues with cluster nodes and infrastructure.</li> <li>Troubleshoot cluster components: Diagnose problems with etcd, control plane components, and kubelet.</li> <li>Monitoring: Monitor cluster and application resource usage.</li> <li>Logging: Manage and evaluate container logs and output streams.</li> <li>Networking issues: Troubleshoot services, DNS resolution (CoreDNS), and connectivity between pods/services.</li> </ul>"},{"location":"exam-overview/curriculum/#2-cluster-architecture-installation-configuration-25","title":"2. Cluster Architecture, Installation &amp; Configuration (25%)","text":"<ul> <li>RBAC: Manage Role-Based Access Control (Roles, ClusterRoles, Bindings).</li> <li>Installation: Prepare infrastructure and install Kubernetes clusters using <code>kubeadm</code>.</li> <li>Lifecycle Management: Manage cluster upgrades and maintenance.</li> <li>High Availability: Implement and configure highly-available control planes.</li> <li>Tools: Use Helm and Kustomize to install and manage cluster components.</li> <li>Extensions: Understand extension interfaces (CNI, CSI, CRI).</li> <li>CRDs &amp; Operators: Manage Custom Resource Definitions and install/configure operators.</li> </ul>"},{"location":"exam-overview/curriculum/#3-services-networking-20","title":"3. Services &amp; Networking (20%)","text":"<ul> <li>Host Networking: Understand host networking configuration on cluster nodes.</li> <li>Pod Connectivity: Understand and troubleshoot connectivity between Pods.</li> <li>Network Policies: Define and enforce NetworkPolicies to secure traffic.</li> <li>Service Types: Use ClusterIP, NodePort, and LoadBalancer types effectively.</li> <li>Gateway API: Use the newer Gateway API for managing Ingress traffic.</li> <li>Ingress: Configure Ingress controllers and Ingress resources.</li> <li>DNS: Configure and troubleshoot CoreDNS.</li> </ul>"},{"location":"exam-overview/curriculum/#4-workloads-scheduling-15","title":"4. Workloads &amp; Scheduling (15%)","text":"<ul> <li>Deployments: Perform rolling updates and rollbacks.</li> <li>Secrets &amp; ConfigMaps: Use them to configure applications securely.</li> <li>Autoscaling: Configure workload autoscaling (Horizontal Pod Autoscaler).</li> <li>Self-Healing: Understand probes and replicas for robust deployments.</li> <li>Advanced Scheduling: Configure Pod admission and scheduling using PriorityClass, Taints, Tolerations, and Node Affinity.</li> <li>StatefulSets: Deploy and manage stateful applications.</li> </ul>"},{"location":"exam-overview/curriculum/#5-storage-10","title":"5. Storage (10%)","text":"<ul> <li>StorageClasses: Implement StorageClasses and dynamic volume provisioning.</li> <li>Volume Configuration: Configure volume types, access modes, and reclaim policies.</li> <li>PV &amp; PVC: Manage PersistentVolumes and PersistentVolumeClaims.</li> <li>Application Storage: Configure applications with persistent storage.</li> </ul>"},{"location":"exam-overview/curriculum/#key-updates-for-2025","title":"Key Updates for 2025","text":"<ul> <li>Weightage Shift: Troubleshooting (30%) and Cluster Architecture (25%) now make up the majority (55%) of the exam.</li> <li>New Tools: Helm, Kustomize, and Gateway API are now part of the curriculum.</li> <li>Modern Scheduling: PriorityClass is explicitly mentioned.</li> <li>Extensibility: Focus on CRDs and Operators.</li> </ul>"},{"location":"exam-tips/deployment-components/","title":"Minimum Components for a Deployment on Kubernetes Cluster","text":""},{"location":"exam-tips/deployment-components/#overview","title":"Overview","text":"<p>This guide explains the minimum components needed to create and run a deployment on a Kubernetes cluster. Understanding these essential building blocks will help you deploy applications efficiently without unnecessary complexity.</p>"},{"location":"exam-tips/deployment-components/#the-minimum-components","title":"The Minimum Components","text":"<p>To run a deployment on Kubernetes, you need to understand these essential building blocks. We'll start with the absolute minimum and build up to recommended configurations.</p>"},{"location":"exam-tips/deployment-components/#quick-summary-what-do-you-actually-need","title":"Quick Summary: What Do You Actually Need?","text":"Scenario Required Components Result Absolute Minimum Deployment only Pods run, but no network access Practical Minimum Deployment + Service Running app with network access Recommended Minimum Namespace + Deployment + Service Organized, accessible app Production Ready Namespace + Deployment + Service + HPA Auto-scaling production app"},{"location":"exam-tips/deployment-components/#1-namespace-optional-but-recommended","title":"1. Namespace (Optional but Recommended)","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: autoscale\n</code></pre>"},{"location":"exam-tips/deployment-components/#what-it-does","title":"What it does","text":"<ul> <li>Logical separation/grouping of resources</li> <li>Isolation between different apps/teams</li> <li>Not strictly required (can use <code>default</code> namespace)</li> </ul>"},{"location":"exam-tips/deployment-components/#why-you-need-it","title":"Why you need it","text":"<ul> <li>Organization: Keep related resources together</li> <li>Access control: Apply RBAC policies per namespace</li> <li>Resource quotas: Limit resource usage per namespace</li> </ul> <p>Tip</p> <p>While not required, using namespaces is a best practice for organizing your applications, especially in multi-team environments.</p>"},{"location":"exam-tips/deployment-components/#2-deployment-required-core-component","title":"2. Deployment (REQUIRED - Core Component)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache-server\n  namespace: autoscale\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apache-server\n  template:\n    metadata:\n      labels:\n        app: apache-server\n    spec:\n      containers:\n      - name: apache-server\n        image: httpd:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n</code></pre>"},{"location":"exam-tips/deployment-components/#what-it-does_1","title":"What it does","text":"<ul> <li>Creates and manages Pods (running containers)</li> <li>Ensures desired number of replicas are running</li> <li>Handles updates and rollbacks</li> <li>Self-healing (restarts failed pods)</li> </ul>"},{"location":"exam-tips/deployment-components/#why-you-need-it_1","title":"Why you need it","text":"<p>Warning</p> <p>Without this, you have no running application! The Deployment is the core component that actually runs your containerized application.</p>"},{"location":"exam-tips/deployment-components/#3-service-required-for-networking","title":"3. Service (Required for Networking)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: apache-server\n  namespace: autoscale\nspec:\n  selector:\n    app: apache-server\n  ports:\n  - port: 80\n    targetPort: 80\n  type: ClusterIP\n</code></pre>"},{"location":"exam-tips/deployment-components/#what-it-does_2","title":"What it does","text":"<ul> <li>Provides stable network endpoint to access pods</li> <li>Load balances between multiple pod replicas</li> <li>Gives pods a DNS name (<code>apache-server.autoscale.svc.cluster.local</code>)</li> </ul>"},{"location":"exam-tips/deployment-components/#why-you-need-it_2","title":"Why you need it","text":"<ul> <li>Pods have dynamic IPs that change when they restart</li> <li>Service provides a stable way to reach your app</li> <li>Enables load balancing across multiple replicas</li> </ul>"},{"location":"exam-tips/deployment-components/#service-types","title":"Service Types","text":"Type Description Use Case <code>ClusterIP</code> Internal only (default) Internal microservices <code>NodePort</code> Exposes on node's IP Development/testing <code>LoadBalancer</code> Cloud load balancer Production external access <code>ExternalName</code> DNS alias External service integration"},{"location":"exam-tips/deployment-components/#4-hpa-horizontal-pod-autoscaler-optional-for-autoscaling","title":"4. HPA - Horizontal Pod Autoscaler (Optional - for Autoscaling)","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: apache-server\n  namespace: autoscale\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: apache-server\n  minReplicas: 1\n  maxReplicas: 4\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n</code></pre>"},{"location":"exam-tips/deployment-components/#what-it-does_3","title":"What it does","text":"<ul> <li>Automatically scales pods based on CPU/memory</li> <li>Increases replicas under load</li> <li>Decreases replicas when idle</li> </ul>"},{"location":"exam-tips/deployment-components/#why-you-need-it_3","title":"Why you need it","text":"<ul> <li>Only if you want autoscaling</li> <li>Requires metrics-server to be installed in cluster</li> </ul> <p>Prerequisites</p> <p>For HPA to work, you must have:</p> <ol> <li>Metrics Server installed in your cluster</li> <li>Resource requests defined in your Deployment</li> </ol>"},{"location":"exam-tips/deployment-components/#component-hierarchy","title":"Component Hierarchy","text":"<pre><code>Namespace (autoscale)\n  \u2514\u2500\u2500 Deployment (apache-server)\n        \u251c\u2500\u2500 ReplicaSet (created automatically)\n        \u2502     \u251c\u2500\u2500 Pod 1 (apache container)\n        \u2502     \u2514\u2500\u2500 Pod 2 (if scaled)\n        \u2502\n        \u251c\u2500\u2500 Service (apache-server)\n        \u2502     \u2514\u2500\u2500 Exposes pods via stable IP/DNS\n        \u2502\n        \u2514\u2500\u2500 HPA (apache-server) [optional]\n              \u2514\u2500\u2500 Watches and scales deployment\n</code></pre>"},{"location":"exam-tips/deployment-components/#minimum-configurations","title":"Minimum Configurations","text":""},{"location":"exam-tips/deployment-components/#absolute-minimum-1-component","title":"Absolute Minimum (1 Component)","text":"<p>Just a Deployment - bare minimum to run an application:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apache-server\n  template:\n    metadata:\n      labels:\n        app: apache-server\n    spec:\n      containers:\n      - name: apache-server\n        image: httpd:latest\n</code></pre> <p>This alone will:</p> <ul> <li>\u2705 Create pods</li> <li>\u2705 Keep them running</li> <li>\u274c But you can't access them easily (no Service)</li> <li>\u274c No autoscaling (no HPA)</li> </ul>"},{"location":"exam-tips/deployment-components/#practical-minimum-2-components","title":"Practical Minimum (2 Components)","text":"<p>Deployment + Service for a working, accessible application:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apache-server\n  template:\n    metadata:\n      labels:\n        app: apache-server\n    spec:\n      containers:\n      - name: apache-server\n        image: httpd:latest\n        ports:\n        - containerPort: 80\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: apache-server\nspec:\n  selector:\n    app: apache-server\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <p>This gives you:</p> <ul> <li>\u2705 Running pods</li> <li>\u2705 Network access to pods</li> <li>\u2705 Load balancing</li> <li>\u274c No autoscaling</li> </ul>"},{"location":"exam-tips/deployment-components/#production-ready-all-4-components","title":"Production Ready (All 4 Components)","text":"<p>For production deployments, include all components:</p> <ol> <li>Namespace - Organization</li> <li>Deployment - Runs the app</li> <li>Service - Provides access</li> <li>HPA - Handles scaling</li> </ol>"},{"location":"exam-tips/deployment-components/#component-roles-summary","title":"Component Roles Summary","text":"Component Purpose Required? What Happens Without It Namespace Organization/isolation No (uses <code>default</code>) Everything goes in <code>default</code> namespace Deployment Runs containers YES No application running! Service Network access Recommended Can't easily access pods HPA Autoscaling No Manual scaling only ConfigMap Configuration data No Hard-code configs in container Secret Sensitive data No Hard-code secrets (bad practice!) PersistentVolume Storage No Data lost when pod dies Ingress External HTTP access No Only internal or NodePort"},{"location":"exam-tips/deployment-components/#additional-common-components","title":"Additional Common Components","text":"<p>Beyond the basic components, you may need these for more advanced use cases:</p>"},{"location":"exam-tips/deployment-components/#configmap-for-configuration","title":"ConfigMap (for Configuration)","text":"<p>Store non-sensitive configuration data:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: apache-config\n  namespace: autoscale\ndata:\n  httpd.conf: |\n    ServerName localhost\n    Listen 80\n</code></pre> <p>Use Case: Application configuration files, environment variables</p>"},{"location":"exam-tips/deployment-components/#secret-for-passwordskeys","title":"Secret (for Passwords/Keys)","text":"<p>Store sensitive information:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-password\n  namespace: autoscale\ntype: Opaque\ndata:\n  password: cGFzc3dvcmQxMjM=  # base64 encoded\n</code></pre> <p>Use Case: Database passwords, API keys, TLS certificates</p> <p>Security Note</p> <p>Always base64 encode secret values and never commit unencrypted secrets to version control.</p>"},{"location":"exam-tips/deployment-components/#persistentvolumeclaim-for-storage","title":"PersistentVolumeClaim (for Storage)","text":"<p>Request persistent storage:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: apache-data\n  namespace: autoscale\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>Use Case: Database storage, file uploads, application state</p>"},{"location":"exam-tips/deployment-components/#ingress-for-external-access","title":"Ingress (for External Access)","text":"<p>Expose HTTP/HTTPS routes to services:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: apache-ingress\n  namespace: autoscale\nspec:\n  rules:\n  - host: apache.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: apache-server\n            port:\n              number: 80\n</code></pre> <p>Use Case: Production external access with domain names, SSL termination, path-based routing</p> <p>Ingress Controller Required</p> <p>Ingress resources require an Ingress Controller (like NGINX Ingress Controller) to be installed in your cluster.</p>"},{"location":"exam-tips/deployment-components/#complete-example","title":"Complete Example","text":"<p>Here's a complete, production-ready deployment with all recommended components:</p> <pre><code>---\n# Namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: autoscale\n\n---\n# Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache-server\n  namespace: autoscale\n  labels:\n    app: apache-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apache-server\n  template:\n    metadata:\n      labels:\n        app: apache-server\n    spec:\n      containers:\n      - name: apache-server\n        image: httpd:latest\n        ports:\n        - containerPort: 80\n          name: http\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: apache-server\n  namespace: autoscale\nspec:\n  selector:\n    app: apache-server\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n  type: ClusterIP\n\n---\n# Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: apache-server\n  namespace: autoscale\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: apache-server\n  minReplicas: 1\n  maxReplicas: 4\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 30\n</code></pre>"},{"location":"exam-tips/deployment-components/#deployment-checklist","title":"Deployment Checklist","text":"<p>Before deploying to production, ensure you have:</p> <ul> <li>[ ] Namespace created for organization</li> <li>[ ] Deployment with proper resource requests/limits</li> <li>[ ] Service to expose your application</li> <li>[ ] HPA configured if autoscaling is needed</li> <li>[ ] Metrics Server installed (for HPA)</li> <li>[ ] ConfigMaps for configuration data</li> <li>[ ] Secrets for sensitive information</li> <li>[ ] PersistentVolumes if storage is needed</li> <li>[ ] Ingress for external HTTP/HTTPS access</li> <li>[ ] Resource quotas and limits configured</li> <li>[ ] Labels and selectors properly defined</li> </ul>"},{"location":"exam-tips/deployment-components/#quick-reference","title":"Quick Reference","text":""},{"location":"exam-tips/deployment-components/#minimum-to-run-a-deployment","title":"Minimum to Run a Deployment","text":"<p>Required: 1. Deployment</p>"},{"location":"exam-tips/deployment-components/#minimum-to-run-and-access-a-deployment","title":"Minimum to Run and Access a Deployment","text":"<p>Recommended: 1. Deployment (required) 2. Service (recommended)</p>"},{"location":"exam-tips/deployment-components/#production-ready-setup","title":"Production-Ready Setup","text":"<p>Recommended: 1. Namespace 2. Deployment 3. Service 4. HPA (if autoscaling needed) 5. ConfigMap/Secret (if configuration needed) 6. PersistentVolume (if storage needed) 7. Ingress (if external access needed)</p>"},{"location":"exam-tips/deployment-components/#next-steps","title":"Next Steps","text":"<p>After understanding these components:</p> <ol> <li>Deploy a test application using the complete example above</li> <li>Monitor your deployment using <code>kubectl get</code> commands</li> <li>Scale your application manually or with HPA</li> <li>Add persistent storage if your app needs it</li> <li>Configure external access using Ingress</li> <li>Implement CI/CD to automate deployments</li> </ol>"},{"location":"exam-tips/deployment-components/#additional-resources","title":"Additional Resources","text":"<ul> <li>Kubernetes Official Documentation</li> <li>Kubernetes Best Practices</li> <li>kubectl Cheat Sheet</li> </ul>"},{"location":"exam-tips/dry-run-workflow/","title":"CKA Exam: kubectl --dry-run=client Guide","text":""},{"location":"exam-tips/dry-run-workflow/#overview","title":"Overview","text":"<p><code>--dry-run=client</code> is one of the most useful flags for the CKA exam. It simulates creating a resource without actually creating it. Combined with <code>-o yaml</code>, it generates YAML definitions that you can modify and apply.</p> <p>Time Savings</p> <p>Using <code>--dry-run=client</code> can reduce YAML creation time from 3-5 minutes to just 30 seconds!</p>"},{"location":"exam-tips/dry-run-workflow/#what-is-dry-runclient","title":"What is --dry-run=client?","text":"<p>The flag simulates resource creation without actually creating the resource in the cluster. This allows you to:</p> <ul> <li>Generate valid YAML templates</li> <li>Validate commands before execution</li> <li>Preview what will be created</li> <li>Quickly modify configurations</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#key-use-cases-in-cka-exam","title":"Key Use Cases in CKA Exam","text":""},{"location":"exam-tips/dry-run-workflow/#1-generate-yaml-templates-quickly","title":"1. Generate YAML Templates Quickly","text":"<p>Instead of writing YAML from scratch (time-consuming and error-prone), generate it with kubectl commands.</p>"},{"location":"exam-tips/dry-run-workflow/#pods","title":"Pods","text":"<pre><code># Basic pod YAML\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Pod with labels\nkubectl run nginx --image=nginx \\\n  --labels=app=web,tier=frontend \\\n  --dry-run=client -o yaml &gt; pod.yaml\n\n# Pod with resource limits\nkubectl run nginx --image=nginx \\\n  --requests='cpu=100m,memory=256Mi' \\\n  --limits='cpu=200m,memory=512Mi' \\\n  --dry-run=client -o yaml &gt; pod.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#deployments","title":"Deployments","text":"<pre><code># Basic deployment\nkubectl create deployment nginx --image=nginx --replicas=3 \\\n  --dry-run=client -o yaml &gt; deployment.yaml\n\n# Deployment with port exposed\nkubectl create deployment nginx --image=nginx --port=80 \\\n  --dry-run=client -o yaml &gt; deployment.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#services","title":"Services","text":"<pre><code># ClusterIP service\nkubectl expose pod nginx --port=80 --target-port=8080 \\\n  --dry-run=client -o yaml &gt; service.yaml\n\n# NodePort service\nkubectl create service nodeport nginx --tcp=80:8080 \\\n  --dry-run=client -o yaml &gt; service.yaml\n\n# LoadBalancer from deployment\nkubectl expose deployment nginx --port=80 --type=LoadBalancer \\\n  --dry-run=client -o yaml &gt; service.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#configmaps","title":"ConfigMaps","text":"<pre><code># From literal values\nkubectl create configmap app-config \\\n  --from-literal=key1=value1 \\\n  --from-literal=key2=value2 \\\n  --dry-run=client -o yaml &gt; configmap.yaml\n\n# From file\nkubectl create configmap app-config --from-file=config.txt \\\n  --dry-run=client -o yaml &gt; configmap.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#secrets","title":"Secrets","text":"<pre><code># Generic secret\nkubectl create secret generic db-secret \\\n  --from-literal=password=mypass123 \\\n  --dry-run=client -o yaml &gt; secret.yaml\n\n# TLS secret\nkubectl create secret tls tls-secret \\\n  --cert=path/to/cert \\\n  --key=path/to/key \\\n  --dry-run=client -o yaml &gt; secret.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#jobs","title":"Jobs","text":"<pre><code>kubectl create job test-job --image=busybox -- echo \"Hello\" \\\n  --dry-run=client -o yaml &gt; job.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#cronjobs","title":"CronJobs","text":"<pre><code>kubectl create cronjob test-cron --image=busybox \\\n  --schedule=\"*/5 * * * *\" -- echo \"Hello\" \\\n  --dry-run=client -o yaml &gt; cronjob.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#serviceaccounts","title":"ServiceAccounts","text":"<pre><code>kubectl create serviceaccount my-sa \\\n  --dry-run=client -o yaml &gt; sa.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#roles","title":"Roles","text":"<pre><code>kubectl create role pod-reader \\\n  --verb=get,list,watch \\\n  --resource=pods \\\n  --dry-run=client -o yaml &gt; role.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#rolebindings","title":"RoleBindings","text":"<pre><code>kubectl create rolebinding pod-reader-binding \\\n  --role=pod-reader \\\n  --serviceaccount=default:my-sa \\\n  --dry-run=client -o yaml &gt; rolebinding.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#2-modify-and-apply-workflow","title":"2. Modify and Apply Workflow","text":"<p>The typical CKA exam workflow for complex resources:</p> <pre><code># Step 1: Generate YAML template\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Step 2: Edit the YAML\nvim pod.yaml\n# Add volumes, environment variables, security context, etc.\n\n# Step 3: Apply the modified YAML\nkubectl apply -f pod.yaml\n</code></pre> <p>Pro Workflow</p> <p>This three-step process (Generate \u2192 Edit \u2192 Apply) is faster and less error-prone than writing YAML from scratch.</p>"},{"location":"exam-tips/dry-run-workflow/#3-validate-before-creating","title":"3. Validate Before Creating","text":"<p>Check if your command is correct without actually creating the resource:</p> <pre><code># Test the command syntax\nkubectl run test --image=nginx --port=80 --env=VAR=value --dry-run=client\n\n# Output shows any errors in your command without creating anything\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#4-quick-reference-generation","title":"4. Quick Reference Generation","text":"<p>Generate a template when you forget the exact YAML structure:</p> <pre><code># \"How do I structure a Job again?\"\nkubectl create job example --image=busybox --dry-run=client -o yaml\n\n# See the correct structure, then modify as needed\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#5-combine-multiple-resources","title":"5. Combine Multiple Resources","text":"<pre><code># Generate multiple resources and combine into one file\nkubectl create deployment web --image=nginx --dry-run=client -o yaml &gt; app.yaml\necho \"---\" &gt;&gt; app.yaml\nkubectl expose deployment web --port=80 --dry-run=client -o yaml &gt;&gt; app.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#common-cka-exam-scenarios","title":"Common CKA Exam Scenarios","text":""},{"location":"exam-tips/dry-run-workflow/#scenario-1-create-a-pod-with-specific-requirements","title":"Scenario 1: Create a Pod with Specific Requirements","text":"<p>Question: Create a pod named <code>web</code> with image <code>nginx:1.19</code>, label <code>tier=frontend</code>, resource requests of 100m CPU and 128Mi memory.</p> <p>Without dry-run (slow and error-prone):</p> <pre><code># Must write entire YAML manually\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    tier: frontend\nspec:\n  containers:\n  - name: web\n    image: nginx:1.19\n    resources:\n      requests:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n</code></pre> <p>With dry-run (fast and accurate):</p> <pre><code>kubectl run web --image=nginx:1.19 \\\n  --labels=tier=frontend \\\n  --requests='cpu=100m,memory=128Mi' \\\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#scenario-2-deployment-with-3-replicas","title":"Scenario 2: Deployment with 3 Replicas","text":"<p>Question: Create a deployment <code>app</code> with image <code>redis</code>, 3 replicas, in namespace <code>production</code>.</p> <pre><code>kubectl create deployment app --image=redis --replicas=3 \\\n  -n production --dry-run=client -o yaml &gt; deploy.yaml\n\n# Edit if needed, then apply\nkubectl apply -f deploy.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#scenario-3-expose-a-deployment","title":"Scenario 3: Expose a Deployment","text":"<p>Question: Expose deployment <code>web</code> on port 80, target port 8080 as a NodePort service.</p> <pre><code>kubectl expose deployment web \\\n  --port=80 --target-port=8080 --type=NodePort \\\n  --dry-run=client -o yaml &gt; service.yaml\n\n# Optionally specify nodePort in YAML, then apply\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#scenario-4-configmap-from-multiple-sources","title":"Scenario 4: ConfigMap from Multiple Sources","text":"<p>Question: Create a ConfigMap with database connection details.</p> <pre><code>kubectl create configmap app-config \\\n  --from-literal=DB_HOST=mysql \\\n  --from-literal=DB_PORT=3306 \\\n  --dry-run=client -o yaml &gt; cm.yaml\n\nkubectl apply -f cm.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#scenario-5-role-with-specific-permissions","title":"Scenario 5: Role with Specific Permissions","text":"<p>Question: Create a role that can get, list, and watch pods and services.</p> <pre><code>kubectl create role dev-role \\\n  --verb=get,list,watch \\\n  --resource=pods,services \\\n  --dry-run=client -o yaml &gt; role.yaml\n\nkubectl apply -f role.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#time-comparison","title":"Time Comparison","text":"Method Time Required Error Rate Flexibility Write YAML manually 3-5 minutes High Full Use <code>--dry-run=client</code> 30 seconds Low High Pure imperative commands 10 seconds Low Limited"},{"location":"exam-tips/dry-run-workflow/#advantages-and-limitations","title":"Advantages and Limitations","text":""},{"location":"exam-tips/dry-run-workflow/#advantages","title":"Advantages","text":"<p>\u2705 Speed - Generate templates in seconds \u2705 Accuracy - Kubernetes generates syntactically valid YAML \u2705 Flexibility - Easy to modify generated YAML before applying \u2705 Validation - Check commands before execution \u2705 Learning - See correct YAML structure for reference \u2705 Consistency - Ensures proper formatting and indentation  </p>"},{"location":"exam-tips/dry-run-workflow/#limitations","title":"Limitations","text":"<p>\u26a0\ufe0f Not all resource types support <code>--dry-run=client</code> \u26a0\ufe0f Complex configurations still need manual editing \u26a0\ufe0f Some fields can't be set via kubectl create/run flags \u26a0\ufe0f Advanced features require YAML editing (affinity, tolerations, etc.)  </p>"},{"location":"exam-tips/dry-run-workflow/#kubernetes-documentation-in-cka-exam","title":"Kubernetes Documentation in CKA Exam","text":""},{"location":"exam-tips/dry-run-workflow/#whats-allowed","title":"What's Allowed?","text":"<p>The CKA is an open-book exam. You're allowed to access:</p> <p>\u2705 kubernetes.io/docs \u2705 kubernetes.io/blog \u2705 GitHub Kubernetes repos  </p> <p>No External Resources</p> <p>You cannot access Stack Overflow, Medium articles, or other external sites during the exam.</p>"},{"location":"exam-tips/dry-run-workflow/#most-useful-documentation-pages","title":"Most Useful Documentation Pages","text":""},{"location":"exam-tips/dry-run-workflow/#1-kubectl-cheat-sheet","title":"1. kubectl Cheat Sheet","text":"<pre><code>https://kubernetes.io/docs/reference/kubectl/cheatsheet/\n</code></pre> <ul> <li>Quick reference for all kubectl commands</li> <li>Shows <code>--dry-run=client</code> examples</li> <li>Common operations and shortcuts</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#2-kubectl-command-reference","title":"2. kubectl Command Reference","text":"<pre><code>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands\n</code></pre> <ul> <li>Detailed command documentation</li> <li>All available flags and options</li> <li>Usage examples</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#3-api-reference","title":"3. API Reference","text":"<pre><code>https://kubernetes.io/docs/reference/kubernetes-api/\n</code></pre> <ul> <li>Understanding YAML structure</li> <li>All available fields for each resource type</li> <li>Field descriptions and defaults</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#documentation-strategy","title":"Documentation Strategy","text":"<p>Use documentation for:</p> <ul> <li>Complex YAML structures (StatefulSets, DaemonSets, NetworkPolicies)</li> <li>Specific field names you forgot</li> <li>Advanced configurations (affinity, tolerations, taints, PodDisruptionBudgets)</li> <li>Unfamiliar resource types</li> </ul> <p>Don't use documentation for:</p> <ul> <li>Basic pod/deployment/service creation</li> <li>Common kubectl commands</li> <li>Simple YAML modifications</li> </ul> <p>Time Management</p> <p>Don't spend more than 2 minutes searching documentation. If you can't find it quickly, flag the question and move on.</p>"},{"location":"exam-tips/dry-run-workflow/#recommended-approach-for-cka","title":"Recommended Approach for CKA","text":""},{"location":"exam-tips/dry-run-workflow/#before-the-exam-practice-phase","title":"Before the Exam (Practice Phase)","text":""},{"location":"exam-tips/dry-run-workflow/#1-memorize-common-patterns","title":"1. Memorize Common Patterns","text":"<p>Practice these until they're muscle memory:</p> <ul> <li><code>kubectl run</code> for pods</li> <li><code>kubectl create deployment</code> for deployments</li> <li><code>kubectl expose</code> for services</li> <li><code>kubectl create configmap/secret</code> for configuration</li> <li><code>kubectl create role/rolebinding</code> for RBAC</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#2-practice-the-workflow","title":"2. Practice the Workflow","text":"<pre><code># Generate \u2192 Edit \u2192 Apply workflow\nkubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; deploy.yaml\nvim deploy.yaml\nkubectl apply -f deploy.yaml\n</code></pre> <p>Practice this workflow until it becomes second nature.</p>"},{"location":"exam-tips/dry-run-workflow/#3-set-up-aliases","title":"3. Set Up Aliases","text":"<p>Create useful aliases in your <code>.bashrc</code> or <code>.bash_profile</code>:</p> <pre><code># Add this to your ~/.bashrc or just run it at the start of the exam\nalias k=kubectl\nalias kdr='kubectl --dry-run=client -o yaml'\nalias kgp='kubectl get pods'\nalias kgs='kubectl get svc'\nalias kgd='kubectl get deployments'\n\n# Usage examples\nk run nginx --image=nginx $kdr &gt; pod.yaml\nkgp -o wide\n</code></pre> <p>Exam Environment</p> <p>You can set up aliases during the exam, but practice beforehand so you don't waste time.</p>"},{"location":"exam-tips/dry-run-workflow/#during-the-exam","title":"During the Exam","text":""},{"location":"exam-tips/dry-run-workflow/#1-use-dry-runclient-first","title":"1. Use --dry-run=client First","text":"<p>For these resource types, always start with <code>--dry-run</code>:</p> <ul> <li>Pods (<code>kubectl run</code>)</li> <li>Deployments (<code>kubectl create deployment</code>)</li> <li>Services (<code>kubectl expose</code> or <code>kubectl create service</code>)</li> <li>ConfigMaps (<code>kubectl create configmap</code>)</li> <li>Secrets (<code>kubectl create secret</code>)</li> <li>Jobs (<code>kubectl create job</code>)</li> <li>CronJobs (<code>kubectl create cronjob</code>)</li> <li>Roles/RoleBindings (<code>kubectl create role/rolebinding</code>)</li> <li>ServiceAccounts (<code>kubectl create serviceaccount</code>)</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#2-when-to-use-documentation","title":"2. When to Use Documentation","text":"<p>Refer to kubernetes.io/docs for:</p> <ul> <li>StatefulSets</li> <li>DaemonSets</li> <li>NetworkPolicies</li> <li>PodDisruptionBudgets</li> <li>ResourceQuotas</li> <li>LimitRanges</li> <li>Custom configurations (affinity, tolerations, taints, init containers)</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#3-time-management-tips","title":"3. Time Management Tips","text":"<ul> <li>Read all questions first, flag difficult ones</li> <li>Start with questions you can solve quickly using <code>--dry-run</code></li> <li>Don't spend more than 5-7 minutes per question</li> <li>If stuck, move on and come back later</li> <li>Leave 15-20 minutes at the end to review</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#essential-commands-to-memorize","title":"Essential Commands to Memorize","text":""},{"location":"exam-tips/dry-run-workflow/#core-generators","title":"Core Generators","text":"<pre><code># Pods\nkubectl run NAME --image=IMAGE --dry-run=client -o yaml\n\n# Deployments\nkubectl create deployment NAME --image=IMAGE --replicas=N --dry-run=client -o yaml\n\n# Services - from pod/deployment\nkubectl expose TYPE NAME --port=PORT --target-port=PORT --type=TYPE --dry-run=client -o yaml\n\n# Services - standalone\nkubectl create service TYPE NAME --tcp=PORT:TARGETPORT --dry-run=client -o yaml\n\n# ConfigMaps\nkubectl create configmap NAME --from-literal=KEY=VALUE --dry-run=client -o yaml\n\n# Secrets\nkubectl create secret generic NAME --from-literal=KEY=VALUE --dry-run=client -o yaml\n\n# Jobs\nkubectl create job NAME --image=IMAGE -- COMMAND --dry-run=client -o yaml\n\n# CronJobs\nkubectl create cronjob NAME --image=IMAGE --schedule=\"CRON\" -- COMMAND --dry-run=client -o yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#rbac-resources","title":"RBAC Resources","text":"<pre><code># ServiceAccounts\nkubectl create serviceaccount NAME --dry-run=client -o yaml\n\n# Roles\nkubectl create role NAME --verb=VERB --resource=RESOURCE --dry-run=client -o yaml\n\n# RoleBindings\nkubectl create rolebinding NAME --role=ROLE --serviceaccount=NAMESPACE:SA --dry-run=client -o yaml\n\n# ClusterRoles\nkubectl create clusterrole NAME --verb=VERB --resource=RESOURCE --dry-run=client -o yaml\n\n# ClusterRoleBindings\nkubectl create clusterrolebinding NAME --clusterrole=ROLE --serviceaccount=NAMESPACE:SA --dry-run=client -o yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#pro-tips-for-cka-success","title":"Pro Tips for CKA Success","text":""},{"location":"exam-tips/dry-run-workflow/#1-use-imperative-commands-when-possible","title":"1. Use Imperative Commands When Possible","text":"<p>For simple resources without complex requirements:</p> <pre><code># Faster than dry-run \u2192 edit \u2192 apply\nkubectl run nginx --image=nginx --port=80 --labels=app=web\n\n# Create and expose in one command\nkubectl run nginx --image=nginx --port=80 --expose\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#2-combine-with-kubectl-apply","title":"2. Combine with kubectl apply","text":"<p>One-liner creation for simple cases:</p> <pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#3-use-kubectl-explain","title":"3. Use kubectl explain","text":"<p>When you forget field structures:</p> <pre><code># See all fields for pods\nkubectl explain pod.spec.containers\n\n# See deployment fields\nkubectl explain deployment.spec.template\n\n# See nested fields\nkubectl explain pod.spec.containers.resources\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#4-set-namespace-context","title":"4. Set Namespace Context","text":"<p>Avoid typing <code>-n namespace</code> repeatedly:</p> <pre><code># Set namespace for all subsequent commands\nkubectl config set-context --current --namespace=production\n\n# Now all commands use 'production' namespace\nkubectl get pods  # same as: kubectl get pods -n production\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#5-use-kubectl-get-with-custom-output","title":"5. Use kubectl get with Custom Output","text":"<pre><code># Show pod names and nodes\nkubectl get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName\n\n# Show with wide output for more details\nkubectl get pods -o wide\n\n# Output as YAML to see all fields\nkubectl get pod nginx -o yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#6-master-vim-basics","title":"6. Master vim Basics","text":"<p>You'll need to edit YAML files quickly:</p> <pre><code># Essential vim commands\ni           # Insert mode\nEsc         # Exit insert mode\n:wq         # Write and quit\n:q!         # Quit without saving\ndd          # Delete line\nyy          # Copy line\np           # Paste\nu           # Undo\n/search     # Search for text\n:set number # Show line numbers\n:set paste  # Paste mode (preserves formatting)\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#practice-exercises","title":"Practice Exercises","text":""},{"location":"exam-tips/dry-run-workflow/#exercise-1-multi-container-pod","title":"Exercise 1: Multi-Container Pod","text":"<p>Create a pod with: - Name: <code>multi-pod</code> - First container: <code>nginx</code> with image <code>nginx:1.19</code> - Second container: <code>redis</code> with image <code>redis:alpine</code> - Label: <code>app=web</code></p> <pre><code># Generate base pod\nkubectl run multi-pod --image=nginx:1.19 --labels=app=web --dry-run=client -o yaml &gt; pod.yaml\n\n# Edit to add second container\nvim pod.yaml\n\n# Add under containers:\n# - name: redis\n#   image: redis:alpine\n\n# Apply\nkubectl apply -f pod.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#exercise-2-deployment-with-configmap","title":"Exercise 2: Deployment with ConfigMap","text":"<p>Create: 1. ConfigMap with <code>DB_HOST=postgres</code> and <code>DB_PORT=5432</code> 2. Deployment using the ConfigMap as environment variables</p> <pre><code># ConfigMap\nkubectl create configmap db-config \\\n  --from-literal=DB_HOST=postgres \\\n  --from-literal=DB_PORT=5432 \\\n  --dry-run=client -o yaml &gt; configmap.yaml\n\nkubectl apply -f configmap.yaml\n\n# Deployment\nkubectl create deployment app --image=nginx --dry-run=client -o yaml &gt; deploy.yaml\n\n# Edit deploy.yaml to add envFrom:\n# envFrom:\n# - configMapRef:\n#     name: db-config\n\nkubectl apply -f deploy.yaml\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#exercise-3-service-and-networkpolicy","title":"Exercise 3: Service and NetworkPolicy","text":"<p>Create: 1. A deployment with 3 replicas 2. Expose it as a ClusterIP service 3. Create a NetworkPolicy allowing traffic only from specific pods</p> <pre><code># Deployment\nkubectl create deployment web --image=nginx --replicas=3\n\n# Service\nkubectl expose deployment web --port=80 --dry-run=client -o yaml &gt; service.yaml\nkubectl apply -f service.yaml\n\n# NetworkPolicy (need to create YAML manually or from docs)\n# Search kubernetes.io/docs for NetworkPolicy examples\n</code></pre>"},{"location":"exam-tips/dry-run-workflow/#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"exam-tips/dry-run-workflow/#generate-resources","title":"Generate Resources","text":"Resource Command Pod <code>kubectl run NAME --image=IMAGE --dry-run=client -o yaml</code> Deployment <code>kubectl create deployment NAME --image=IMAGE --dry-run=client -o yaml</code> Service (from resource) <code>kubectl expose TYPE NAME --port=PORT --dry-run=client -o yaml</code> Service (standalone) <code>kubectl create service TYPE NAME --tcp=PORT:PORT --dry-run=client -o yaml</code> ConfigMap <code>kubectl create configmap NAME --from-literal=K=V --dry-run=client -o yaml</code> Secret <code>kubectl create secret generic NAME --from-literal=K=V --dry-run=client -o yaml</code> Job <code>kubectl create job NAME --image=IMAGE --dry-run=client -o yaml</code> CronJob <code>kubectl create cronjob NAME --image=IMAGE --schedule=\"* * * * *\" --dry-run=client -o yaml</code>"},{"location":"exam-tips/dry-run-workflow/#common-flags","title":"Common Flags","text":"Flag Purpose <code>--dry-run=client</code> Simulate command without creating <code>-o yaml</code> Output in YAML format <code>-o json</code> Output in JSON format <code>-n NAMESPACE</code> Specify namespace <code>--replicas=N</code> Set number of replicas <code>--port=PORT</code> Set container port <code>--expose</code> Create service automatically <code>--labels=KEY=VALUE</code> Add labels <code>--requests=KEY=VALUE</code> Set resource requests <code>--limits=KEY=VALUE</code> Set resource limits"},{"location":"exam-tips/dry-run-workflow/#summary","title":"Summary","text":""},{"location":"exam-tips/dry-run-workflow/#is-dry-runclient-useful-in-cka","title":"Is --dry-run=client Useful in CKA?","text":"<p>Absolutely YES! It's one of the most valuable tools for:</p> <p>\u2705 Generating YAML templates quickly \u2705 Saving time on repetitive tasks \u2705 Reducing syntax errors and typos \u2705 Validating commands before execution \u2705 Learning correct YAML structure  </p>"},{"location":"exam-tips/dry-run-workflow/#should-you-use-the-kubernetes-documentation","title":"Should You Use the Kubernetes Documentation?","text":"<p>Yes, but strategically:</p> <ul> <li>\u2705 Use it for complex structures you don't remember</li> <li>\u274c Don't waste time searching for basic commands</li> <li>\u2705 Memorize the most common <code>--dry-run</code> patterns</li> <li>\u274c Don't rely on docs for every question</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#final-recommendations","title":"Final Recommendations","text":"<p>Exam Success Strategy</p> <ol> <li>Master <code>--dry-run=client</code> for common resources</li> <li>Practice the Generate \u2192 Edit \u2192 Apply workflow</li> <li>Memorize essential commands and flags</li> <li>Use documentation only for complex/unfamiliar resources</li> <li>Manage time wisely - don't get stuck on one question</li> <li>Set up useful aliases at the start of the exam</li> </ol> <p>Practice makes perfect! The more you use <code>--dry-run=client</code> in your daily work, the faster and more confident you'll be during the exam.</p>"},{"location":"exam-tips/dry-run-workflow/#-","title":"---","text":""},{"location":"exam-tips/dry-run-workflow/#appendix-concise-summary-shortcuts","title":"Appendix: Concise Summary &amp; Shortcuts","text":"<p>Below is the concise version of the Dry-Run workflow for quick reference and essential command patterns.</p>"},{"location":"exam-tips/dry-run-workflow/#1-what-does-it-actually-do","title":"1. What does it actually do?","text":"<ul> <li><code>--dry-run=client</code>: Tells <code>kubectl</code> to simulate the command locally. It checks the syntax but does not send it to the cluster.</li> <li><code>-o yaml</code>: Tells <code>kubectl</code> to output the result as a YAML manifest instead of a success message.</li> </ul>"},{"location":"exam-tips/dry-run-workflow/#2-situations-where-you-must-use-it","title":"2. Situations where you MUST use it","text":""},{"location":"exam-tips/dry-run-workflow/#a-creating-pods-with-complex-logic","title":"A. Creating Pods with complex logic","text":"<p><code>kubectl run</code> only supports basic flags. If you need to add Resource Limits, Environment Variables, or Volume Mounts, you generate the skeleton first. <pre><code># Goal: Pod with specific environment variables and limits\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Then edit pod.yaml to add 'env' and 'resources'\n</code></pre></p>"},{"location":"exam-tips/dry-run-workflow/#b-creating-daemonsets-the-shortcut","title":"B. Creating DaemonSets (The Shortcut)","text":"<p>Since there is no <code>kubectl create daemonset</code> command: 1.  Generate a Deployment:     <pre><code>kubectl create deployment my-ds --image=nginx --dry-run=client -o yaml &gt; ds.yaml\n</code></pre> 2.  Change <code>kind: Deployment</code> to <code>kind: DaemonSet</code>. 3.  Remove <code>replicas: 1</code>.</p>"},{"location":"exam-tips/dry-run-workflow/#c-creating-services-connecting-to-pods","title":"C. Creating Services (Connecting to Pods)","text":"<p>Instead of typing selectors manually: <pre><code>kubectl expose pod webapp --name=webapp-service --type=NodePort --port=80 --dry-run=client -o yaml &gt; svc.yaml\n</code></pre></p>"},{"location":"exam-tips/dry-run-workflow/#d-multi-container-pods","title":"D. Multi-container Pods","text":"<p>You can't create a multi-container pod using a single CLI command. Generate a one-container pod first, then add the second container in YAML.</p>"},{"location":"exam-tips/dry-run-workflow/#3-cli-vs-kubernetes-documentation","title":"3. CLI vs. Kubernetes Documentation","text":"<p>Should you use the Kubernetes Quick Reference or the CLI?</p> Method When to use it Pros/Cons CLI (<code>dry-run</code>) 90% of the time. Creating Pods, Deployments, Services, ConfigMaps, Secrets, CronJobs. Pro: Fastest, Zero indentation errors.  Con: Doesn't support Affinity, Taints, or NetworkPolicy. Official Docs For \"Heavy\" Logic. Node Affinity, Ingress, NetworkPolicy, PV/PVC. Pro: Copy-paste large blocks of complex YAML.  Con: High risk of copy-paste indentation errors."},{"location":"exam-tips/dry-run-workflow/#4-the-work-life-balance-of-cka","title":"4. The \"Work-Life Balance\" of CKA","text":"<p>The Golden Strategy: 1.  Generate the skeleton using <code>kubectl ... --dry-run=client -o yaml</code>. 2.  Add the \"Organs\" (Affinity, Taints, etc.) by copy-pasting small snippets from the official docs.</p>"},{"location":"exam-tips/dry-run-workflow/#5-summary-cheat-sheet-for-cli-generation","title":"5. Summary Cheat Sheet for CLI Generation","text":"Resource Base Command Pod <code>kubectl run pod-name --image=nginx</code> Deployment <code>kubectl create deployment dep-name --image=nginx</code> Service (ClusterIP) <code>kubectl create service clusterip svc-name --tcp=80:80</code> Service (NodePort) <code>kubectl expose deployment dep-name --type=NodePort --port=80</code> ConfigMap <code>kubectl create configmap my-config --from-literal=key=value</code> Secret <code>kubectl create secret generic my-secret --from-literal=pass=123</code> CronJob <code>kubectl create cronjob my-job --image=nginx --schedule=\"*/1 * * * *\"</code> <p>Always append <code>--dry-run=client -o yaml &gt; file.yaml</code> to these!</p>"},{"location":"exam-tips/kubectl-commands/","title":"CKA Exam Tips: kubectl Commands","text":"<p>This guide contains essential <code>kubectl</code> command patterns and time-saving tricks for the CKA exam.</p>"},{"location":"exam-tips/kubectl-commands/#important-exam-tip-avoid-writing-yaml-files","title":"\u26a1 Important Exam Tip: Avoid Writing YAML Files!","text":"<p>Here's the reality of the CKA exam:</p> <p>Creating and editing YAML files in the CLI is difficult and time-consuming. During the exam: - \u274c Copying/pasting YAML from browser to terminal is awkward - \u274c Manual indentation errors waste precious time - \u274c Typos in <code>apiVersion</code> or <code>kind</code> cause frustrating failures</p> <p>The Solution: Use <code>kubectl run</code> and <code>kubectl create</code> commands to generate YAML templates automatically!</p>"},{"location":"exam-tips/kubectl-commands/#the-exam-strategy","title":"The Exam Strategy","text":"<p>Instead of writing YAML from scratch: <pre><code># \u274c Don't do this (slow, error-prone)\nvim pod.yaml\n# Type everything manually...\n</code></pre></p> <p>Do this instead: <pre><code># \u2705 Generate the YAML automatically\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\nvim pod.yaml  # Make minor edits if needed\nkubectl apply -f pod.yaml\n</code></pre></p> <p>Or even better, skip the YAML file entirely: <pre><code># \u2705 Create directly (fastest)\nkubectl run nginx --image=nginx\nkubectl create deployment nginx --image=nginx --replicas=4\n</code></pre></p>"},{"location":"exam-tips/kubectl-commands/#official-reference","title":"Official Reference","text":"<p>Bookmark this page for the exam: kubectl Conventions</p>"},{"location":"exam-tips/kubectl-commands/#the-golden-rule-dry-run-o-yaml","title":"The Golden Rule: --dry-run + -o yaml","text":"<p>The #1 time-saving trick for the exam:</p> <pre><code>kubectl create &lt;resource&gt; &lt;name&gt; &lt;options&gt; --dry-run=client -o yaml &gt; file.yaml\n</code></pre> <p>What this does: 1. Generates a perfect YAML template 2. Doesn't create anything in the cluster 3. Saves you from typing YAML from scratch 4. Eliminates typos and indentation errors</p> <p>Example: <pre><code>kubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml &gt; deploy.yaml\nvim deploy.yaml  # Make any edits\nkubectl apply -f deploy.yaml\n</code></pre></p>"},{"location":"exam-tips/kubectl-commands/#kubectl-create-vs-kubectl-apply","title":"kubectl create vs kubectl apply","text":"Command When to Use Behavior if Resource Exists <code>kubectl create</code> Quick one-time tasks, generating templates \u274c Fails with error <code>kubectl apply</code> Production, updates, repeatable deployments \u2705 Updates the resource"},{"location":"exam-tips/kubectl-commands/#exam-strategy","title":"Exam Strategy","text":"<ol> <li> <p>Use <code>create</code> for speed: <pre><code>kubectl create deployment nginx --image=nginx --replicas=3\n</code></pre></p> </li> <li> <p>Use <code>create --dry-run</code> to generate YAML: <pre><code>kubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; deploy.yaml\n</code></pre></p> </li> <li> <p>Use <code>apply</code> when updating: <pre><code>kubectl apply -f deploy.yaml\nvim deploy.yaml  # Make changes\nkubectl apply -f deploy.yaml  # Update\n</code></pre></p> </li> </ol>"},{"location":"exam-tips/kubectl-commands/#-dry-run-explained","title":"--dry-run Explained","text":""},{"location":"exam-tips/kubectl-commands/#what-it-does","title":"What It Does","text":"<p>Simulates the command without actually creating anything.</p> <pre><code># Without dry-run (creates the resource)\nkubectl create deployment nginx --image=nginx\n\n# With dry-run (just shows what would happen)\nkubectl create deployment nginx --image=nginx --dry-run=client\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#client-vs-server","title":"client vs server","text":"Flag Where Validation Happens Use Case <code>--dry-run=client</code> Your machine (kubectl) Exam default - Fast, generates templates <code>--dry-run=server</code> API Server (cluster) Full validation, checks quotas <p>For CKA: Always use <code>--dry-run=client</code> (faster).</p>"},{"location":"exam-tips/kubectl-commands/#quick-reference-essential-commands","title":"Quick Reference: Essential Commands","text":"<p>These are the most common commands you'll use in the exam. Memorize these patterns!</p>"},{"location":"exam-tips/kubectl-commands/#create-an-nginx-pod","title":"Create an NGINX Pod","text":"<pre><code>kubectl run nginx --image=nginx\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#generate-pod-manifest-yaml-file-dont-create-it","title":"Generate Pod Manifest YAML file (don't create it)","text":"<pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#create-a-deployment","title":"Create a Deployment","text":"<pre><code>kubectl create deployment nginx --image=nginx\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#generate-deployment-yaml-file-dont-create-it","title":"Generate Deployment YAML file (don't create it)","text":"<pre><code>kubectl create deployment nginx --image=nginx --dry-run=client -o yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#generate-deployment-yaml-and-save-to-file","title":"Generate Deployment YAML and save to file","text":"<pre><code>kubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre> <p>Then make necessary changes to the file (e.g., adding more replicas) and create the deployment: <pre><code>kubectl create -f nginx-deployment.yaml\n</code></pre></p>"},{"location":"exam-tips/kubectl-commands/#create-deployment-with-replicas-kubernetes-119","title":"Create Deployment with Replicas (Kubernetes 1.19+)","text":"<pre><code>kubectl create deployment nginx --image=nginx --replicas=4 --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#essential-kubectl-create-commands","title":"Essential kubectl create Commands","text":""},{"location":"exam-tips/kubectl-commands/#1-deployment","title":"1. Deployment","text":"<pre><code># Basic\nkubectl create deployment nginx --image=nginx\n\n# With replicas\nkubectl create deployment nginx --image=nginx --replicas=3\n\n# Generate YAML\nkubectl create deployment nginx --image=nginx --replicas=3 --dry-run=client -o yaml &gt; deploy.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#2-pod","title":"2. Pod","text":"<pre><code># Basic\nkubectl run nginx --image=nginx\n\n# With port\nkubectl run nginx --image=nginx --port=80\n\n# With labels\nkubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\"\n\n# Generate YAML\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#3-service","title":"3. Service","text":"<pre><code># ClusterIP\nkubectl create service clusterip my-svc --tcp=80:8080\n\n# NodePort\nkubectl create service nodeport my-svc --tcp=80:8080 --node-port=30080\n\n# LoadBalancer\nkubectl create service loadbalancer my-svc --tcp=80:8080\n\n# Expose a deployment\nkubectl expose deployment nginx --port=80 --target-port=8080\n\n# Generate YAML\nkubectl create service clusterip my-svc --tcp=80:8080 --dry-run=client -o yaml &gt; svc.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#4-configmap","title":"4. ConfigMap","text":"<pre><code># From literal values\nkubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2\n\n# From file\nkubectl create configmap my-config --from-file=config.txt\n\n# From directory\nkubectl create configmap my-config --from-file=./config-dir/\n\n# Generate YAML\nkubectl create configmap my-config --from-literal=key=value --dry-run=client -o yaml &gt; cm.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#5-secret","title":"5. Secret","text":"<pre><code># Generic secret\nkubectl create secret generic my-secret --from-literal=password=supersecret\n\n# Docker registry secret\nkubectl create secret docker-registry my-registry \\\n  --docker-server=registry.example.com \\\n  --docker-username=user \\\n  --docker-password=pass\n\n# TLS secret\nkubectl create secret tls my-tls --cert=cert.pem --key=key.pem\n\n# Generate YAML\nkubectl create secret generic my-secret --from-literal=password=secret --dry-run=client -o yaml &gt; secret.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#6-namespace","title":"6. Namespace","text":"<pre><code>kubectl create namespace dev\nkubectl create namespace production\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#7-serviceaccount","title":"7. ServiceAccount","text":"<pre><code>kubectl create serviceaccount my-sa\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#8-job","title":"8. Job","text":"<pre><code>kubectl create job my-job --image=busybox -- echo \"Hello World\"\n\n# Generate YAML\nkubectl create job my-job --image=busybox --dry-run=client -o yaml &gt; job.yaml -- echo \"Hello\"\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#9-cronjob","title":"9. CronJob","text":"<pre><code>kubectl create cronjob my-cronjob --image=busybox --schedule=\"*/5 * * * *\" -- echo \"Hello\"\n\n# Generate YAML\nkubectl create cronjob my-cronjob --image=busybox --schedule=\"*/5 * * * *\" --dry-run=client -o yaml &gt; cronjob.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#10-role-rbac","title":"10. Role (RBAC)","text":"<pre><code>kubectl create role pod-reader --verb=get,list,watch --resource=pods\n\n# Generate YAML\nkubectl create role pod-reader --verb=get,list --resource=pods --dry-run=client -o yaml &gt; role.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#11-rolebinding-rbac","title":"11. RoleBinding (RBAC)","text":"<pre><code>kubectl create rolebinding read-pods --role=pod-reader --user=jane\n\n# Generate YAML\nkubectl create rolebinding read-pods --role=pod-reader --user=jane --dry-run=client -o yaml &gt; rb.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#12-clusterrole-rbac","title":"12. ClusterRole (RBAC)","text":"<pre><code>kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#13-clusterrolebinding-rbac","title":"13. ClusterRoleBinding (RBAC)","text":"<pre><code>kubectl create clusterrolebinding read-pods --clusterrole=pod-reader --user=jane\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#quick-reference-common-tasks","title":"Quick Reference: Common Tasks","text":""},{"location":"exam-tips/kubectl-commands/#scale-a-deployment","title":"Scale a Deployment","text":"<pre><code>kubectl scale deployment nginx --replicas=5\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#update-image","title":"Update Image","text":"<pre><code>kubectl set image deployment/nginx nginx=nginx:1.21\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#rollout-commands","title":"Rollout Commands","text":"<pre><code># Check rollout status\nkubectl rollout status deployment nginx\n\n# View rollout history\nkubectl rollout history deployment nginx\n\n# Undo rollout\nkubectl rollout undo deployment nginx\n\n# Undo to specific revision\nkubectl rollout undo deployment nginx --to-revision=2\n\n# Restart deployment (recreate all pods)\nkubectl rollout restart deployment nginx\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#get-resources","title":"Get Resources","text":"<pre><code># All resources in namespace\nkubectl get all\n\n# Specific resource types\nkubectl get pods\nkubectl get deployments\nkubectl get services\nkubectl get configmaps\nkubectl get secrets\n\n# All namespaces\nkubectl get pods --all-namespaces\nkubectl get pods -A  # Short form\n\n# With labels\nkubectl get pods -l app=nginx\nkubectl get pods -l app=nginx,tier=frontend\n\n# Show labels\nkubectl get pods --show-labels\n\n# Wide output (more details)\nkubectl get pods -o wide\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#describe-resources","title":"Describe Resources","text":"<pre><code>kubectl describe pod nginx\nkubectl describe deployment nginx\nkubectl describe service nginx\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#logs","title":"Logs","text":"<pre><code># Current logs\nkubectl logs nginx\n\n# Previous container logs (after crash)\nkubectl logs nginx --previous\n\n# Follow logs (tail -f)\nkubectl logs -f nginx\n\n# Multiple containers in pod\nkubectl logs nginx -c container-name\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#execute-commands","title":"Execute Commands","text":"<pre><code># Interactive shell\nkubectl exec -it nginx -- /bin/bash\nkubectl exec -it nginx -- sh\n\n# Run single command\nkubectl exec nginx -- ls /\nkubectl exec nginx -- env\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#delete-resources","title":"Delete Resources","text":"<pre><code># Delete by name\nkubectl delete pod nginx\nkubectl delete deployment nginx\n\n# Delete by label\nkubectl delete pods -l app=nginx\n\n# Delete all pods in namespace\nkubectl delete pods --all\n\n# Force delete (stuck pod)\nkubectl delete pod nginx --force --grace-period=0\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#time-saving-aliases","title":"Time-Saving Aliases","text":"<p>Add these to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kgs='kubectl get svc'\nalias kgd='kubectl get deployments'\nalias kd='kubectl describe'\nalias kdp='kubectl describe pod'\nalias kl='kubectl logs'\nalias kex='kubectl exec -it'\nalias ka='kubectl apply -f'\nalias kdel='kubectl delete'\n</code></pre> <p>In the exam, you can set these up at the start: <pre><code>alias k=kubectl\ncomplete -F __start_kubectl k  # Enable autocomplete for 'k'\n</code></pre></p>"},{"location":"exam-tips/kubectl-commands/#exam-workflow-example","title":"Exam Workflow Example","text":"<p>Task: Create a Deployment with 3 replicas, expose it as a Service, and create a ConfigMap.</p>"},{"location":"exam-tips/kubectl-commands/#step-1-generate-yaml-templates","title":"Step 1: Generate YAML templates","text":"<pre><code># Deployment\nkubectl create deployment web --image=nginx --replicas=3 --dry-run=client -o yaml &gt; deploy.yaml\n\n# Service\nkubectl expose deployment web --port=80 --dry-run=client -o yaml &gt; svc.yaml\n\n# ConfigMap\nkubectl create configmap web-config --from-literal=env=prod --dry-run=client -o yaml &gt; cm.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#step-2-edit-if-needed","title":"Step 2: Edit if needed","text":"<pre><code>vim deploy.yaml  # Add labels, resource limits, etc.\nvim svc.yaml     # Change service type if needed\nvim cm.yaml      # Add more config keys\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#step-3-apply-all","title":"Step 3: Apply all","text":"<pre><code>kubectl apply -f deploy.yaml\nkubectl apply -f svc.yaml\nkubectl apply -f cm.yaml\n\n# Or apply entire directory\nkubectl apply -f .\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#step-4-verify","title":"Step 4: Verify","text":"<pre><code>kubectl get all\nkubectl get configmap\nkubectl describe deployment web\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#common-exam-scenarios","title":"Common Exam Scenarios","text":""},{"location":"exam-tips/kubectl-commands/#scenario-1-create-3-replicas-of-nginx","title":"Scenario 1: \"Create 3 replicas of nginx\"","text":"<pre><code>kubectl create deployment nginx --image=nginx --replicas=3\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-2-expose-the-deployment-on-port-80","title":"Scenario 2: \"Expose the deployment on port 80\"","text":"<pre><code>kubectl expose deployment nginx --port=80 --target-port=80\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-3-scale-to-5-replicas","title":"Scenario 3: \"Scale to 5 replicas\"","text":"<pre><code>kubectl scale deployment nginx --replicas=5\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-4-update-image-to-nginx121","title":"Scenario 4: \"Update image to nginx:1.21\"","text":"<pre><code>kubectl set image deployment/nginx nginx=nginx:1.21\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-5-create-a-configmap-from-file","title":"Scenario 5: \"Create a ConfigMap from file\"","text":"<pre><code>kubectl create configmap app-config --from-file=config.properties\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-6-create-a-secret","title":"Scenario 6: \"Create a Secret\"","text":"<pre><code>kubectl create secret generic db-creds --from-literal=username=admin --from-literal=password=secret\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-7-create-a-pod-with-specific-labels","title":"Scenario 7: \"Create a Pod with specific labels\"","text":"<pre><code>kubectl run nginx --image=nginx --labels=\"app=web,tier=frontend\" --dry-run=client -o yaml &gt; pod.yaml\nkubectl apply -f pod.yaml\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#scenario-8-rollback-a-deployment","title":"Scenario 8: \"Rollback a Deployment\"","text":"<pre><code>kubectl rollout undo deployment nginx\n</code></pre>"},{"location":"exam-tips/kubectl-commands/#summary-exam-cheat-sheet","title":"Summary: Exam Cheat Sheet","text":"Task Command Generate YAML <code>kubectl create ... --dry-run=client -o yaml &gt; file.yaml</code> Create Deployment <code>kubectl create deployment nginx --image=nginx --replicas=3</code> Create Pod <code>kubectl run nginx --image=nginx</code> Create Service <code>kubectl expose deployment nginx --port=80</code> Create ConfigMap <code>kubectl create configmap my-config --from-literal=key=value</code> Create Secret <code>kubectl create secret generic my-secret --from-literal=password=secret</code> Scale <code>kubectl scale deployment nginx --replicas=5</code> Update Image <code>kubectl set image deployment/nginx nginx=nginx:1.21</code> Rollback <code>kubectl rollout undo deployment nginx</code> Get Resources <code>kubectl get pods -o wide</code> Describe <code>kubectl describe pod nginx</code> Logs <code>kubectl logs nginx</code> Exec <code>kubectl exec -it nginx -- /bin/bash</code> Delete <code>kubectl delete pod nginx</code> <p>Remember: Speed is critical in the exam. Master <code>--dry-run=client -o yaml</code> to save time!</p>"},{"location":"fundamentals/annotations/","title":"Annotations in Kubernetes","text":"<p>Annotations are used to attach non-identifying metadata to objects. Unlike Labels, Annotations are not used to select or group objects. Instead, they are used to store supplementary information that can be retrieved by tools, libraries, or administrative interfaces.</p>"},{"location":"fundamentals/annotations/#annotations-vs-labels","title":"\ud83e\uddd0 Annotations vs. Labels","text":"Feature Labels (The \"Tags\") Annotations (The \"Notes\") Selection Yes (via Selectors) No Data Type Short, strictly formatted Large, flexible (JSON, structured text) Max Size 63 characters 256 KB Primary Use Filtering and Grouping Supplementary info for tools/humans"},{"location":"fundamentals/annotations/#practical-examples","title":"\ud83d\udee0\ufe0f Practical Examples","text":""},{"location":"fundamentals/annotations/#1-ingress-configuration-most-common-cka-use","title":"1. Ingress Configuration (Most Common CKA use)","text":"<p>Ingress controllers like Nginx use annotations to handle complex routing rules or SSL configuration that isn't part of the core Kubernetes API.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        pathType: Prefix\n        backend:\n          service:\n            name: test\n            port:\n              number: 80\n</code></pre>"},{"location":"fundamentals/annotations/#2-recording-deployment-change-cause","title":"2. Recording Deployment \"Change Cause\"","text":"<p>When you perform a rolling update, it's helpful to see why a change happened in the rollout history. You can use an annotation to record the reason.</p> <pre><code>kubectl annotate deployment/my-app kubernetes.io/change-cause=\"Updated image to v2.0 for security fix\"\n</code></pre> <p>Then, when you check the history, the message will appear: <pre><code>kubectl rollout history deployment/my-app\n</code></pre></p>"},{"location":"fundamentals/annotations/#3-external-tooling-cicd","title":"3. External Tooling &amp; CI/CD","text":"<p>Annotations are perfect for storing information about the build process or the owner of the resource.</p> <pre><code>metadata:\n  annotations:\n    build-id: \"2024-01-15-v1.4.2\"\n    commit-hash: \"af21d9b\"\n    on-call-contact: \"@oncall-devs\"\n    description: \"Legacy database sync service\"\n</code></pre>"},{"location":"fundamentals/annotations/#cli-management","title":"\u2328\ufe0f CLI Management","text":"<p>Manage annotations directly from the terminal just like labels:</p> Goal Command Add/Update <code>kubectl annotate pod my-pod owner=marketing</code> Overwrite <code>kubectl annotate pod my-pod owner=sales --overwrite</code> Remove <code>kubectl annotate pod my-pod owner-</code> (Suffix with dash) Filter (Manual) <code>kubectl get pods -o jsonpath='{.items[?(@.metadata.annotations.owner==\"marketing\")].metadata.name}'</code>"},{"location":"fundamentals/annotations/#cka-exam-tips","title":"\u2705 CKA Exam Tips","text":"<ol> <li>Selection Trap: Remember that <code>kubectl get pods -l key=value</code> only works for Labels. To find an annotation, you must use <code>describe</code> or <code>-o yaml/json</code> and search (grep).</li> <li>Immutability: Unlike <code>nodeName</code>, annotations can be changed on a running pod at any time without needing to recreate it.</li> <li>Large Payloads: If you need to store more than just a simple string (like a certificate or a configuration JSON), use an Annotation, not a label.</li> </ol>"},{"location":"fundamentals/api-versions/","title":"Kubernetes API Versions Guide","text":"<p>Understanding <code>apiVersion</code> is critical for writing correct Kubernetes manifests. This guide explains why some resources use <code>v1</code> while others use <code>apps/v1</code>, <code>batch/v1</code>, etc.</p>"},{"location":"fundamentals/api-versions/#the-history-why-the-split","title":"The History (Why the Split?)","text":""},{"location":"fundamentals/api-versions/#the-core-api-v1","title":"The \"Core\" API (v1)","text":"<p>When Kubernetes was first created, everything was in the core API (<code>v1</code>). These are the \"original\" resources that form the foundation of Kubernetes: *   Pod *   Service *   ConfigMap *   Secret *   Namespace *   PersistentVolume *   PersistentVolumeClaim</p> <p>Why <code>v1</code>? These are so fundamental that they don't need a group name. They're just \"version 1 of the core API.\"</p>"},{"location":"fundamentals/api-versions/#the-apps-api-appsv1","title":"The \"Apps\" API (apps/v1)","text":"<p>As Kubernetes evolved, they needed to add new resource types without breaking the core API. So they created API Groups.</p> <ul> <li>Deployment</li> <li>ReplicaSet</li> <li>StatefulSet</li> <li>DaemonSet</li> </ul> <p>Why <code>apps/v1</code>? These are all related to application workloads, so they were grouped together in the <code>apps</code> API group.</p>"},{"location":"fundamentals/api-versions/#api-version-format","title":"API Version Format","text":"<p>The format is: <code>&lt;group&gt;/&lt;version&gt;</code></p> <ul> <li>No group = Core API \u2192 Just <code>v1</code></li> <li>With group = Named API \u2192 <code>apps/v1</code>, <code>batch/v1</code>, etc.</li> </ul> <p>Examples: <pre><code>apiVersion: v1              # Core API (no group)\nkind: Pod\n\napiVersion: apps/v1         # Apps API group\nkind: Deployment\n\napiVersion: batch/v1        # Batch API group\nkind: Job\n</code></pre></p>"},{"location":"fundamentals/api-versions/#complete-api-version-reference","title":"Complete API Version Reference","text":"Resource API Version API Group Category Pod <code>v1</code> (core) Workload Service <code>v1</code> (core) Networking ConfigMap <code>v1</code> (core) Configuration Secret <code>v1</code> (core) Configuration Namespace <code>v1</code> (core) Organization PersistentVolume <code>v1</code> (core) Storage PersistentVolumeClaim <code>v1</code> (core) Storage Node <code>v1</code> (core) Cluster ServiceAccount <code>v1</code> (core) Security Endpoints <code>v1</code> (core) Networking Event <code>v1</code> (core) Monitoring LimitRange <code>v1</code> (core) Resource Management ResourceQuota <code>v1</code> (core) Resource Management Deployment <code>apps/v1</code> apps Workload ReplicaSet <code>apps/v1</code> apps Workload StatefulSet <code>apps/v1</code> apps Workload DaemonSet <code>apps/v1</code> apps Workload Job <code>batch/v1</code> batch Workload CronJob <code>batch/v1</code> batch Workload Ingress <code>networking.k8s.io/v1</code> networking Networking NetworkPolicy <code>networking.k8s.io/v1</code> networking Security IngressClass <code>networking.k8s.io/v1</code> networking Networking Role <code>rbac.authorization.k8s.io/v1</code> rbac Security RoleBinding <code>rbac.authorization.k8s.io/v1</code> rbac Security ClusterRole <code>rbac.authorization.k8s.io/v1</code> rbac Security ClusterRoleBinding <code>rbac.authorization.k8s.io/v1</code> rbac Security HorizontalPodAutoscaler <code>autoscaling/v2</code> autoscaling Scaling VerticalPodAutoscaler <code>autoscaling.k8s.io/v1</code> autoscaling Scaling PodDisruptionBudget <code>policy/v1</code> policy Availability StorageClass <code>storage.k8s.io/v1</code> storage Storage VolumeAttachment <code>storage.k8s.io/v1</code> storage Storage CSIDriver <code>storage.k8s.io/v1</code> storage Storage PriorityClass <code>scheduling.k8s.io/v1</code> scheduling Scheduling CustomResourceDefinition <code>apiextensions.k8s.io/v1</code> apiextensions Extension"},{"location":"fundamentals/api-versions/#why-this-matters","title":"Why This Matters","text":""},{"location":"fundamentals/api-versions/#1-evolution-without-breaking-changes","title":"1. Evolution Without Breaking Changes","text":"<p>By using API groups, Kubernetes can: *   Add new features to <code>apps/v2</code> without breaking <code>apps/v1</code> *   Deprecate old versions gradually *   Keep the core API stable</p> <p>Example: <pre><code># Old version (deprecated)\napiVersion: extensions/v1beta1\nkind: Deployment\n\n# Current version (stable)\napiVersion: apps/v1\nkind: Deployment\n</code></pre></p>"},{"location":"fundamentals/api-versions/#2-organization-by-purpose","title":"2. Organization by Purpose","text":"<p>Resources are grouped by functionality: *   <code>apps/*</code> \u2192 Application workloads (Deployments, StatefulSets) *   <code>batch/*</code> \u2192 Batch processing (Jobs, CronJobs) *   <code>networking.k8s.io/*</code> \u2192 Networking (Ingress, NetworkPolicy) *   <code>rbac.authorization.k8s.io/*</code> \u2192 Access control (Roles, RoleBindings)</p>"},{"location":"fundamentals/api-versions/#3-custom-resources","title":"3. Custom Resources","text":"<p>You can create your own API groups for Custom Resource Definitions (CRDs): <pre><code>apiVersion: mycompany.com/v1\nkind: MyCustomResource\n</code></pre></p>"},{"location":"fundamentals/api-versions/#common-patterns","title":"Common Patterns","text":""},{"location":"fundamentals/api-versions/#core-api-v1","title":"Core API (v1)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  key: value\n</code></pre>"},{"location":"fundamentals/api-versions/#apps-api-appsv1","title":"Apps API (apps/v1)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-statefulset\nspec:\n  serviceName: \"my-service\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre>"},{"location":"fundamentals/api-versions/#batch-api-batchv1","title":"Batch API (batch/v1)","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: my-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command: [\"echo\", \"Hello World\"]\n      restartPolicy: Never\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: busybox\n            image: busybox\n            command: [\"echo\", \"Hello World\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"fundamentals/api-versions/#networking-api","title":"Networking API","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-service\n            port:\n              number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: my-network-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: myapp\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: frontend\n</code></pre>"},{"location":"fundamentals/api-versions/#rbac-api","title":"RBAC API","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"fundamentals/api-versions/#how-to-find-the-right-api-version","title":"How to Find the Right API Version","text":""},{"location":"fundamentals/api-versions/#method-1-kubectl-explain","title":"Method 1: kubectl explain","text":"<p>The fastest way to find the correct API version:</p> <pre><code>kubectl explain deployment\n# Output: \n# VERSION: apps/v1\n# KIND:    Deployment\n\nkubectl explain pod\n# Output:\n# VERSION: v1\n# KIND:    Pod\n\nkubectl explain ingress\n# Output:\n# VERSION: networking.k8s.io/v1\n# KIND:    Ingress\n</code></pre>"},{"location":"fundamentals/api-versions/#method-2-kubectl-api-resources","title":"Method 2: kubectl api-resources","text":"<p>List all available resources and their API versions:</p> <pre><code>kubectl api-resources | grep deployment\n# Output: deployments   deploy   apps/v1   true   Deployment\n\nkubectl api-resources | grep pod\n# Output: pods   po   v1   true   Pod\n\nkubectl api-resources | grep ingress\n# Output: ingresses   ing   networking.k8s.io/v1   true   Ingress\n</code></pre>"},{"location":"fundamentals/api-versions/#method-3-kubectl-api-versions","title":"Method 3: kubectl api-versions","text":"<p>List all available API versions in your cluster:</p> <pre><code>kubectl api-versions\n# Output:\n# admissionregistration.k8s.io/v1\n# apiextensions.k8s.io/v1\n# apiregistration.k8s.io/v1\n# apps/v1\n# authentication.k8s.io/v1\n# authorization.k8s.io/v1\n# autoscaling/v1\n# autoscaling/v2\n# batch/v1\n# certificates.k8s.io/v1\n# coordination.k8s.io/v1\n# discovery.k8s.io/v1\n# events.k8s.io/v1\n# networking.k8s.io/v1\n# node.k8s.io/v1\n# policy/v1\n# rbac.authorization.k8s.io/v1\n# scheduling.k8s.io/v1\n# storage.k8s.io/v1\n# v1\n</code></pre>"},{"location":"fundamentals/api-versions/#version-stability-levels","title":"Version Stability Levels","text":"<p>Kubernetes uses version suffixes to indicate stability:</p> Suffix Meaning Example Stability v1 Stable <code>apps/v1</code> Production-ready, won't change v1beta1 Beta <code>batch/v1beta1</code> Feature-complete, may change slightly v1alpha1 Alpha <code>autoscaling/v1alpha1</code> Experimental, may change significantly <p>CKA Tip: Always use stable (<code>v1</code>) versions in the exam unless specifically asked to use beta/alpha.</p>"},{"location":"fundamentals/api-versions/#common-mistakes","title":"Common Mistakes","text":""},{"location":"fundamentals/api-versions/#mistake-1-using-the-wrong-api-version","title":"Mistake 1: Using the wrong API version","text":"<pre><code># WRONG (old, deprecated)\napiVersion: extensions/v1beta1\nkind: Deployment\n\n# CORRECT (current, stable)\napiVersion: apps/v1\nkind: Deployment\n</code></pre>"},{"location":"fundamentals/api-versions/#mistake-2-forgetting-the-api-group","title":"Mistake 2: Forgetting the API group","text":"<pre><code># WRONG (missing \"apps/\")\napiVersion: v1\nkind: Deployment\n\n# CORRECT\napiVersion: apps/v1\nkind: Deployment\n</code></pre>"},{"location":"fundamentals/api-versions/#mistake-3-using-beta-versions-in-production","title":"Mistake 3: Using beta versions in production","text":"<pre><code># RISKY (beta version may change)\napiVersion: batch/v1beta1\nkind: CronJob\n\n# BETTER (stable version)\napiVersion: batch/v1\nkind: CronJob\n</code></pre>"},{"location":"fundamentals/api-versions/#summary","title":"Summary","text":"<p>Simple Rule: *   Old, fundamental resources \u2192 <code>v1</code> (no group) *   Everything else \u2192 <code>&lt;group&gt;/v1</code> (with group)</p> <p>Why the split? Kubernetes needed to grow without breaking existing resources.</p> <p>How to remember: *   If it's a basic building block (Pod, Service, ConfigMap) \u2192 <code>v1</code> *   If it's a higher-level abstraction (Deployment, Job, Ingress) \u2192 <code>&lt;group&gt;/v1</code></p> <p>Pro Tip: When in doubt, use <code>kubectl explain &lt;resource&gt;</code> to find the correct API version!</p>"},{"location":"fundamentals/configmaps/","title":"ConfigMaps in Kubernetes","text":"<p>ConfigMaps allow you to separate configuration data from your application code, making your containerized applications more portable and easier to manage.</p>"},{"location":"fundamentals/configmaps/#1-the-settings-file-analogy","title":"1. The \"Settings File\" Analogy","text":"<p>Think of a ConfigMap like a settings.ini file for your application: *   Without ConfigMap: Configuration is hardcoded in the container image. To change a setting, you rebuild the image. *   With ConfigMap: Configuration lives outside the container. You can change settings without rebuilding anything.</p> <p>Real-world example: <pre><code>Your web app needs a database URL:\n\n\u274c Bad (Hardcoded):\n  app.py contains: DB_HOST = \"mysql.prod.com\"\n\n\u2705 Good (ConfigMap):\n  ConfigMap contains: DB_HOST=mysql.prod.com\n  app.py reads from environment variable\n</code></pre></p>"},{"location":"fundamentals/configmaps/#2-what-is-a-configmap","title":"2. What is a ConfigMap?","text":"<p>A ConfigMap is a Kubernetes object that stores non-confidential configuration data as key-value pairs.</p>"},{"location":"fundamentals/configmaps/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>Non-namespaced data: Available within a specific namespace</li> <li>Plain text: Not encrypted (use Secrets for sensitive data)</li> <li>Multiple formats: Simple strings, files, or even entire config files</li> <li>Decoupled: Configuration lives separately from pod definitions</li> </ul>"},{"location":"fundamentals/configmaps/#3-why-use-configmaps","title":"3. Why Use ConfigMaps?","text":""},{"location":"fundamentals/configmaps/#the-12-factor-app-principle","title":"The \"12-Factor App\" Principle","text":"<p>Problem: You have the same application running in 3 environments:</p> Environment Database URL Log Level API Endpoint Dev <code>mysql.dev.local</code> <code>DEBUG</code> <code>api.dev.example.com</code> Staging <code>mysql.staging.local</code> <code>INFO</code> <code>api.staging.example.com</code> Production <code>mysql.prod.local</code> <code>ERROR</code> <code>api.example.com</code> <p>Without ConfigMaps: - You need 3 different container images (one per environment) - Any config change requires rebuilding and redeploying images</p> <p>With ConfigMaps: - One container image for all environments - Different ConfigMaps per environment - Change config without touching code or images</p>"},{"location":"fundamentals/configmaps/#4-creating-configmaps","title":"4. Creating ConfigMaps","text":""},{"location":"fundamentals/configmaps/#method-1-from-literal-values-key-value-pairs","title":"Method 1: From Literal Values (Key-Value Pairs)","text":"<pre><code>kubectl create configmap app-config \\\n  --from-literal=DB_HOST=mysql.prod.com \\\n  --from-literal=DB_PORT=3306 \\\n  --from-literal=LOG_LEVEL=INFO\n</code></pre> <p>What gets created: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DB_HOST: mysql.prod.com\n  DB_PORT: \"3306\"\n  LOG_LEVEL: INFO\n</code></pre></p>"},{"location":"fundamentals/configmaps/#method-2-from-a-file","title":"Method 2: From a File","text":"<p>Step 1: Create a properties file <pre><code>cat &gt; app.properties &lt;&lt;EOF\ndatabase.host=mysql.prod.com\ndatabase.port=3306\nlog.level=INFO\ncache.enabled=true\nEOF\n</code></pre></p> <p>Step 2: Create ConfigMap from file <pre><code>kubectl create configmap app-config --from-file=app.properties\n</code></pre></p> <p>What gets created: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  app.properties: |\n    database.host=mysql.prod.com\n    database.port=3306\n    log.level=INFO\n    cache.enabled=true\n</code></pre></p>"},{"location":"fundamentals/configmaps/#method-3-from-multiple-files","title":"Method 3: From Multiple Files","text":"<pre><code>kubectl create configmap nginx-config \\\n  --from-file=nginx.conf \\\n  --from-file=default.conf \\\n  --from-file=ssl-params.conf\n</code></pre>"},{"location":"fundamentals/configmaps/#method-4-from-a-directory","title":"Method 4: From a Directory","text":"<pre><code># All files in the directory become separate keys\nkubectl create configmap web-config --from-file=/path/to/config/dir/\n</code></pre>"},{"location":"fundamentals/configmaps/#method-5-from-yaml-manifest-declarative","title":"Method 5: From YAML Manifest (Declarative)","text":"<pre><code># config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: production\ndata:\n  # Simple key-value pairs\n  DB_HOST: mysql.prod.com\n  DB_PORT: \"3306\"\n  LOG_LEVEL: INFO\n\n  # Multi-line file content\n  app.properties: |\n    database.host=mysql.prod.com\n    database.port=3306\n    log.level=INFO\n    cache.enabled=true\n\n  # JSON config\n  config.json: |\n    {\n      \"server\": {\n        \"port\": 8080,\n        \"host\": \"0.0.0.0\"\n      }\n    }\n</code></pre> <pre><code>kubectl apply -f config.yaml\n</code></pre>"},{"location":"fundamentals/configmaps/#5-using-configmaps-in-pods","title":"5. Using ConfigMaps in Pods","text":"<p>There are 3 main ways to consume ConfigMap data in pods:</p>"},{"location":"fundamentals/configmaps/#method-1-environment-variables-individual-keys","title":"Method 1: Environment Variables (Individual Keys)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    env:\n    - name: DATABASE_HOST        # Environment variable name\n      valueFrom:\n        configMapKeyRef:\n          name: app-config       # ConfigMap name\n          key: DB_HOST           # Key from ConfigMap\n    - name: DATABASE_PORT\n      valueFrom:\n        configMapKeyRef:\n          name: app-config\n          key: DB_PORT\n</code></pre> <p>Inside the container: <pre><code>echo $DATABASE_HOST  # mysql.prod.com\necho $DATABASE_PORT  # 3306\n</code></pre></p>"},{"location":"fundamentals/configmaps/#method-2-environment-variables-all-keys-at-once","title":"Method 2: Environment Variables (All Keys at Once)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    envFrom:\n    - configMapRef:\n        name: app-config  # Imports ALL keys as env vars\n</code></pre> <p>Inside the container: <pre><code>echo $DB_HOST    # mysql.prod.com\necho $DB_PORT    # 3306\necho $LOG_LEVEL  # INFO\n</code></pre></p>"},{"location":"fundamentals/configmaps/#method-3-volume-mounts-files","title":"Method 3: Volume Mounts (Files)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx/conf.d  # Where to mount\n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config  # ConfigMap to mount\n</code></pre> <p>Inside the container: <pre><code>ls /etc/nginx/conf.d/\n# nginx.conf\n# default.conf\n# ssl-params.conf\n\ncat /etc/nginx/conf.d/nginx.conf\n# &lt;contents of the nginx.conf from ConfigMap&gt;\n</code></pre></p>"},{"location":"fundamentals/configmaps/#method-4-volume-mount-specific-keys-as-files","title":"Method 4: Volume Mount (Specific Keys as Files)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: config\n      mountPath: /config\n  volumes:\n  - name: config\n    configMap:\n      name: app-config\n      items:\n      - key: app.properties      # Key from ConfigMap\n        path: application.properties  # Filename in container\n</code></pre> <p>Inside the container: <pre><code>ls /config/\n# application.properties\n\ncat /config/application.properties\n# database.host=mysql.prod.com\n# database.port=3306\n# ...\n</code></pre></p>"},{"location":"fundamentals/configmaps/#6-real-world-example-multi-environment-application","title":"6. Real-World Example: Multi-Environment Application","text":""},{"location":"fundamentals/configmaps/#scenario","title":"Scenario","text":"<p>You have a Node.js app that needs different configs for dev/staging/prod.</p>"},{"location":"fundamentals/configmaps/#step-1-create-environment-specific-configmaps","title":"Step 1: Create Environment-Specific ConfigMaps","text":"<p>Dev: <pre><code>kubectl create configmap app-config \\\n  --from-literal=DB_HOST=mysql.dev.local \\\n  --from-literal=LOG_LEVEL=DEBUG \\\n  --from-literal=API_URL=http://api.dev.local \\\n  --namespace=dev\n</code></pre></p> <p>Production: <pre><code>kubectl create configmap app-config \\\n  --from-literal=DB_HOST=mysql.prod.local \\\n  --from-literal=LOG_LEVEL=ERROR \\\n  --from-literal=API_URL=https://api.example.com \\\n  --namespace=prod\n</code></pre></p>"},{"location":"fundamentals/configmaps/#step-2-same-deployment-different-namespaces","title":"Step 2: Same Deployment, Different Namespaces","text":"<pre><code># deployment.yaml (same for all environments!)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nodejs-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nodejs\n  template:\n    metadata:\n      labels:\n        app: nodejs\n    spec:\n      containers:\n      - name: app\n        image: mycompany/nodejs-app:1.0  # Same image everywhere!\n        envFrom:\n        - configMapRef:\n            name: app-config  # Same ConfigMap name\n</code></pre> <p>Deploy: <pre><code># Dev environment\nkubectl apply -f deployment.yaml -n dev\n\n# Production environment\nkubectl apply -f deployment.yaml -n prod\n</code></pre></p> <p>Result: Same code, different configuration!</p>"},{"location":"fundamentals/configmaps/#7-updating-configmaps","title":"7. Updating ConfigMaps","text":""},{"location":"fundamentals/configmaps/#important-behavior","title":"Important Behavior","text":"<p>Environment Variables: NOT automatically updated <pre><code>env:\n  - name: DB_HOST\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: DB_HOST\n</code></pre> If you change the ConfigMap, the env var in running pods stays the same. You must restart the pod.</p> <p>Volume Mounts: Automatically updated (with delay) <pre><code>volumeMounts:\n  - name: config\n    mountPath: /etc/config\n</code></pre> If you change the ConfigMap, the files are updated in the pod within ~60 seconds. The app must re-read the files to pick up changes.</p>"},{"location":"fundamentals/configmaps/#how-to-update-a-configmap","title":"How to Update a ConfigMap","text":"<pre><code># Method 1: Edit directly\nkubectl edit configmap app-config\n\n# Method 2: Replace from file\nkubectl create configmap app-config --from-literal=DB_HOST=new-host --dry-run=client -o yaml | kubectl apply -f -\n\n# Method 3: Patch\nkubectl patch configmap app-config -p '{\"data\":{\"DB_HOST\":\"new-host\"}}'\n</code></pre>"},{"location":"fundamentals/configmaps/#forcing-pod-restart-after-configmap-change","title":"Forcing Pod Restart After ConfigMap Change","text":"<pre><code># Rollout restart (for Deployments)\nkubectl rollout restart deployment nodejs-app\n\n# Delete pods (for bare Pods)\nkubectl delete pod web-app\n</code></pre>"},{"location":"fundamentals/configmaps/#8-best-practices","title":"8. Best Practices","text":""},{"location":"fundamentals/configmaps/#dos","title":"\u2705 Do's","text":"<ol> <li> <p>Use for non-sensitive data only <pre><code>data:\n  LOG_LEVEL: INFO        # \u2705 Good\n  API_ENDPOINT: api.com  # \u2705 Good\n</code></pre></p> </li> <li> <p>Keep ConfigMaps small (&lt; 1MB recommended)</p> </li> <li> <p>Use descriptive names <pre><code>\u2705 kubectl create cm app-config\n\u2705 kubectl create cm nginx-server-config\n\u274c kubectl create cm config  # Too generic\n</code></pre></p> </li> <li> <p>Version your ConfigMaps <pre><code>metadata:\n  name: app-config-v2  # Versioned name\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/configmaps/#donts","title":"\u274c Don'ts","text":"<ol> <li> <p>Don't store secrets in ConfigMaps <pre><code>data:\n  DB_PASSWORD: supersecret  # \u274c Use Secrets instead!\n</code></pre></p> </li> <li> <p>Don't create huge ConfigMaps</p> </li> <li>Limit: 1MB per ConfigMap</li> <li> <p>Use external config stores for large data</p> </li> <li> <p>Don't assume instant updates</p> </li> <li>Volume-mounted configs update with delay (~60s)</li> <li>Environment variables never update automatically</li> </ol>"},{"location":"fundamentals/configmaps/#9-kubectl-commands-reference","title":"9. kubectl Commands Reference","text":""},{"location":"fundamentals/configmaps/#creating-configmaps","title":"Creating ConfigMaps","text":"<pre><code># From literal values\nkubectl create cm app-config --from-literal=key1=value1 --from-literal=key2=value2\n\n# From file\nkubectl create cm app-config --from-file=config.properties\n\n# From multiple files\nkubectl create cm nginx-config --from-file=nginx.conf --from-file=ssl.conf\n\n# From directory\nkubectl create cm web-config --from-file=/path/to/configs/\n\n# From YAML\nkubectl apply -f configmap.yaml\n\n# Dry-run (generate YAML)\nkubectl create cm app-config --from-literal=key=value --dry-run=client -o yaml\n</code></pre>"},{"location":"fundamentals/configmaps/#viewing-configmaps","title":"Viewing ConfigMaps","text":"<pre><code># List all ConfigMaps\nkubectl get configmaps\nkubectl get cm\n\n# View specific ConfigMap\nkubectl get cm app-config -o yaml\n\n# View just the data\nkubectl get cm app-config -o jsonpath='{.data}'\n\n# Get a specific key\nkubectl get cm app-config -o jsonpath='{.data.DB_HOST}'\n\n# Describe (shows usage info)\nkubectl describe cm app-config\n</code></pre>"},{"location":"fundamentals/configmaps/#editing-configmaps","title":"Editing ConfigMaps","text":"<pre><code># Edit interactively\nkubectl edit cm app-config\n\n# Replace from file\nkubectl create cm app-config --from-file=new-config.txt --dry-run=client -o yaml | kubectl apply -f -\n\n# Delete\nkubectl delete cm app-config\n</code></pre>"},{"location":"fundamentals/configmaps/#checking-which-pods-use-a-configmap","title":"Checking Which Pods Use a ConfigMap","text":"<pre><code># Show all pods in namespace with volume mounts\nkubectl get pods -o json | jq '.items[] | select(.spec.volumes[]?.configMap.name==\"app-config\") | .metadata.name'\n\n# Describe pod to see configmap references\nkubectl describe pod web-app | grep -A 5 configmap\n</code></pre>"},{"location":"fundamentals/configmaps/#10-configmaps-vs-secrets","title":"10. ConfigMaps vs Secrets","text":"Feature ConfigMap Secret Purpose Non-sensitive config Sensitive data (passwords, tokens) Encoding Plain text Base64 encoded Visibility Easy to view Slightly obfuscated Use for DB host, log levels, URLs DB passwords, API keys, certificates Size limit 1MB 1MB <p>Example: <pre><code># ConfigMap (non-sensitive)\ndata:\n  DB_HOST: mysql.prod.com\n  DB_PORT: \"3306\"\n\n# Secret (sensitive)\ndata:\n  DB_PASSWORD: c3VwZXJzZWNyZXQ=  # Base64 encoded\n</code></pre></p>"},{"location":"fundamentals/configmaps/#11-troubleshooting","title":"11. Troubleshooting","text":""},{"location":"fundamentals/configmaps/#issue-1-pod-cant-find-configmap","title":"Issue 1: Pod Can't Find ConfigMap","text":"<p>Symptom: <pre><code>Error: configmaps \"app-config\" not found\n</code></pre></p> <p>Check: <pre><code># Verify ConfigMap exists in same namespace as pod\nkubectl get cm -n &lt;namespace&gt;\n\n# Check pod events\nkubectl describe pod &lt;pod-name&gt;\n</code></pre></p>"},{"location":"fundamentals/configmaps/#issue-2-environment-variable-is-empty","title":"Issue 2: Environment Variable is Empty","text":"<p>Symptom: <pre><code>echo $DB_HOST\n# (empty)\n</code></pre></p> <p>Possible causes: 1. Key name mismatch 2. ConfigMap doesn't exist 3. Wrong namespace</p> <p>Debug: <pre><code># Check the exact key names\nkubectl get cm app-config -o yaml\n\n# Exec into pod and check env\nkubectl exec -it web-app -- env | grep DB_HOST\n</code></pre></p>"},{"location":"fundamentals/configmaps/#issue-3-configmap-updated-but-pod-still-shows-old-values","title":"Issue 3: ConfigMap Updated but Pod Still Shows Old Values","text":"<p>For environment variables: <pre><code># Env vars don't auto-update - restart pod\nkubectl rollout restart deployment &lt;name&gt;\n</code></pre></p> <p>For volume mounts: <pre><code># Wait ~60 seconds for kubelet to sync\n# Then check inside container\nkubectl exec -it web-app -- cat /etc/config/app.properties\n</code></pre></p>"},{"location":"fundamentals/configmaps/#12-cka-exam-tips","title":"12. CKA Exam Tips","text":""},{"location":"fundamentals/configmaps/#task-create-a-configmap-from-literals","title":"Task: \"Create a ConfigMap from literals\"","text":"<pre><code>kubectl create configmap webapp-config \\\n  --from-literal=APP_COLOR=blue \\\n  --from-literal=APP_MODE=prod\n</code></pre>"},{"location":"fundamentals/configmaps/#task-create-a-pod-that-uses-configmap-as-environment-variables","title":"Task: \"Create a pod that uses ConfigMap as environment variables\"","text":"<pre><code># Step 1: Generate pod YAML\nkubectl run webapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Step 2: Edit to add configmap\nvim pod.yaml\n</code></pre> <p>Add: <pre><code>spec:\n  containers:\n  - name: webapp\n    image: nginx\n    envFrom:\n    - configMapRef:\n        name: webapp-config\n</code></pre></p> <pre><code># Step 3: Apply\nkubectl apply -f pod.yaml\n</code></pre>"},{"location":"fundamentals/configmaps/#task-mount-configmap-as-a-volume","title":"Task: \"Mount ConfigMap as a volume\"","text":"<pre><code># Generate pod\nkubectl run webapp --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> <p>Edit to add: <pre><code>spec:\n  containers:\n  - name: webapp\n    image: nginx\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n  volumes:\n  - name: config-volume\n    configMap:\n      name: webapp-config\n</code></pre></p>"},{"location":"fundamentals/configmaps/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 ConfigMaps store non-sensitive configuration as key-value pairs \u2705 Decouple configuration from application code \u2705 Three consumption methods: env vars, envFrom, and volume mounts \u2705 Environment variables don't auto-update when ConfigMap changes \u2705 Volume-mounted files do auto-update (with ~60s delay) \u2705 Use Secrets for sensitive data, not ConfigMaps \u2705 Name ConfigMaps descriptively and consider versioning \u2705 Maximum size: 1MB per ConfigMap  </p>"},{"location":"fundamentals/configmaps/#quick-command-reference","title":"Quick Command Reference","text":"<pre><code># Create\nkubectl create cm &lt;name&gt; --from-literal=key=value\nkubectl create cm &lt;name&gt; --from-file=file.txt\n\n# View\nkubectl get cm\nkubectl get cm &lt;name&gt; -o yaml\n\n# Use in pod (env)\nenv:\n  - name: VAR\n    valueFrom:\n      configMapKeyRef:\n        name: &lt;cm-name&gt;\n        key: &lt;key&gt;\n\n# Use in pod (volume)\nvolumes:\n  - name: config\n    configMap:\n      name: &lt;cm-name&gt;\n</code></pre>"},{"location":"fundamentals/custom-key-values/","title":"Custom Key-Value Pairs in Kubernetes","text":"<p>One of the most powerful features of Kubernetes is the ability to add custom metadata and configuration data to your resources. This guide covers all the places where you can add your own key-value pairs.</p>"},{"location":"fundamentals/custom-key-values/#1-labels-organizing-and-selecting-resources","title":"1. Labels (Organizing and Selecting Resources)","text":"<p>Purpose: Arbitrary key-value pairs used to organize, group, and select resources.</p> <p>Where: <code>metadata.labels</code></p> <p>Use Cases: *   Grouping resources by application, environment, team *   Selecting pods with <code>kubectl get pods -l app=myapp</code> *   Service selectors, ReplicaSet selectors</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\n  labels:\n    app: myapp\n    environment: production\n    team: backend\n    version: \"1.0\"\n    custom-key: custom-value\n</code></pre></p> <p>Key Characteristics: *   Used for identification and selection *   Can be queried with label selectors (<code>-l</code> flag) *   Limited to 63 characters per key/value *   Must follow DNS subdomain naming rules</p> <p>Common Label Patterns: <pre><code>labels:\n  app.kubernetes.io/name: myapp\n  app.kubernetes.io/version: \"1.0\"\n  app.kubernetes.io/component: frontend\n  app.kubernetes.io/part-of: ecommerce\n  app.kubernetes.io/managed-by: helm\n</code></pre></p>"},{"location":"fundamentals/custom-key-values/#2-annotations-non-identifying-metadata","title":"2. Annotations (Non-Identifying Metadata)","text":"<p>Purpose: Arbitrary key-value pairs for storing non-identifying information like notes, documentation links, or tool-specific metadata.</p> <p>Where: <code>metadata.annotations</code></p> <p>Use Cases: *   Documentation and notes *   Build/release information *   Tool-specific configuration (Prometheus, Istio, etc.) *   Change tracking</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\n  annotations:\n    description: \"This is my production web server\"\n    documentation: \"https://example.com/docs\"\n    slack-channel: \"#devops\"\n    build-version: \"abc123\"\n    last-updated-by: \"john@example.com\"\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9090\"\n</code></pre></p> <p>Key Characteristics: *   NOT used for selection (no <code>-l</code> queries) *   Can store larger values (up to 256KB total per resource) *   Often used by external tools (Ingress controllers, service meshes)</p> <p>Labels vs Annotations: | Feature | Labels | Annotations | | :--- | :--- | :--- | | Purpose | Identification &amp; Selection | Documentation &amp; Metadata | | Queryable | Yes (<code>-l app=web</code>) | No | | Size Limit | 63 chars per value | 256KB total | | Example | <code>app: nginx</code> | <code>description: \"Web server\"</code> |</p>"},{"location":"fundamentals/custom-key-values/#3-configmap-data-configuration-files","title":"3. ConfigMap Data (Configuration Files)","text":"<p>Purpose: Store non-sensitive configuration data as key-value pairs.</p> <p>Where: <code>data</code> or <code>binaryData</code> fields</p> <p>Use Cases: *   Application configuration files *   Environment-specific settings *   Scripts or templates</p> <p>Example: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  # Key-value pairs (strings)\n  database_url: \"postgres://db.example.com:5432/mydb\"\n  log_level: \"info\"\n\n  # Multi-line configuration file\n  config.yaml: |\n    server:\n      port: 8080\n      host: 0.0.0.0\n    database:\n      max_connections: 100\n\n  # Another file\n  nginx.conf: |\n    server {\n      listen 80;\n      server_name example.com;\n    }\n</code></pre></p> <p>Consuming ConfigMaps: <pre><code># As environment variables\nenv:\n  - name: DATABASE_URL\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: database_url\n\n# As volume mount\nvolumes:\n  - name: config\n    configMap:\n      name: app-config\n</code></pre></p>"},{"location":"fundamentals/custom-key-values/#4-secret-data-sensitive-information","title":"4. Secret Data (Sensitive Information)","text":"<p>Purpose: Store sensitive data like passwords, tokens, SSH keys.</p> <p>Where: <code>data</code> (base64-encoded) or <code>stringData</code> (plain text, auto-encoded)</p> <p>Use Cases: *   Database passwords *   API tokens *   TLS certificates</p> <p>Example: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\nstringData:\n  # Plain text (Kubernetes will base64-encode automatically)\n  username: admin\n  password: super-secret-password\n  api-token: \"abc123xyz\"\n\ndata:\n  # Already base64-encoded\n  ssh-key: LS0tLS1CRUdJTi...\n</code></pre></p> <p>Consuming Secrets: <pre><code># As environment variables\nenv:\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-credentials\n        key: password\n\n# As volume mount\nvolumes:\n  - name: secrets\n    secret:\n      secretName: db-credentials\n</code></pre></p> <p>Important: Secrets are NOT encrypted by default in etcd. Use encryption at rest for production.</p>"},{"location":"fundamentals/custom-key-values/#5-environment-variables-container-level","title":"5. Environment Variables (Container-Level)","text":"<p>Purpose: Pass custom configuration to containers as environment variables.</p> <p>Where: <code>spec.containers[].env</code></p> <p>Use Cases: *   Application-specific settings *   Feature flags *   Runtime configuration</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    env:\n      # Static values\n      - name: MY_CUSTOM_VAR\n        value: \"anything\"\n      - name: ENVIRONMENT\n        value: \"production\"\n      - name: FEATURE_FLAG_NEW_UI\n        value: \"true\"\n\n      # From ConfigMap\n      - name: DATABASE_URL\n        valueFrom:\n          configMapKeyRef:\n            name: app-config\n            key: database_url\n\n      # From Secret\n      - name: API_KEY\n        valueFrom:\n          secretKeyRef:\n            name: api-secrets\n            key: api-token\n\n      # From Pod metadata (Downward API)\n      - name: POD_NAME\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.name\n      - name: POD_NAMESPACE\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.namespace\n</code></pre></p>"},{"location":"fundamentals/custom-key-values/#6-downward-api-podcontainer-metadata","title":"6. Downward API (Pod/Container Metadata)","text":"<p>Purpose: Expose Pod or Container metadata as environment variables or files.</p> <p>Available Fields: *   <code>metadata.name</code> - Pod name *   <code>metadata.namespace</code> - Namespace *   <code>metadata.labels['key']</code> - Specific label *   <code>metadata.annotations['key']</code> - Specific annotation *   <code>spec.nodeName</code> - Node name *   <code>status.podIP</code> - Pod IP address</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\n  labels:\n    app: web\n    version: \"1.0\"\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    env:\n      - name: MY_POD_NAME\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.name\n      - name: MY_POD_NAMESPACE\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.namespace\n      - name: MY_APP_VERSION\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.labels['version']\n</code></pre></p>"},{"location":"fundamentals/custom-key-values/#summary-table","title":"Summary Table","text":"Location Purpose Queryable Size Limit Example Use Case Labels Identification &amp; Selection Yes 63 chars <code>app: nginx</code>, <code>env: prod</code> Annotations Documentation &amp; Metadata No 256KB <code>description: \"Web server\"</code> ConfigMap Non-sensitive config No 1MB <code>database_url: \"...\"</code> Secret Sensitive data No 1MB <code>password: \"...\"</code> Env Vars Container config No N/A <code>FEATURE_FLAG: \"true\"</code> Downward API Pod/Container metadata No N/A <code>POD_NAME</code>, <code>POD_IP</code>"},{"location":"fundamentals/custom-key-values/#best-practices","title":"Best Practices","text":""},{"location":"fundamentals/custom-key-values/#1-use-labels-for-selection","title":"1. Use Labels for Selection","text":"<pre><code># Good - Can select with kubectl get pods -l app=nginx\nlabels:\n  app: nginx\n  tier: frontend\n</code></pre>"},{"location":"fundamentals/custom-key-values/#2-use-annotations-for-documentation","title":"2. Use Annotations for Documentation","text":"<pre><code># Good - Human-readable notes\nannotations:\n  description: \"Production web server for example.com\"\n  oncall: \"team-platform@example.com\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#3-use-configmaps-for-non-sensitive-config","title":"3. Use ConfigMaps for Non-Sensitive Config","text":"<pre><code># Good - Database host (not password)\ndata:\n  database_host: \"db.example.com\"\n  cache_ttl: \"3600\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#4-use-secrets-for-sensitive-data","title":"4. Use Secrets for Sensitive Data","text":"<pre><code># Good - Passwords, tokens\nstringData:\n  db_password: \"super-secret\"\n  api_token: \"abc123\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#5-use-env-vars-for-runtime-config","title":"5. Use Env Vars for Runtime Config","text":"<pre><code># Good - Feature flags, runtime settings\nenv:\n  - name: LOG_LEVEL\n    value: \"debug\"\n  - name: ENABLE_FEATURE_X\n    value: \"true\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#common-patterns","title":"Common Patterns","text":""},{"location":"fundamentals/custom-key-values/#pattern-1-multi-environment-setup","title":"Pattern 1: Multi-Environment Setup","text":"<pre><code># Development\nlabels:\n  app: myapp\n  environment: dev\nenv:\n  - name: LOG_LEVEL\n    value: \"debug\"\n\n# Production\nlabels:\n  app: myapp\n  environment: prod\nenv:\n  - name: LOG_LEVEL\n    value: \"error\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#pattern-2-external-tool-integration","title":"Pattern 2: External Tool Integration","text":"<pre><code># Prometheus scraping\nannotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"9090\"\n  prometheus.io/path: \"/metrics\"\n\n# Istio sidecar injection\nannotations:\n  sidecar.istio.io/inject: \"true\"\n</code></pre>"},{"location":"fundamentals/custom-key-values/#pattern-3-blue-green-deployment","title":"Pattern 3: Blue-Green Deployment","text":"<pre><code># Blue version\nlabels:\n  app: myapp\n  version: blue\n\n# Green version\nlabels:\n  app: myapp\n  version: green\n</code></pre>"},{"location":"fundamentals/daemonsets/","title":"Understanding DaemonSets in Kubernetes","text":""},{"location":"fundamentals/daemonsets/#what-are-daemonsets","title":"What are DaemonSets?","text":"<p>A DaemonSet is a Kubernetes controller that ensures a copy of a pod runs on all (or selected) nodes in your cluster.</p>"},{"location":"fundamentals/daemonsets/#key-concept-daemonset-vs-pods","title":"Key Concept: DaemonSet vs Pods","text":"<ul> <li>DaemonSet = A Kubernetes controller/object that creates and manages pods</li> <li>Pods = The actual running containers created by the DaemonSet</li> </ul> <p>Important Distinction</p> <p>The DaemonSet itself is NOT a pod - it's a controller that creates pods.</p>"},{"location":"fundamentals/daemonsets/#the-hierarchy","title":"The Hierarchy","text":"<pre><code>DaemonSet (the controller)\n    \u2514\u2500&gt; Creates and manages\n        \u2514\u2500&gt; Pods (the actual workload)\n            \u2514\u2500&gt; Contains\n                \u2514\u2500&gt; Containers (e.g., Fluentd container running inside)\n</code></pre>"},{"location":"fundamentals/daemonsets/#daemonsets-and-namespaces","title":"DaemonSets and Namespaces","text":""},{"location":"fundamentals/daemonsets/#how-it-works","title":"How It Works","text":"<p>When you create a DaemonSet in a specific namespace:</p> <ol> <li>The DaemonSet object lives in that namespace</li> <li>The pods created by it are also in that same namespace</li> <li>But those pods run on ALL nodes in the cluster (or selected nodes)</li> </ol> <p>Key Understanding</p> <p>Namespaces = Logical groupings for organizing Kubernetes objects Nodes = Physical/virtual machines in your cluster</p> <p>DaemonSets ensure one pod runs on every node (physical), but the DaemonSet exists in a specific namespace (logical).</p>"},{"location":"fundamentals/daemonsets/#visual-example-fluentd-daemonset","title":"Visual Example: Fluentd DaemonSet","text":"<pre><code>Cluster with 3 nodes:\n\nNode 1                  Node 2                  Node 3\n\u251c\u2500 fluentd pod         \u251c\u2500 fluentd pod         \u251c\u2500 fluentd pod\n\u2502  (namespace: logging) \u2502  (namespace: logging) \u2502  (namespace: logging)\n\u2502                       \u2502                       \u2502\n\u251c\u2500 app-a pod           \u251c\u2500 app-b pod           \u251c\u2500 app-c pod\n\u2502  (namespace: prod)    \u2502  (namespace: prod)    \u2502  (namespace: dev)\n\u2502                       \u2502                       \u2502\n\u2514\u2500 database pod        \u2514\u2500 nginx pod           \u2514\u2500 redis pod\n   (namespace: prod)       (namespace: staging)    (namespace: dev)\n</code></pre> <p>All Fluentd pods are in the <code>logging</code> namespace, but they run on every node to collect logs from pods in ALL namespaces.</p>"},{"location":"fundamentals/daemonsets/#example-fluentd-daemonset","title":"Example: Fluentd DaemonSet","text":""},{"location":"fundamentals/daemonsets/#creating-a-fluentd-daemonset","title":"Creating a Fluentd DaemonSet","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: logging  # DaemonSet is in 'logging' namespace\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:  # This is the pod template\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      containers:\n      - name: fluentd\n        image: fluent/fluentd:latest\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n</code></pre>"},{"location":"fundamentals/daemonsets/#what-kubernetes-does","title":"What Kubernetes Does","text":"<ol> <li>Creates the DaemonSet object (just a definition, not running anything yet)</li> <li>Looks at all nodes in the cluster</li> <li>Creates one pod per node using the template in the DaemonSet</li> <li>The pods actually run Fluentd</li> </ol>"},{"location":"fundamentals/daemonsets/#viewing-daemonsets-and-pods","title":"Viewing DaemonSets and Pods","text":""},{"location":"fundamentals/daemonsets/#view-the-daemonset-controller","title":"View the DaemonSet Controller","text":"<pre><code>kubectl get daemonset -n logging\n</code></pre> <p>Output: <pre><code>NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE\nfluentd   3         3         3       3            3\n</code></pre></p> <p>The \"3\" means 3 nodes in the cluster, so 3 pods were created.</p>"},{"location":"fundamentals/daemonsets/#view-the-actual-pods","title":"View the Actual Pods","text":"<pre><code>kubectl get pods -n logging -o wide\n</code></pre> <p>Output: <pre><code>NAME            READY   STATUS    RESTARTS   AGE   NODE\nfluentd-abc123  1/1     Running   0          5m    node1\nfluentd-def456  1/1     Running   0          5m    node2\nfluentd-ghi789  1/1     Running   0          5m    node3\n</code></pre></p>"},{"location":"fundamentals/daemonsets/#self-healing-behavior","title":"Self-Healing Behavior","text":"<p>DaemonSets automatically recreate pods if they're deleted:</p> <pre><code># Delete one pod manually\nkubectl delete pod fluentd-abc123 -n logging\n\n# Check pods again - DaemonSet recreated it!\nkubectl get pods -n logging\n\n# Output:\nNAME            READY   STATUS    RESTARTS   AGE   NODE\nfluentd-xyz999  1/1     Running   0          5s    node1  # New pod!\nfluentd-def456  1/1     Running   0          5m    node2\nfluentd-ghi789  1/1     Running   0          5m    node3\n</code></pre>"},{"location":"fundamentals/daemonsets/#node-selection","title":"Node Selection","text":""},{"location":"fundamentals/daemonsets/#using-nodeselector","title":"Using nodeSelector","text":"<p>Control which nodes get DaemonSet pods:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      nodeSelector:\n        logging: \"true\"  # Only nodes with this label\n      containers:\n      - name: fluentd\n        image: fluent/fluentd:latest\n</code></pre> <p>Label specific nodes:</p> <pre><code># Label nodes for Fluentd\nkubectl label nodes node1 logging=true\nkubectl label nodes node2 logging=true\n# node3 has no label, so no Fluentd pod there\n</code></pre>"},{"location":"fundamentals/daemonsets/#using-node-affinity","title":"Using Node Affinity","text":"<p>More advanced node selection:</p> <pre><code>spec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/hostname\n                operator: In\n                values:\n                - node1\n                - node2\n</code></pre>"},{"location":"fundamentals/daemonsets/#common-daemonset-use-cases","title":"Common DaemonSet Use Cases","text":"<p>All these follow the same pattern: DaemonSet in one namespace, pods on all nodes.</p>"},{"location":"fundamentals/daemonsets/#logging-agents","title":"Logging Agents","text":"<pre><code># Fluentd, Filebeat, Logstash\nnamespace: logging\nruns on: all nodes\npurpose: collect logs from all pods\n</code></pre>"},{"location":"fundamentals/daemonsets/#monitoring-agents","title":"Monitoring Agents","text":"<pre><code># Prometheus Node Exporter, Datadog agent\nnamespace: monitoring\nruns on: all nodes\npurpose: collect metrics from all nodes/pods\n</code></pre>"},{"location":"fundamentals/daemonsets/#networking","title":"Networking","text":"<pre><code># Calico, Weave, Cilium CNI plugins\nnamespace: kube-system\nruns on: all nodes\npurpose: provide networking for all pods\n</code></pre>"},{"location":"fundamentals/daemonsets/#storage","title":"Storage","text":"<pre><code># Ceph, GlusterFS clients\nnamespace: storage\nruns on: all nodes\npurpose: provide storage access\n</code></pre>"},{"location":"fundamentals/daemonsets/#daemonsets-vs-other-controllers","title":"DaemonSets vs Other Controllers","text":"Controller Creates Purpose DaemonSet Pods (one per node) Ensure pod on every node Deployment ReplicaSet \u2192 Pods Manage stateless apps StatefulSet Pods (with stable identity) Manage stateful apps Job Pods (run to completion) Run batch tasks CronJob Jobs \u2192 Pods Run scheduled tasks"},{"location":"fundamentals/daemonsets/#rbac-for-cross-namespace-access","title":"RBAC for Cross-Namespace Access","text":"<p>Fluentd needs cluster-wide permissions to read logs from all namespaces:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: logging\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole  # ClusterRole, not Role\nmetadata:\n  name: fluentd\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"namespaces\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding  # Binds across all namespaces\nmetadata:\n  name: fluentd\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: fluentd\nsubjects:\n- kind: ServiceAccount\n  name: fluentd\n  namespace: logging\n</code></pre>"},{"location":"fundamentals/daemonsets/#daemonsets-and-resourcequotas","title":"DaemonSets and ResourceQuotas","text":"<p>Important Consideration</p> <p>DaemonSet pods DO count against namespace ResourceQuotas.</p> <p>If you have:</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: logging-quota\n  namespace: logging\nspec:\n  hard:\n    pods: \"5\"\n    requests.cpu: \"2\"\n    requests.memory: \"4Gi\"\n</code></pre> <p>And your cluster has 10 nodes, your Fluentd DaemonSet will try to create 10 pods but will fail because the quota only allows 5 pods.</p> <p>Solution: Set appropriate quotas for namespaces with DaemonSets.</p>"},{"location":"fundamentals/daemonsets/#practical-workflow","title":"Practical Workflow","text":""},{"location":"fundamentals/daemonsets/#1-create-the-daemonset","title":"1. Create the DaemonSet","text":"<pre><code>kubectl apply -f fluentd-daemonset.yaml\n</code></pre>"},{"location":"fundamentals/daemonsets/#2-verify-daemonset-created","title":"2. Verify DaemonSet Created","text":"<pre><code>kubectl get daemonset -n logging\n</code></pre>"},{"location":"fundamentals/daemonsets/#3-check-pods-were-created","title":"3. Check Pods Were Created","text":"<pre><code>kubectl get pods -n logging -o wide\n</code></pre>"},{"location":"fundamentals/daemonsets/#4-view-logs-from-a-pod","title":"4. View Logs from a Pod","text":"<pre><code>kubectl logs fluentd-abc123 -n logging\n</code></pre>"},{"location":"fundamentals/daemonsets/#5-update-the-daemonset","title":"5. Update the DaemonSet","text":"<pre><code># Edit the DaemonSet\nkubectl edit daemonset fluentd -n logging\n\n# Or apply updated YAML\nkubectl apply -f fluentd-daemonset.yaml\n</code></pre>"},{"location":"fundamentals/daemonsets/#6-delete-the-daemonset","title":"6. Delete the DaemonSet","text":"<pre><code># This deletes the DaemonSet AND all its pods\nkubectl delete daemonset fluentd -n logging\n</code></pre>"},{"location":"fundamentals/daemonsets/#analogy-factory-workers","title":"Analogy: Factory Workers","text":"<p>Think of it like a factory:</p> <ul> <li>DaemonSet = The factory manager that says \"I need one worker on every floor\"</li> <li>Pods = The actual workers doing the job on each floor</li> <li>Containers = The tools the workers use (Fluentd software)</li> <li>Nodes = The floors in the factory building</li> <li>Namespace = The department that manages these workers</li> </ul>"},{"location":"fundamentals/daemonsets/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 DaemonSet = Definition/controller (doesn't run workloads itself) \u2705 Pods = The actual running instances created by the DaemonSet \u2705 You create a DaemonSet, it creates pods \u2705 DaemonSet is in a specific namespace, but pods run on ALL nodes \u2705 If you delete the DaemonSet, all its pods are deleted \u2705 If a pod dies, the DaemonSet recreates it automatically \u2705 Namespaces = logical organization, Nodes = physical infrastructure</p>"},{"location":"fundamentals/daemonsets/#mental-model","title":"Mental Model","text":"<ul> <li>Namespace = where the DaemonSet and its pods \"live\" logically</li> <li>Nodes = where the pods actually run physically</li> <li>DaemonSets bridge the two by ensuring pods are on every node, regardless of namespace</li> </ul>"},{"location":"fundamentals/daemonsets/#-","title":"---","text":""},{"location":"fundamentals/daemonsets/#appendix-original-version-quick-summary","title":"Appendix: Original Version (Quick Summary)","text":"<p>Below is the original, concise version of the DaemonSet documentation for quick reference.</p>"},{"location":"fundamentals/daemonsets/#1-the-resident-agent-analogy","title":"1. The \"Resident Agent\" Analogy","text":"<p>Think of a DaemonSet like a Landlord's Security Guard.  *   Deployment: Like a store chain. You want 10 stores total, and you don't care exactly which street they are on as long as there are 10. *   DaemonSet: Like a security guard for an apartment building. Each building (Node) must have exactly one guard. If a new building is built, a new guard is hired automatically.</p>"},{"location":"fundamentals/daemonsets/#2-common-use-cases-the-infrastucture-layer","title":"2. Common Use Cases (The \"Infrastucture\" Layer)","text":"<p>You rarely use DaemonSets for your own web apps. You use them for tools that help the cluster run:</p> <ol> <li>Logging: <code>fluentd</code> or <code>logstash</code> running on every node to collect logs.</li> <li>Monitoring: <code>prometheus-node-exporter</code> to gather hardware metrics from every node.</li> <li>Networking: <code>kube-proxy</code> and CNI plugins (like Calico or Flannel) must run on every node to enable communication.</li> <li>Storage: <code>ceph</code> or <code>glusterfs</code> to provide distributed storage across the nodes.</li> </ol>"},{"location":"fundamentals/daemonsets/#3-how-scheduling-works","title":"3. How Scheduling Works","text":"<p>In the past, the DaemonSet controller handled its own scheduling. Now, it uses the standard Kubernetes scheduler.</p> <ul> <li>Taints &amp; Tolerations: DaemonSets automatically handle standard node taints (like <code>node.kubernetes.io/unschedulable</code>) so they can run even on \"Maintenance\" nodes.</li> <li>Node Affinity: You can use <code>nodeAffinity</code> within a DaemonSet to say \"Run this on all nodes with a GPU\" instead of every single node in the cluster.</li> </ul>"},{"location":"fundamentals/daemonsets/#4-practical-example-the-monitoring-agent","title":"4. Practical Example (The \"Monitoring\" Agent)","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: monitoring-agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: monitoring-agent\n  template:\n    metadata:\n      labels:\n        name: monitoring-agent\n    spec:\n      containers:\n      - name: agent\n        image: prometheus/node-exporter:latest\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n</code></pre>"},{"location":"fundamentals/daemonsets/#5-deployment-vs-daemonset-quick-comparison","title":"5. Deployment vs. DaemonSet: Quick Comparison","text":"Feature Deployment / ReplicaSet DaemonSet Pod Count Defined by <code>replicas: X</code> Defined by the number of Nodes Placement Scheduler decides (spreads them out) One per Node (guaranteed) New Node? Nothing changes Automaticaly spawns a new Pod Use Case Web Apps, APIs, DBs Logs, Monitoring, Networking"},{"location":"fundamentals/daemonsets/#6-summary-cheat-sheet","title":"6. Summary Cheat Sheet","text":"Question Answer Can I scale a DaemonSet? No. There is no <code>replicas</code> field. It scales with the cluster. Can I run 2 pods per node? No. A DaemonSet is strictly 1-per-node (or 0 if filtered by labels). How to check them? <code>kubectl get ds</code>"},{"location":"fundamentals/daemonsets/#7-commands-to-check","title":"7. Commands to check","text":"<pre><code># List all DaemonSets\nkubectl get ds -A\n\n# See which pods belong to which node\nkubectl get pods -o wide\n\n# Describe for troubleshooting\nkubectl describe ds monitoring-agent\n</code></pre>"},{"location":"fundamentals/editing-resources/","title":"A Quick Note on Editing Resources","text":"<p>In Kubernetes, the rules for editing a resource depend heavily on whether you are touching a Pod directly or a Controller (like a Deployment).</p>"},{"location":"fundamentals/editing-resources/#1-editing-a-pod","title":"1. Editing a POD","text":"<p>Pods are considered immutable (unchangeable). Once a Pod is born, most of its DNA is locked.</p>"},{"location":"fundamentals/editing-resources/#what-you-can-edit-on-a-running-pod","title":"What you CAN edit on a running Pod:","text":"<ul> <li><code>spec.containers[*].image</code> (Changing the image)</li> <li><code>spec.initContainers[*].image</code></li> <li><code>spec.activeDeadlineSeconds</code></li> <li><code>spec.tolerations</code></li> </ul>"},{"location":"fundamentals/editing-resources/#what-you-cannot-edit","title":"What you CANNOT edit:","text":"<p>You cannot change environment variables, labels (on the spec), service accounts, or resource limits on a running pod.</p>"},{"location":"fundamentals/editing-resources/#how-to-edit-a-pod-anyway-exam-tricks","title":"How to \"Edit\" a Pod anyway (Exam Tricks):","text":""},{"location":"fundamentals/editing-resources/#option-1-the-edit-failure-shortcut","title":"Option 1: The <code>edit</code> failure shortcut","text":"<ol> <li>Run <code>kubectl edit pod &lt;pod-name&gt;</code>.</li> <li>Make your changes.</li> <li>When you save, Kubernetes will deny the change and tell you it's not editable.</li> <li>Crucially: It will tell you it saved your changes to a temporary file (e.g., <code>/tmp/kubectl-edit-ccvrq.yaml</code>).</li> <li>Action:      <pre><code>kubectl delete pod &lt;pod-name&gt;\nkubectl create -f /tmp/kubectl-edit-xxxx.yaml\n</code></pre></li> </ol>"},{"location":"fundamentals/editing-resources/#option-2-the-yaml-export-safer","title":"Option 2: The YAML export (Safer)","text":"<ol> <li>Extract the YAML:     <pre><code>kubectl get pod &lt;pod-name&gt; -o yaml &gt; my-new-pod.yaml\n</code></pre></li> <li>Edit the file manually using <code>vi</code> or <code>nano</code>.</li> <li>Delete the old pod:     <pre><code>kubectl delete pod &lt;pod-name&gt;\n</code></pre></li> <li>Create the new pod:     <pre><code>kubectl create -f my-new-pod.yaml\n</code></pre></li> </ol>"},{"location":"fundamentals/editing-resources/#2-editing-deployments","title":"2. Editing DEPLOYMENTS","text":"<p>Deployments are much smarter. You can edit any field in the Pod template because the Deployment controller handles the lifecycle for you.</p>"},{"location":"fundamentals/editing-resources/#how-it-works","title":"How it works:","text":"<p>When you edit a Deployment, it notices the change, triggers a Rolling Update, kills the old pods, and spawns new ones with your updated settings.</p>"},{"location":"fundamentals/editing-resources/#the-command","title":"The Command:","text":"<pre><code>kubectl edit deployment my-deployment\n</code></pre> <p>CKA Tip: If you are asked to change environment variables or resource limits for a Pod that belongs to a Deployment, ALWAYS edit the Deployment, not the individual Pod.</p>"},{"location":"fundamentals/labels-selectors/","title":"Labels and Selectors","text":"<p>Labels and Selectors are the \"glue\" that holds Kubernetes together. They provide a sophisticated filtering mechanism that allows resources to find and manage each other without relying on unstable identifiers like IP addresses or Pod names.</p> <p>In the ephemeral world of Kubernetes, where Pods are created and destroyed constantly, labels are the only way to maintain a stable relationship between components.</p>"},{"location":"fundamentals/labels-selectors/#labels-the-metadata-tags","title":"\ud83c\udff7\ufe0f Labels: The metadata \"Tags\"","text":"<p>Labels are simple key-value pairs that are attached to objects, such as Pods. They are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users.</p>"},{"location":"fundamentals/labels-selectors/#metadata-vs-labels","title":"Metadata vs. Labels","text":"<ul> <li>Labels: Used for identifying and grouping resources (e.g., <code>env=prod</code>, <code>tier=frontend</code>).</li> <li>Annotations: Used for non-identifying metadata (e.g., build timestamps, git commit hashes, contact info). Annotations cannot be used for selection.</li> </ul>"},{"location":"fundamentals/labels-selectors/#example-pod-with-labels","title":"Example Pod with Labels","text":"<pre><code>metadata:\n  name: web-server\n  labels:\n    app: nginx\n    tier: frontend\n    environment: production\n</code></pre>"},{"location":"fundamentals/labels-selectors/#selectors-the-queries","title":"\ud83d\udd0d Selectors: The \"Queries\"","text":"<p>A Label Selector is a grouped expression that filters resources based on their labels. It is the core mechanism used by Controllers (Deployments, ReplicaSets) and Services to target specific groups of Pods.</p>"},{"location":"fundamentals/labels-selectors/#1-equality-based-selectors","title":"1. Equality-based Selectors","text":"<p>These use simple <code>=</code>, <code>==</code>, or <code>!=</code> operators. - <code>env=prod</code>: Find resources where the label <code>env</code> is <code>prod</code>. - <code>tier!=frontend</code>: Find resources where the label <code>tier</code> is NOT <code>frontend</code>.</p>"},{"location":"fundamentals/labels-selectors/#2-set-based-selectors","title":"2. Set-based Selectors","text":"<p>These allow filtering based on a set of values. - <code>env in (prod, dev)</code>: Find resources where <code>env</code> is either <code>prod</code> or <code>dev</code>. - <code>tier notin (frontend, backend)</code>: Find resources where <code>tier</code> is neither. - <code>partition</code>: Find resources that simply have the label <code>partition</code> defined (regardless of value).</p>"},{"location":"fundamentals/labels-selectors/#the-match-making-process","title":"\ud83c\udfd7\ufe0f The Match-Making Process","text":"<p>In most high-level objects like Deployments and Services, labels appear in two distinct places for specific reasons.</p> <pre><code>kind: Deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web           # 1. THE SELECTOR (The Search Query)\n  template:\n    metadata:\n      labels:\n        app: web         # 2. THE POD LABELS (The Tag applied to new Pods)\n</code></pre>"},{"location":"fundamentals/labels-selectors/#the-match-rule","title":"The \"Match\" Rule","text":"<p>The values in <code>spec.selector.matchLabels</code> MUST be present in the <code>spec.template.metadata.labels</code> of the Pods created. If they don't match, the Deployment controller will never \"see\" the Pods it just created and will keep trying to create more (infinite loop).</p>"},{"location":"fundamentals/labels-selectors/#cli-power-user-commands","title":"\u2328\ufe0f CLI Power User Commands","text":"<p>The CKA exam requires speed. Master these <code>kubectl</code> flag combinations:</p> Goal Command Visible Check <code>kubectl get pods --show-labels</code> Simple Filter <code>kubectl get pods -l app=nginx</code> AND Logic <code>kubectl get pods -l app=nginx,env=prod</code> NOT Logic <code>kubectl get pods -l 'env!=prod'</code> Column View <code>kubectl get pods -L environment,tier</code> (Shows values as columns) Bulk Labeling <code>kubectl label pods -l app=nginx version=v1</code> Remove Label <code>kubectl label pods my-pod version-</code> (Suffix with a dash)"},{"location":"fundamentals/labels-selectors/#cka-tips-best-practices","title":"\u2705 CKA Tips &amp; Best Practices","text":"<ol> <li>Stable Services: Services use selectors to find Pods to send traffic to. If you manually change a Pod's label so it no longer matches the Service selector, that Pod is effectively \"removed from rotation\" (useful for debugging).</li> <li>Deployment Updates: You cannot change the <code>.spec.selector</code> of an existing Deployment. You must delete the Deployment and recreate it if you need to change the selector.</li> <li>Namespace Selectors: NetworkPolicies often use <code>namespaceSelector</code> combined with <code>podSelector</code>. Ensure your Namespaces have the labels you are trying to select!</li> </ol>"},{"location":"fundamentals/limitrange/","title":"LimitRange (Resource Guardrails)","text":"<p>A LimitRange is a policy that sets resource constraints (limits and requests) for all Pods or Containers in a specific namespace. </p> <p>Think of it as the \"Standard Operating Procedure\" for a namespace.</p> <p>CRITICAL NOTE for CKA</p> <p>LimitRange does NOT affect existing Pods.  If you create or update a LimitRange, any Pods already running in the namespace will continue with their old settings (or no settings) until they are deleted and recreated.</p>"},{"location":"fundamentals/limitrange/#1-why-use-limitrange","title":"1. Why use LimitRange?","text":"<ol> <li>Defaults: If a developer forgets to set <code>requests</code> or <code>limits</code>, Kubernetes will automatically inject the values you define in the LimitRange.</li> <li>Enforcement: It prevents a developer from requesting 100 Cores on a node that only has 8 (it will block the Pod creation immediately).</li> <li>Stability: It ensures no single container is too small (wasting management overhead) or too big (risking the node).</li> </ol>"},{"location":"fundamentals/limitrange/#2-key-components-of-a-limitrange","title":"2. Key Components of a LimitRange","text":"Field Purpose <code>default</code> The Limit automatically applied if the developer leaves it blank. <code>defaultRequest</code> The Request automatically applied if the developer leaves it blank. <code>min</code> The absolute minimum allowed value. Pods smaller than this will be rejected. <code>max</code> The absolute maximum allowed value. Pods bigger than this will be rejected."},{"location":"fundamentals/limitrange/#3-practical-example-the-safety-net","title":"3. Practical Example (The \"Safety Net\")","text":"<p>If you apply this YAML to your <code>dev</code> namespace, every tiny Pod will suddenly have a \"size.\"</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-min-max-demo-lr\nspec:\n  limits:\n  - default:                # Default LIMIT\n      cpu: 500m\n      memory: 512Mi\n    defaultRequest:         # Default REQUEST\n      cpu: 200m\n      memory: 256Mi\n    max:                    # Upper Guardrail\n      cpu: \"4\"\n      memory: 2Gi\n    min:                    # Lower Guardrail\n      cpu: 100m\n      memory: 64Mi\n    type: Container\n</code></pre>"},{"location":"fundamentals/limitrange/#what-happens-now","title":"What happens now?","text":"<ul> <li>Case A: A developer runs <code>kubectl run web --image=nginx</code>. <ul> <li>Result: Kubernetes automatically gives it 200m CPU Request and 500m CPU Limit.</li> </ul> </li> <li>Case B: A developer tries to request <code>cpu: 10</code> (10 cores).<ul> <li>Result: Rejected! Error: <code>Forbidden: maximum cpu usage per Container is 4, but provided is 10</code>.</li> </ul> </li> </ul>"},{"location":"fundamentals/limitrange/#4-limitrange-vs-resourcequota","title":"4. LimitRange vs. ResourceQuota","text":"<p>It is easy to confuse these two. Remember the distinction:</p> <ul> <li>LimitRange: Controls the Individual. (e.g., \"Any single person can drink max 2 sodas\").</li> <li>ResourceQuota: Controls the Total. (e.g., \"This whole party can only have 50 sodas total\").</li> </ul>"},{"location":"fundamentals/limitrange/#5-summary-cheat-sheet","title":"5. Summary Cheat Sheet","text":"Question Answer Does it affect running pods? No. It only checks at the \"door\" when a pod is being created. What if I change the LimitRange? Old pods keep their old values. New pods get the new ones. What happens if <code>min</code> &gt; <code>request</code>? The Pod will be rejected."},{"location":"fundamentals/limitrange/#6-commands-to-check","title":"6. Commands to check","text":"<pre><code># See LimitRanges in the current namespace\nkubectl get limitrange\n\n# See the specific settings (Defaults and Guardrails)\nkubectl describe limitrange &lt;name&gt;\n</code></pre>"},{"location":"fundamentals/replicasets-vs-deployments/","title":"ReplicaSets vs. Deployments","text":"<p>While both manage multiple Pods, a Deployment is a higher-level abstraction that manages ReplicaSets. In 99% of production cases, you should use a Deployment.</p>"},{"location":"fundamentals/replicasets-vs-deployments/#1-the-relationship","title":"1. The Relationship","text":"<pre><code>Deployment (The Boss)\n    \u2514\u2500\u2500 ReplicaSet (The Supervisor)\n            \u2514\u2500\u2500 Pods (The Workers)\n</code></pre> <ol> <li>You create a Deployment.</li> <li>The Deployment creates a ReplicaSet.</li> <li>The ReplicaSet ensures the correct number of Pods are running.</li> </ol>"},{"location":"fundamentals/replicasets-vs-deployments/#2-key-differences","title":"2. Key Differences","text":"Feature ReplicaSet Deployment Primary Goal Desired count of identical Pods Rolling Updates and Version Tracking Updating Pods Difficult. You must manually delete old Pods to get new ones. Automatic. Performs a Rolling Update. Rollback Manual. No history kept. Easy. <code>kubectl rollout undo</code>. Discrepancy High risk of \"Mixed Generations\" if template changes. No risk. Deployment creates a new RS for new versions."},{"location":"fundamentals/replicasets-vs-deployments/#3-the-discrepancy-problem-in-replicasets","title":"3. The \"Discrepancy\" Problem in ReplicaSets","text":"<p>If you edit the Pod Template in a ReplicaSet: 1.  The existing Pods stay as they are (unaffected). 2.  Only newly created Pods (after a scale-up or crash) will use the new template. 3.  Result: You have \"Mixed Generations\" of Pods managed by the same object. This is a debugging nightmare.</p>"},{"location":"fundamentals/replicasets-vs-deployments/#4-how-deployments-solve-it-rolling-update","title":"4. How Deployments Solve It (Rolling Update)","text":"<p>When you change the template in a Deployment: 1.  The Deployment creates a NEW ReplicaSet. 2.  It scales UP the new RS (1 Pod at a time). 3.  It scales DOWN the old RS (1 Pod at a time). 4.  Result: A smooth transition where all old Pods are replaced by new ones.</p>"},{"location":"fundamentals/replicasets-vs-deployments/#5-when-to-use-which","title":"5. When to use which?","text":"<ul> <li>ReplicaSet: Use it only when you need a static set of Pods that never change their image/config (rarely).</li> <li>Deployment: Use it for everything else. It gives you lifecycle management (Update, Rollback, Pause).</li> </ul>"},{"location":"fundamentals/replicasets-vs-deployments/#6-cka-exam-commands","title":"6. CKA Exam Commands","text":""},{"location":"fundamentals/replicasets-vs-deployments/#deployment-rollouts","title":"Deployment Rollouts","text":"<pre><code># Check status\nkubectl rollout status deployment/my-app\n\n# View history\nkubectl rollout history deployment/my-app\n\n# Rollback to previous version\nkubectl rollout undo deployment/my-app\n</code></pre>"},{"location":"fundamentals/replicasets-vs-deployments/#scale-same-for-both","title":"Scale (Same for both)","text":"<pre><code>kubectl scale deployment/my-app --replicas=5\nkubectl scale rs/my-rs --replicas=5\n</code></pre>"},{"location":"fundamentals/replicasets-vs-deployments/#summary","title":"Summary","text":"<ul> <li>ReplicaSets focus on keeping a set of pods running.</li> <li>Deployments focus on how those pods evolve over time (updates/rollbacks).</li> <li>Always use Deployments to ensure consistent configuration across all Pods.</li> </ul>"},{"location":"fundamentals/resource-management/","title":"Resource Requests and Limits","text":"<p>In Kubernetes, managing CPU and Memory is critical for cluster stability. Without these settings, a single \"noisy neighbor\" pod can crash your entire node.</p>"},{"location":"fundamentals/resource-management/#1-requests-vs-limits-the-bank-analogy","title":"1. Requests vs. Limits (The Bank Analogy)","text":"Term What it is Analogy Requests Minimum Guaranteed. The amount of resources the pod is guaranteed to have. Minimum Balance: The bank promises you can always withdraw this much. Limits Maximum Allowed. The upper ceiling the pod is not allowed to cross. Credit Limit: The absolute maximum the bank allows you to spend."},{"location":"fundamentals/resource-management/#2-cpu-vs-memory-the-pressure-difference","title":"2. CPU vs. Memory: The \"Pressure\" Difference","text":"<p>It is vital to understand that Kubernetes treats CPU and Memory very differently when a pod hits its limit.</p>"},{"location":"fundamentals/resource-management/#a-cpu-compressible-resource","title":"A. CPU (Compressible Resource)","text":"<ul> <li>Behavior: When a pod hits its CPU limit, Kubernetes throttles it.</li> <li>Result: The app doesn't crash; it just runs slower. (Like a car hitting a speed governor).</li> <li>Unit: Measured in \"millicores\" (e.g., <code>500m</code> = 0.5 CPU cores).</li> </ul>"},{"location":"fundamentals/resource-management/#b-memory-non-compressible-resource","title":"B. Memory (Non-Compressible Resource)","text":"<ul> <li>Behavior: When a pod hits its Memory limit, Kubernetes Terminates it immediately.</li> <li>Result: The Pod is OOMKilled (Out Of Memory). (Like a balloon popping).</li> <li>Unit: Measured in bytes (e.g., <code>256Mi</code>, <code>1Gi</code>).</li> </ul>"},{"location":"fundamentals/resource-management/#3-detailed-behavior-comparison","title":"3. Detailed Behavior Comparison","text":"Resource Type At Request level At Limit level CPU (Compressible) Minimum Guaranteed. If the node has spare CPU, the pod can \"burst\" above this. Hard Ceiling. App is throttled. It is NOT killed, but performance drops. Memory (Non-Compressible) Reserved. This amount is physically set aside. If a node is full, no new pods can request it. Kill Switch. If the app tries to use even 1 byte more, it is killed (OOM) immediately."},{"location":"fundamentals/resource-management/#why-the-difference","title":"Why the difference?","text":"<ul> <li>CPU is a \"time\" resource. You can just give a process fewer slices of time per second. It's like a person walking slower.</li> <li>Memory is a \"space\" resource. If you run out of physical space, you can't just \"slow down\" the memory usage. It's like a room that's full\u2014you can't fit another person in without someone leaving or the walls breaking.</li> </ul>"},{"location":"fundamentals/resource-management/#4-the-ideal-scenarios","title":"4. The \"Ideal Scenarios\"","text":"<p>Setting these numbers is an art, but there are some \"Golden Rules\" used in production:</p>"},{"location":"fundamentals/resource-management/#the-ideal-memory-scenario-requests-limits","title":"The Ideal Memory Scenario (Requests = Limits)","text":"<p>For Memory, the ideal scenario is almost always to set Requests equal to Limits. *   Why? Memory is a \"hard\" resource. If you tell K8s a pod requests 256Mi but might use 1Gi (limit), K8s might schedule it on a node that only has 300Mi free. When the pod starts growing toward its 1Gi limit, it will crash or cause the node to start killing other pods. *   Result: You get the Guaranteed QoS class, which makes your pod the last one to be killed in an emergency.</p>"},{"location":"fundamentals/resource-management/#the-ideal-cpu-scenario-requests-limits","title":"The Ideal CPU Scenario (Requests &lt; Limits)","text":"<p>For CPU, it is often better to leave some \"headroom\" (Burstable QoS). *   Ideal Setting: Set Request to your app's average usage and Limit to its peak burst usage. *   Why? Since CPU only throttles (slows down) rather than kills, it's safer to overcommit slightly. This allows your app to \"burst\" during a startup or a traffic spike without wasting expensive CPU cycles when the app is idle.</p>"},{"location":"fundamentals/resource-management/#the-goldilocks-rule","title":"The \"Goldilocks\" Rule","text":"<ul> <li>Too Low: Pod crashes (OOM) or is too slow to respond (Throttling).</li> <li>Too High: You are wasting money and \"locking\" resources that other pods could use, leading to fragmented nodes.</li> <li>Just Right: Set requests to the 90th percentile of your actual usage.</li> </ul>"},{"location":"fundamentals/resource-management/#5-the-no-cpu-limits-strategy-expert-tip","title":"5. The \"No CPU Limits\" Strategy (Expert Tip)","text":"<p>Many advanced Kubernetes courses (and high-scale companies) recommend setting Requests but NO Limits for CPU.</p>"},{"location":"fundamentals/resource-management/#why-remove-cpu-limits","title":"Why remove CPU Limits?","text":"<ol> <li>Avoid Artificial Throttling: Even if a node is 90% idle, a CPU limit can artificially slow down your app. This wastes the \"slack\" (unused) resources of the cluster.</li> <li>Slack Harvesting: If your app has no limit, it can \"harvest\" any unused CPU cycles from the node to process tasks significantly faster.</li> <li>K8s Fair Share: Kubernetes is smart! If other pods need their CPU (because they have Requests), it will automatically squeeze your \"limit-less\" pod back down to its original Request amount. It won't let you steal from others.</li> </ol> <p>In summary: *   Memory: ALWAYS set limits (to prevent OOM). *   CPU: Consider \"No Limits\" to maximize performance, as long as you have a solid Request to guarantee your fair share.</p>"},{"location":"fundamentals/resource-management/#6-qos-classes-explained","title":"6. QoS Classes Explained","text":"<p>Kubernetes automatically assigns a Quality of Service (QoS) class to your pod based on how you set your requests and limits.</p> QoS Class How to get it Priority Guaranteed Requests == Limits (for both CPU &amp; RAM) Highest. The node will fight to keep these alive. Burstable Requests &lt; Limits Medium. Pod can use extra resources if the node has them. BestEffort No Requests, No Limits Lowest. First to be killed if the node runs out of memory. <p>Ideal Behavior: For critical production databases or core services, always use Guaranteed (Requests = Limits). For web apps, use Burstable.</p>"},{"location":"fundamentals/resource-management/#7-real-time-examples-troubleshooting","title":"7. Real-Time Examples &amp; Troubleshooting","text":""},{"location":"fundamentals/resource-management/#example-1-the-throttled-web-server","title":"Example 1: The \"Throttled\" Web Server","text":"<ul> <li>Setting: <code>request: 100m</code>, <code>limit: 200m</code>.</li> <li>Scenario: A sudden spike in traffic.</li> <li>Result: The pod stays <code>Running</code>, but page load times slow down from 200ms to 2s.</li> <li>Fix: Check <code>kubectl top pod</code> and increase the CPU limit.</li> </ul>"},{"location":"fundamentals/resource-management/#example-2-the-popping-java-app-oomkilled","title":"Example 2: The \"Popping\" Java App (OOMKilled)","text":"<ul> <li>Setting: <code>request: 512Mi</code>, <code>limit: 512Mi</code>.</li> <li>Scenario: The Java Heap grows beyond 512Mi.</li> <li>Result: Status changes to <code>CrashLoopBackOff</code>. <code>kubectl describe pod</code> shows <code>Reason: OOMKilled</code>.</li> <li>Fix: Increase the memory limit or tune the app's memory usage.</li> </ul>"},{"location":"fundamentals/resource-management/#8-summary-cheat-sheet","title":"8. Summary Cheat Sheet","text":"Situation Action How does Scheduler decide? It uses Requests. If a node has 1Gi free, it can fit a pod requesting 512Mi. What if Limits &gt; Node Capacity? This is called Overcommitting. It's fine until everyone tries to use their limits at the same time. Standard units? CPU: <code>m</code> (millicores), Memory: <code>Ei, Pi, Ti, Gi, Mi, Ki</code>."},{"location":"fundamentals/resource-management/#9-how-to-check-usage","title":"9. How to check usage","text":"<pre><code># Check node resources\nkubectl top node\n\n# Check pod resources\nkubectl top pod\n\n# Check if a pod was OOMKilled\nkubectl describe pod &lt;pod-name&gt; | grep -A 5 \"Last State\"\n</code></pre>"},{"location":"fundamentals/resource-quota/","title":"Resource Quotas (The Budget)","text":"<p>A ResourceQuota provides constraints that limit the aggregate resource consumption per namespace. While LimitRange is about the individual, ResourceQuota is about the Total.</p>"},{"location":"fundamentals/resource-quota/#1-the-dining-budget-analogy","title":"1. The \"Dining Budget\" Analogy","text":"Feature LimitRange ResourceQuota Concept Menu Item Price Limit The Total Bill Logic \"Nobody can order a steak that costs more than $50.\" \"The whole table cannot spend more than $200 total.\" Enforcement Individual Containers Entire Namespace"},{"location":"fundamentals/resource-quota/#2-why-tech-companies-love-quotas","title":"2. Why Tech Companies Love Quotas","text":"<p>In large companies (like Google or Netflix), hundreds of teams share the same physical clusters. This is called Multi-tenancy.</p>"},{"location":"fundamentals/resource-quota/#the-tech-sayings-of-resource-management","title":"The \"Tech Sayings\" of Resource Management:","text":"<ul> <li>\"Good fences make good neighbors\": Quotas ensure that one \"hungry\" team doesn't accidentally eat all the CPU in the cluster and crash everyone else's apps.</li> <li>\"Trust but Verify\": You trust your developers to set their resources correctly, but you verify with a Quota to ensure they don't bankrupt the cluster.</li> <li>\"Preventing Cluster Starvation\": Without quotas, a runaway script could request 1000 pods and leave zero space for critical production services.</li> </ul>"},{"location":"fundamentals/resource-quota/#3-real-life-scenarios","title":"3. Real-Life Scenarios","text":""},{"location":"fundamentals/resource-quota/#scenario-a-the-free-tier-vs-paid-tier","title":"Scenario A: The \"Free Tier\" vs \"Paid Tier\"","text":"<p>An Ed-tech company gives students free namespaces to practice.  *   Quota: 1 CPU and 2Gi RAM total per student. *   Outcome: Even if a student writes a loop that spins up 100 pods, they will all fail after the 1st CPU is used. The student's \"mess\" is contained.</p>"},{"location":"fundamentals/resource-quota/#scenario-b-cost-center-billing","title":"Scenario B: Cost Center Billing","text":"<p>A company wants to charge the \"Marketing\" team for their cloud usage.  *   Quota: They are allocated 50 CPU Cores. *   Outcome: If they want more, they have to \"buy\" more quota. This makes cloud costs predictable for the finance department.</p>"},{"location":"fundamentals/resource-quota/#4-practical-example-the-hard-limit","title":"4. Practical Example (The \"Hard Limit\")","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-resources\n  namespace: marketing\nspec:\n  hard:\n    requests.cpu: \"4\"         # Total CPU guaranteed to this team\n    requests.memory: 8Gi      # Total RAM guaranteed to this team\n    limits.cpu: \"10\"          # Total max CPU (burst) allowed\n    limits.memory: 16Gi       # Total max RAM allowed\n    pods: \"20\"                # Max number of pods allowed\n    services: \"5\"             # Max number of load balancers/services\n</code></pre>"},{"location":"fundamentals/resource-quota/#5-the-chicken-and-egg-problem-crucial-tip","title":"5. The \"Chicken and Egg\" Problem (Crucial Tip)","text":"<p>PRO TIP for CKA &amp; PRODUCTION</p> <p>If you have a ResourceQuota in a namespace, every single Pod MUST have its own <code>requests</code> and <code>limits</code> defined. </p> <p>If a developer tries to run a pod without defining them, Kubernetes will REJECT the pod because it doesn't know how to \"bill\" it against the quota. This is why LimitRange and ResourceQuota are almost always used together.</p>"},{"location":"fundamentals/resource-quota/#6-commands-to-check","title":"6. Commands to check","text":"<pre><code># See quotas in the namespace\nkubectl get quota\n\n# See current usage vs limits (Very helpful!)\nkubectl describe quota compute-resources\n</code></pre>"},{"location":"fundamentals/resource-quota/#what-describe-looks-like","title":"What <code>describe</code> looks like:","text":"<pre><code>Resource           Used  Hard\n--------           ----  ----\nrequests.cpu       100m  4\nrequests.memory    256Mi 8Gi\npods               1     20\n</code></pre>"},{"location":"fundamentals/service-types/","title":"Kubernetes Service Types Explained","text":"<p>Understanding Service types is critical for the CKA exam. This guide explains when to use ClusterIP, NodePort, and LoadBalancer.</p>"},{"location":"fundamentals/service-types/#service-types-summary","title":"Service Types Summary","text":"Service Type Accessible From Use Case ClusterIP Inside cluster only Internal communication (microservices) NodePort Outside cluster (via Node IP) Development, testing, simple external access LoadBalancer Outside cluster (via cloud LB) Production external access"},{"location":"fundamentals/service-types/#1-clusterip-internal-only","title":"1. ClusterIP (Internal Only)","text":"<p>Purpose: Communication within the cluster.</p> <p>Who can access it: - \u2705 Other Pods in the cluster - \u2705 Other Services in the cluster - \u274c External users (outside the cluster)</p>"},{"location":"fundamentals/service-types/#example","title":"Example","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  type: ClusterIP        # Default type\n  selector:\n    app: backend\n  ports:\n    - port: 8080\n      targetPort: 8080\n</code></pre>"},{"location":"fundamentals/service-types/#access","title":"Access","text":"<pre><code># From inside a Pod\ncurl http://backend-service:8080\n\n# From outside the cluster\ncurl http://backend-service:8080  # \u274c Won't work!\n</code></pre>"},{"location":"fundamentals/service-types/#use-cases","title":"Use Cases","text":"<ul> <li>Frontend Pod \u2192 Backend Service</li> <li>Backend Pod \u2192 Database Service</li> <li>API Gateway \u2192 Microservices</li> <li>Any internal service-to-service communication</li> </ul>"},{"location":"fundamentals/service-types/#key-points","title":"Key Points","text":"<ul> <li>Default service type (if you don't specify <code>type</code>, you get ClusterIP)</li> <li>Gets a virtual IP that's only routable inside the cluster</li> <li>Most common service type (80% of services are ClusterIP)</li> <li>No external access - completely isolated from outside world</li> </ul>"},{"location":"fundamentals/service-types/#2-nodeport-external-via-node-ip","title":"2. NodePort (External via Node IP)","text":"<p>Purpose: Expose the service outside the cluster using the Node's IP address.</p> <p>Who can access it: - \u2705 Other Pods in the cluster (via ClusterIP) - \u2705 External users (via <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>)</p>"},{"location":"fundamentals/service-types/#example_1","title":"Example","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  type: NodePort\n  selector:\n    app: web\n  ports:\n    - port: 80           # ClusterIP port (internal)\n      targetPort: 8080   # Pod port\n      nodePort: 30080    # External port (30000-32767)\n</code></pre>"},{"location":"fundamentals/service-types/#access_1","title":"Access","text":"<pre><code># From inside the cluster\ncurl http://web-service:80\n\n# From outside the cluster (your laptop, browser)\ncurl http://&lt;node-ip&gt;:30080\n# Example: http://192.168.1.100:30080\n</code></pre>"},{"location":"fundamentals/service-types/#use-cases_1","title":"Use Cases","text":"<ul> <li>Development/testing environments</li> <li>On-premise clusters without cloud load balancers</li> <li>Quick external access for demos</li> <li>Internal corporate applications</li> </ul>"},{"location":"fundamentals/service-types/#key-points_1","title":"Key Points","text":"<ul> <li>Port range: 30000-32767 (configurable, but this is the default)</li> <li>Opens a port on every node in the cluster</li> <li>Includes ClusterIP - you get both internal and external access</li> <li>Not ideal for production - requires knowing node IPs</li> </ul>"},{"location":"fundamentals/service-types/#limitations","title":"Limitations","text":"<ul> <li>You need to know the Node IP address</li> <li>If the node goes down, the IP changes</li> <li>No automatic load balancing across nodes (you need an external LB)</li> <li>Port range is limited (30000-32767)</li> </ul>"},{"location":"fundamentals/service-types/#3-loadbalancer-external-via-cloud-lb","title":"3. LoadBalancer (External via Cloud LB)","text":"<p>Purpose: Expose the service outside the cluster using a cloud load balancer (AWS ELB, GCP LB, Azure LB).</p> <p>Who can access it: - \u2705 Other Pods in the cluster (via ClusterIP) - \u2705 External users (via cloud load balancer IP/DNS)</p>"},{"location":"fundamentals/service-types/#example_2","title":"Example","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: web\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"fundamentals/service-types/#access_2","title":"Access","text":"<pre><code># From inside the cluster\ncurl http://web-service:80\n\n# From outside the cluster (via load balancer)\ncurl http://&lt;load-balancer-ip&gt;:80\n# Example: http://a1b2c3.us-east-1.elb.amazonaws.com\n</code></pre>"},{"location":"fundamentals/service-types/#use-cases_2","title":"Use Cases","text":"<ul> <li>Production web applications</li> <li>Public-facing APIs</li> <li>Any service that needs external access with high availability</li> <li>E-commerce sites, SaaS applications</li> </ul>"},{"location":"fundamentals/service-types/#how-it-works","title":"How It Works","text":"<ol> <li>Kubernetes requests a load balancer from the cloud provider</li> <li>Cloud provider creates the load balancer (AWS ELB, GCP LB, etc.)</li> <li>Load balancer routes traffic to the Nodes</li> <li>Nodes route traffic to the Pods (via kube-proxy)</li> </ol>"},{"location":"fundamentals/service-types/#key-points_2","title":"Key Points","text":"<ul> <li>Requires cloud provider (AWS, GCP, Azure, etc.)</li> <li>Includes NodePort and ClusterIP - you get all three access methods</li> <li>Automatic load balancing across all nodes</li> <li>High availability - survives node failures</li> <li>Costs money - each LoadBalancer service creates a cloud LB ($$$)</li> </ul>"},{"location":"fundamentals/service-types/#limitations_1","title":"Limitations","text":"<ul> <li>Only works in cloud environments (not on-premise or Minikube)</li> <li>Costs money - cloud providers charge for load balancers</li> <li>One LB per service - can get expensive with many services</li> </ul>"},{"location":"fundamentals/service-types/#visual-comparison","title":"Visual Comparison","text":""},{"location":"fundamentals/service-types/#clusterip-internal-only","title":"ClusterIP (Internal Only)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Kubernetes Cluster      \u2502\n\u2502                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502Frontend \u2502\u2500\u2500\u2500\u25b6\u2502 Backend  \u2502   \u2502\n\u2502  \u2502  Pod    \u2502    \u2502 Service  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502ClusterIP \u2502   \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                      \u2502          \u2502\n\u2502                 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502                 \u2502 Backend  \u2502   \u2502\n\u2502                 \u2502   Pod    \u2502   \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2\n         \u2502\n    \u274c External users can't access\n</code></pre>"},{"location":"fundamentals/service-types/#nodeport-external-via-node","title":"NodePort (External via Node)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Kubernetes Cluster      \u2502\n\u2502                                 \u2502\n\u2502  Node IP: 192.168.1.100         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   Web Service        \u2502       \u2502\n\u2502  \u2502   NodePort: 30080    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502             \u2502                    \u2502\n\u2502        \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502        \u2502 Web Pod  \u2502              \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2\n         \u2502\n    \u2705 http://192.168.1.100:30080\n       (External users)\n</code></pre>"},{"location":"fundamentals/service-types/#loadbalancer-external-via-cloud-lb","title":"LoadBalancer (External via Cloud LB)","text":"<pre><code>    \u2601\ufe0f Cloud Load Balancer\n    IP: 54.123.45.67\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Kubernetes Cluster      \u2502\n\u2502                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   Web Service        \u2502       \u2502\n\u2502  \u2502   LoadBalancer       \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502             \u2502                    \u2502\n\u2502        \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502        \u2502 Web Pod  \u2502              \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2\n         \u2502\n    \u2705 http://54.123.45.67\n       (External users)\n</code></pre>"},{"location":"fundamentals/service-types/#quick-decision-guide","title":"Quick Decision Guide","text":"<p>Question: Who needs to access the service?</p>"},{"location":"fundamentals/service-types/#internal-communication-only-pod-pod","title":"Internal communication only (Pod \u2192 Pod)","text":"<p><pre><code>type: ClusterIP  # Default\n</code></pre> Example: Frontend \u2192 Backend, Backend \u2192 Database</p>"},{"location":"fundamentals/service-types/#external-access-developmenttesting","title":"External access (development/testing)","text":"<p><pre><code>type: NodePort\n</code></pre> Example: Testing your app from your laptop</p>"},{"location":"fundamentals/service-types/#external-access-production","title":"External access (production)","text":"<p><pre><code>type: LoadBalancer\n</code></pre> Example: Public-facing website, API</p>"},{"location":"fundamentals/service-types/#the-hierarchy","title":"The Hierarchy","text":"<p>Important concept: Each service type builds on the previous one.</p> <pre><code>LoadBalancer\n    \u2193 includes\nNodePort\n    \u2193 includes\nClusterIP\n</code></pre>"},{"location":"fundamentals/service-types/#what-this-means","title":"What This Means","text":"<p>ClusterIP: - Internal access only</p> <p>NodePort: - Internal access (ClusterIP) - External access via <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code></p> <p>LoadBalancer: - Internal access (ClusterIP) - External access via <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code> - External access via <code>&lt;LoadBalancer-IP&gt;:&lt;Port&gt;</code></p> <p>In other words: - LoadBalancer gives you all three access methods - NodePort gives you two (internal + external via node) - ClusterIP gives you one (internal only)</p>"},{"location":"fundamentals/service-types/#common-exam-scenarios","title":"Common Exam Scenarios","text":""},{"location":"fundamentals/service-types/#scenario-1-expose-the-deployment-internally","title":"Scenario 1: \"Expose the deployment internally\"","text":"<pre><code>kubectl expose deployment backend --port=8080 --type=ClusterIP\n</code></pre>"},{"location":"fundamentals/service-types/#scenario-2-expose-the-deployment-externally-for-testing","title":"Scenario 2: \"Expose the deployment externally for testing\"","text":"<pre><code>kubectl expose deployment web --port=80 --type=NodePort\n</code></pre>"},{"location":"fundamentals/service-types/#scenario-3-expose-the-deployment-to-the-internet","title":"Scenario 3: \"Expose the deployment to the internet\"","text":"<pre><code>kubectl expose deployment web --port=80 --type=LoadBalancer\n</code></pre>"},{"location":"fundamentals/service-types/#scenario-4-create-a-service-for-a-database","title":"Scenario 4: \"Create a service for a database\"","text":"<pre><code># Databases should be internal only\nkubectl expose deployment postgres --port=5432 --type=ClusterIP\n</code></pre>"},{"location":"fundamentals/service-types/#port-terminology","title":"Port Terminology","text":"<p>Understanding the different port fields:</p> <pre><code>ports:\n  - port: 80           # Service port (what clients connect to)\n    targetPort: 8080   # Pod port (where the container listens)\n    nodePort: 30080    # Node port (for NodePort/LoadBalancer only)\n</code></pre>"},{"location":"fundamentals/service-types/#the-flow","title":"The Flow","text":"<p>For ClusterIP: <pre><code>Client \u2192 Service:port (80) \u2192 Pod:targetPort (8080)\n</code></pre></p> <p>For NodePort: <pre><code>External Client \u2192 Node:nodePort (30080) \u2192 Service:port (80) \u2192 Pod:targetPort (8080)\n</code></pre></p> <p>For LoadBalancer: <pre><code>External Client \u2192 LB:port (80) \u2192 Node:nodePort (30080) \u2192 Service:port (80) \u2192 Pod:targetPort (8080)\n</code></pre></p>"},{"location":"fundamentals/service-types/#summary","title":"Summary","text":"Feature ClusterIP NodePort LoadBalancer Internal Access \u2705 Yes \u2705 Yes \u2705 Yes External Access \u274c No \u2705 Yes (via Node IP) \u2705 Yes (via LB IP) Port Range Any 30000-32767 Any Cloud Required \u274c No \u274c No \u2705 Yes Cost Free Free $$$ (LB cost) Use Case Internal services Dev/Test Production High Availability N/A \u274c No \u2705 Yes <p>Remember: - ClusterIP = Internal only (default) - NodePort = External via Node IP (dev/test) - LoadBalancer = External via Cloud LB (production)</p> <p>For the CKA exam: - Most services will be ClusterIP (internal communication) - Use NodePort for quick external access in test scenarios - Use LoadBalancer when the question mentions \"public access\" or \"internet-facing\"</p>"},{"location":"fundamentals/static-pods/","title":"Understanding Static Pods in Kubernetes","text":""},{"location":"fundamentals/static-pods/#what-are-static-pods","title":"What are Static Pods?","text":"<p>Static Pods are pods that are managed directly by the kubelet on a specific node, without the Kubernetes API server or any controllers (like Deployments or ReplicaSets) managing them.</p> <p>Key Difference</p> <ul> <li>Regular Pods: Created and managed by the API server and controllers</li> <li>Static Pods: Created and managed directly by kubelet on each node</li> </ul>"},{"location":"fundamentals/static-pods/#how-static-pods-work","title":"How Static Pods Work","text":""},{"location":"fundamentals/static-pods/#the-basic-concept","title":"The Basic Concept","text":"<pre><code>Regular Pod Flow:\nUser \u2192 kubectl \u2192 API Server \u2192 Scheduler \u2192 Kubelet \u2192 Pod\n\nStatic Pod Flow:\nManifest file on node \u2192 Kubelet reads it \u2192 Kubelet creates Pod\n(No API server, no scheduler involved!)\n</code></pre>"},{"location":"fundamentals/static-pods/#where-kubelet-gets-manifests","title":"Where Kubelet Gets Manifests","text":"<p>The kubelet watches a specific directory on the node's filesystem for YAML/JSON pod manifests:</p> <p>Default location (configurable): <pre><code>/etc/kubernetes/manifests/\n</code></pre></p> <p>This path is defined in the kubelet configuration.</p>"},{"location":"fundamentals/static-pods/#the-process","title":"The Process","text":"<ol> <li>You place a pod manifest in <code>/etc/kubernetes/manifests/</code></li> <li>Kubelet watches this directory (filesystem monitoring)</li> <li>Kubelet reads the manifest and creates the pod</li> <li>Kubelet manages the pod directly</li> <li>If the pod crashes, kubelet restarts it automatically</li> <li>If you delete the manifest file, kubelet deletes the pod</li> <li>If you modify the manifest file, kubelet updates the pod</li> </ol> <p>Self-Healing</p> <p>Static pods are automatically restarted by kubelet if they crash, just like regular pods.</p>"},{"location":"fundamentals/static-pods/#configuration","title":"Configuration","text":""},{"location":"fundamentals/static-pods/#finding-the-static-pod-path","title":"Finding the Static Pod Path","text":"<p>The static pod path is configured in the kubelet config file:</p> <pre><code># Check kubelet config\ncat /var/lib/kubelet/config.yaml\n</code></pre> <p>Look for: <pre><code>staticPodPath: /etc/kubernetes/manifests\n</code></pre></p> <p>Or in the kubelet service file: <pre><code>systemctl status kubelet\n# Look for --pod-manifest-path flag\n</code></pre></p>"},{"location":"fundamentals/static-pods/#alternative-configuration-methods","title":"Alternative Configuration Methods","text":"<p>Method 1: Static Pod Path (Recommended) <pre><code># /var/lib/kubelet/config.yaml\nstaticPodPath: /etc/kubernetes/manifests\n</code></pre></p> <p>Method 2: Manifest URL <pre><code># /var/lib/kubelet/config.yaml\nstaticPodURL: http://example.com/manifests/\n</code></pre></p> <p>Kubelet can fetch manifests from a URL (less common).</p>"},{"location":"fundamentals/static-pods/#creating-a-static-pod","title":"Creating a Static Pod","text":""},{"location":"fundamentals/static-pods/#example-creating-a-static-nginx-pod","title":"Example: Creating a Static Nginx Pod","text":"<p>Step 1: Create the manifest</p> <pre><code># SSH to the node\nssh user@node1\n\n# Create manifest in the static pod directory\nsudo vim /etc/kubernetes/manifests/static-nginx.yaml\n</code></pre> <p>Step 2: Add the pod definition</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: static-nginx\n  labels:\n    app: nginx\n    type: static\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.19\n    ports:\n    - containerPort: 80\n</code></pre> <p>Step 3: Save the file</p> <p>The kubelet automatically detects the file and creates the pod!</p> <p>Step 4: Verify</p> <pre><code># On the node\nsudo crictl pods\n# You'll see static-nginx running\n\n# From master node (if API server is running)\nkubectl get pods -A\n# You'll see static-nginx-node1 (note the node name suffix)\n</code></pre>"},{"location":"fundamentals/static-pods/#naming-convention","title":"Naming Convention","text":"<p>Static pods appear in <code>kubectl get pods</code> with the node name as a suffix:</p> <pre><code>Pod manifest name: static-nginx\nActual pod name:   static-nginx-node1\n                                 ^^^^^ node hostname appended\n</code></pre>"},{"location":"fundamentals/static-pods/#why-static-pods-are-needed","title":"Why Static Pods are Needed","text":""},{"location":"fundamentals/static-pods/#1-bootstrapping-the-control-plane","title":"1. Bootstrapping the Control Plane","text":"<p>The Chicken-and-Egg Problem:</p> <p>How do you run the API server if you need the API server to run pods?</p> <p>Answer: Static Pods!</p> <p>On a kubeadm-created cluster, the control plane components run as static pods:</p> <pre><code># On master node\nls /etc/kubernetes/manifests/\n\n# Output:\netcd.yaml\nkube-apiserver.yaml\nkube-controller-manager.yaml\nkube-scheduler.yaml\n</code></pre> <p>The Bootstrap Process:</p> <ol> <li>Node boots up</li> <li>Kubelet starts (it's a system service)</li> <li>Kubelet reads <code>/etc/kubernetes/manifests/</code></li> <li>Kubelet starts:</li> <li>etcd (static pod)</li> <li>kube-apiserver (static pod)</li> <li>kube-controller-manager (static pod)</li> <li>kube-scheduler (static pod)</li> <li>Now the control plane is running!</li> <li>Now the API server can manage regular pods</li> </ol> <p>Self-Hosting</p> <p>This is called \"self-hosting\" - Kubernetes manages itself using its own mechanisms.</p>"},{"location":"fundamentals/static-pods/#2-critical-node-specific-services","title":"2. Critical Node-Specific Services","text":"<p>Services that must run on specific nodes, independent of the control plane:</p> <ul> <li>Monitoring agents that must run even if the control plane is down</li> <li>Logging agents that collect kubelet logs</li> <li>Node-critical workloads that need guaranteed placement</li> </ul>"},{"location":"fundamentals/static-pods/#3-air-gapped-or-isolated-environments","title":"3. Air-Gapped or Isolated Environments","text":"<p>In environments without API server access:</p> <ul> <li>Edge computing nodes</li> <li>IoT devices</li> <li>Disconnected/offline scenarios</li> <li>Disaster recovery scenarios</li> </ul>"},{"location":"fundamentals/static-pods/#4-cluster-recovery","title":"4. Cluster Recovery","text":"<p>If the control plane crashes but nodes are still running:</p> <ul> <li>Static pods keep running</li> <li>Critical services remain available</li> <li>You can troubleshoot and recover</li> </ul>"},{"location":"fundamentals/static-pods/#what-happens-when-theres-no-master-node","title":"What Happens When There's No Master Node?","text":"<p>This is where static pods shine!</p>"},{"location":"fundamentals/static-pods/#scenario-master-node-is-down","title":"Scenario: Master Node is Down","text":"<pre><code>Before (Normal Operation):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Master Node \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Worker Node  \u2502\n\u2502 API Server  \u2502 manages \u2502 Regular Pods \u2502\n\u2502 Scheduler   \u2502         \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAfter (Master Down):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Master Node \u2502  XXXX   \u2502 Worker Node  \u2502\n\u2502   (DOWN)    \u2502  DEAD   \u2502 Static Pods  \u2502\n\u2502             \u2502         \u2502 STILL RUNNING\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"fundamentals/static-pods/#what-happens-to-different-pod-types","title":"What Happens to Different Pod Types","text":"<p>Static Pods: <pre><code>\u2705 Keep running\n\u2705 Kubelet restarts them if they crash\n\u2705 Completely independent of master\n\u2705 Can still be managed via manifest files\n</code></pre></p> <p>Regular Pods (DaemonSets, Deployments, etc.): <pre><code>\u2705 Keep running (for now)\n\u274c If they crash, they won't be restarted\n\u274c No new pods can be scheduled\n\u274c Cannot be updated or scaled\n\u26a0\ufe0f  Gradually become unhealthy\n</code></pre></p>"},{"location":"fundamentals/static-pods/#practical-example","title":"Practical Example","text":"<p>Setup: <pre><code># Master node runs control plane as static pods\n/etc/kubernetes/manifests/\n  \u251c\u2500\u2500 etcd.yaml\n  \u251c\u2500\u2500 kube-apiserver.yaml\n  \u251c\u2500\u2500 kube-controller-manager.yaml\n  \u2514\u2500\u2500 kube-scheduler.yaml\n\n# Worker node runs application as regular pod\n# Plus a monitoring agent as static pod\n/etc/kubernetes/manifests/\n  \u2514\u2500\u2500 node-monitoring.yaml\n</code></pre></p> <p>Master fails: <pre><code>Master Node:\n  \u274c API server - down\n  \u274c Scheduler - down\n  \u274c Controller manager - down\n\nWorker Node:\n  \u2705 Static monitoring pod - still running\n  \u2705 Application pods - still running (for now)\n  \u274c If app pod crashes - won't restart (no controller)\n  \u2705 If static pod crashes - kubelet restarts it\n</code></pre></p> <p>Recovery: <pre><code># Even with master down, you can still manage static pods\n\n# Add new static pod on worker\nssh worker-node\nsudo vim /etc/kubernetes/manifests/debug-pod.yaml\n# Pod starts immediately!\n\n# Remove static pod\nsudo rm /etc/kubernetes/manifests/debug-pod.yaml\n# Pod stops immediately!\n</code></pre></p>"},{"location":"fundamentals/static-pods/#static-pods-vs-regular-pods","title":"Static Pods vs Regular Pods","text":"Feature Static Pods Regular Pods Managed by Kubelet directly API server + controllers Manifest location Node filesystem etcd (via API server) Survives master failure \u2705 Yes \u274c No (won't restart if crashed) Can be created via kubectl \u274c No \u2705 Yes Visible in kubectl get pods \u2705 Yes (as mirror pods) \u2705 Yes Can be deleted via kubectl \u274c No (must delete manifest file) \u2705 Yes Scheduled by scheduler \u274c No (always on same node) \u2705 Yes Node binding \u2705 Permanent (tied to node) \u274c Can be rescheduled Restart policy \u2705 Kubelet restarts \u2705 Controller restarts"},{"location":"fundamentals/static-pods/#mirror-pods","title":"Mirror Pods","text":"<p>When a static pod is created, kubelet creates a mirror pod in the API server (if it's available).</p>"},{"location":"fundamentals/static-pods/#what-is-a-mirror-pod","title":"What is a Mirror Pod?","text":"<p>A read-only representation of the static pod in the API server:</p> <pre><code>Node Filesystem:           API Server (etcd):\n/etc/kubernetes/manifests/ \n  \u2514\u2500\u2500 nginx.yaml    \u2500\u2500\u2500\u2500\u25b6  Mirror Pod: nginx-node1\n                           (read-only reflection)\n</code></pre> <p>Characteristics: - \u2705 Visible via <code>kubectl get pods</code> - \u274c Cannot be deleted via <code>kubectl delete</code> - \u274c Cannot be modified via <code>kubectl edit</code> - \u2705 Shows status and logs - \u2705 Automatically updated when manifest changes</p> <p>Identifying Mirror Pods:</p> <pre><code>kubectl get pod static-nginx-node1 -o yaml\n</code></pre> <p>Look for this annotation: <pre><code>metadata:\n  annotations:\n    kubernetes.io/config.mirror: \"true\"  # This is a mirror pod!\n  ownerReferences:\n  - apiVersion: v1\n    kind: Node\n    name: node1\n    uid: ...\n</code></pre></p>"},{"location":"fundamentals/static-pods/#trying-to-delete-a-mirror-pod","title":"Trying to Delete a Mirror Pod","text":"<pre><code># This appears to work...\nkubectl delete pod static-nginx-node1\n# pod \"static-nginx-node1\" deleted\n\n# But the pod comes right back!\nkubectl get pods\n# NAME                  READY   STATUS    RESTARTS   AGE\n# static-nginx-node1    1/1     Running   0          3s\n\n# Because kubelet recreates it from the manifest file!\n</code></pre> <p>To actually delete it: <pre><code># SSH to the node\nssh node1\n\n# Delete the manifest file\nsudo rm /etc/kubernetes/manifests/static-nginx.yaml\n\n# Now it's really gone\nkubectl get pods\n# No resources found\n</code></pre></p>"},{"location":"fundamentals/static-pods/#use-cases-and-scenarios","title":"Use Cases and Scenarios","text":""},{"location":"fundamentals/static-pods/#use-case-1-control-plane-components","title":"Use Case 1: Control Plane Components","text":"<p>Scenario: Running Kubernetes control plane itself</p> <pre><code># On master node\nls /etc/kubernetes/manifests/\n</code></pre> <p>Manifests: <pre><code>etcd.yaml                    # Database for cluster state\nkube-apiserver.yaml          # API server\nkube-controller-manager.yaml # Controllers\nkube-scheduler.yaml          # Scheduler\n</code></pre></p> <p>Why static pods? - Control plane needs to start before API server is available - Self-hosting: Kubernetes manages its own components - Survives control plane failures (can restart itself)</p>"},{"location":"fundamentals/static-pods/#use-case-2-node-monitoring-agent","title":"Use Case 2: Node Monitoring Agent","text":"<p>Scenario: Critical monitoring that must always run</p> <pre><code># /etc/kubernetes/manifests/node-exporter.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: node-exporter\n  labels:\n    app: monitoring\nspec:\n  hostNetwork: true\n  hostPID: true\n  containers:\n  - name: node-exporter\n    image: prom/node-exporter:latest\n    ports:\n    - containerPort: 9100\n    volumeMounts:\n    - name: proc\n      mountPath: /host/proc\n      readOnly: true\n    - name: sys\n      mountPath: /host/sys\n      readOnly: true\n  volumes:\n  - name: proc\n    hostPath:\n      path: /proc\n  - name: sys\n    hostPath:\n      path: /sys\n</code></pre> <p>Why static pod? - Runs even if master is down - Monitors node health independently - Guaranteed to be on specific node</p>"},{"location":"fundamentals/static-pods/#use-case-3-edge-computing","title":"Use Case 3: Edge Computing","text":"<p>Scenario: IoT device with intermittent master connectivity</p> <pre><code>Cloud (Master):              Edge Device (Worker):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 API Server   \u2502            \u2502 Kubelet             \u2502\n\u2502 (Sometimes   \u2502 \u25c0\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u25b6\u2502 Static Pods:        \u2502\n\u2502  unreachable)\u2502      \u2502     \u2502  - Data collector   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502     \u2502  - Local processor  \u2502\n                      \u2502     \u2502  - Cache service    \u2502\n        Intermittent  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        connection    \u2502\n                      \u2514\u2500\u2500\u2500 Works offline!\n</code></pre> <p>Static pods on edge device: <pre><code>/etc/kubernetes/manifests/\n  \u251c\u2500\u2500 data-collector.yaml    # Collects sensor data\n  \u251c\u2500\u2500 local-processor.yaml   # Processes data locally\n  \u2514\u2500\u2500 cache-service.yaml     # Caches data for sync\n</code></pre></p> <p>Benefits: - Works offline when master is unreachable - Guaranteed to run on edge device - Kubelet manages lifecycle independently</p>"},{"location":"fundamentals/static-pods/#use-case-4-disaster-recovery","title":"Use Case 4: Disaster Recovery","text":"<p>Scenario: Control plane crashed, need to debug</p> <pre><code># Master is completely down\n# But you can still deploy debug tools as static pods!\n\n# SSH to worker node\nssh worker-node1\n\n# Create debug pod\ncat &lt;&lt;EOF | sudo tee /etc/kubernetes/manifests/debug.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: debug-tools\nspec:\n  hostNetwork: true\n  containers:\n  - name: debug\n    image: nicolaka/netshoot\n    command: [\"sleep\", \"infinity\"]\nEOF\n\n# Pod starts immediately!\n# Now you can exec into it and debug\nsudo crictl exec -it &lt;container-id&gt; bash\n</code></pre>"},{"location":"fundamentals/static-pods/#use-case-5-custom-node-services","title":"Use Case 5: Custom Node Services","text":"<p>Scenario: Node-specific service that must run on exact node</p> <pre><code># /etc/kubernetes/manifests/local-cache.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: local-cache\nspec:\n  containers:\n  - name: redis\n    image: redis:alpine\n    volumeMounts:\n    - name: cache-data\n      mountPath: /data\n  volumes:\n  - name: cache-data\n    hostPath:\n      path: /mnt/ssd/redis-cache  # Fast local SSD\n      type: DirectoryOrCreate\n</code></pre> <p>Why static pod? - Guaranteed placement on node with specific hardware (SSD) - Uses local node storage - No risk of being rescheduled elsewhere</p>"},{"location":"fundamentals/static-pods/#managing-static-pods","title":"Managing Static Pods","text":""},{"location":"fundamentals/static-pods/#creating","title":"Creating","text":"<pre><code># SSH to node\nssh node1\n\n# Create manifest\nsudo vim /etc/kubernetes/manifests/my-pod.yaml\n\n# Kubelet detects and creates it automatically\n</code></pre>"},{"location":"fundamentals/static-pods/#updating","title":"Updating","text":"<pre><code># Edit the manifest file\nsudo vim /etc/kubernetes/manifests/my-pod.yaml\n\n# Kubelet detects changes and recreates the pod\n# (Old pod deleted, new pod created)\n</code></pre>"},{"location":"fundamentals/static-pods/#deleting","title":"Deleting","text":"<pre><code># Remove the manifest file\nsudo rm /etc/kubernetes/manifests/my-pod.yaml\n\n# Kubelet detects deletion and removes the pod\n</code></pre>"},{"location":"fundamentals/static-pods/#viewing-logs","title":"Viewing Logs","text":"<pre><code># From node (using crictl)\nsudo crictl logs &lt;container-id&gt;\n\n# From master (using kubectl, if API server is up)\nkubectl logs static-pod-name-node1\n</code></pre>"},{"location":"fundamentals/static-pods/#debugging","title":"Debugging","text":"<pre><code># Check if kubelet is watching the directory\nsudo journalctl -u kubelet -f\n\n# Check pod status on node\nsudo crictl pods\n\n# Check pod status via kubectl\nkubectl get pods -A -o wide | grep node1\n\n# Describe the mirror pod\nkubectl describe pod static-pod-name-node1\n</code></pre>"},{"location":"fundamentals/static-pods/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"fundamentals/static-pods/#issue-1-static-pod-not-starting","title":"Issue 1: Static Pod Not Starting","text":"<p>Problem: <pre><code># Created manifest but pod doesn't start\nsudo vim /etc/kubernetes/manifests/test.yaml\n# ... wait ...\ncrictl pods  # Pod not showing up\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check kubelet is running: <pre><code>sudo systemctl status kubelet\n</code></pre></p> </li> <li> <p>Check kubelet logs: <pre><code>sudo journalctl -u kubelet -f\n# Look for errors parsing manifest\n</code></pre></p> </li> <li> <p>Verify static pod path: <pre><code>cat /var/lib/kubelet/config.yaml | grep staticPodPath\n</code></pre></p> </li> <li> <p>Check manifest syntax: <pre><code># Validate YAML\nkubectl apply --dry-run=client -f /etc/kubernetes/manifests/test.yaml\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/static-pods/#issue-2-cannot-delete-static-pod","title":"Issue 2: Cannot Delete Static Pod","text":"<p>Problem: <pre><code>kubectl delete pod static-nginx-node1\n# pod \"static-nginx-node1\" deleted\n\nkubectl get pods\n# Pod comes right back!\n</code></pre></p> <p>Solution: <pre><code># You must delete the manifest file on the node\nssh node1\nsudo rm /etc/kubernetes/manifests/static-nginx.yaml\n</code></pre></p>"},{"location":"fundamentals/static-pods/#issue-3-static-pod-path-changed","title":"Issue 3: Static Pod Path Changed","text":"<p>Problem: <pre><code># Changed kubelet config but pods not loading\n</code></pre></p> <p>Solution: <pre><code># Restart kubelet after changing config\nsudo systemctl restart kubelet\n\n# Verify new path is loaded\nsudo journalctl -u kubelet | grep staticPodPath\n</code></pre></p>"},{"location":"fundamentals/static-pods/#static-pods-in-cka-exam","title":"Static Pods in CKA Exam","text":""},{"location":"fundamentals/static-pods/#common-tasks","title":"Common Tasks","text":"<p>Task 1: Create a static pod</p> <pre><code># SSH to the specified node\nssh node01\n\n# Create manifest in static pod directory\nsudo vim /etc/kubernetes/manifests/static-web.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: static-web\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Task 2: Identify static pods</p> <pre><code># Look for pods with node name suffix\nkubectl get pods -A\n\n# Check for mirror pod annotation\nkubectl get pod &lt;pod-name&gt; -o yaml | grep config.mirror\n</code></pre> <p>Task 3: Delete a static pod</p> <pre><code># Identify which node it's on\nkubectl get pod static-pod-node01 -o wide\n\n# SSH to that node\nssh node01\n\n# Find and delete manifest\nls /etc/kubernetes/manifests/\nsudo rm /etc/kubernetes/manifests/static-pod.yaml\n</code></pre> <p>Task 4: Find static pod manifest path</p> <pre><code># Check kubelet config\ncat /var/lib/kubelet/config.yaml | grep staticPodPath\n\n# Or check systemd service\nps aux | grep kubelet | grep pod-manifest-path\n</code></pre>"},{"location":"fundamentals/static-pods/#best-practices","title":"Best Practices","text":""},{"location":"fundamentals/static-pods/#dos","title":"Do's","text":"<p>\u2705 Use for control plane components - Industry standard \u2705 Use for critical node services - Monitoring, logging \u2705 Use for guaranteed placement - When pod MUST be on specific node \u2705 Use for offline scenarios - Edge computing, air-gapped \u2705 Keep manifests simple - Static pods should be straightforward  </p>"},{"location":"fundamentals/static-pods/#donts","title":"Don'ts","text":"<p>\u274c Don't use for regular applications - Use Deployments instead \u274c Don't use for scaling - Static pods don't scale (one per node) \u274c Don't use for stateful sets - Use StatefulSets \u274c Don't use for multi-node deployments - Use DaemonSets \u274c Don't modify via kubectl - Edit manifest files directly  </p>"},{"location":"fundamentals/static-pods/#comparison-with-daemonsets","title":"Comparison with DaemonSets","text":"<p>Both run one pod per node, but they're different:</p> Feature Static Pods DaemonSets Managed by Kubelet DaemonSet controller (API server) Survives master failure \u2705 Yes \u274c No Node selection \u274c Fixed to one node \u2705 All nodes (or selected) Managed via Manifest files kubectl / YAML Use case Control plane, critical services Cluster-wide agents <p>When to use which:</p> <ul> <li>Static Pods: Control plane, critical single-node services</li> <li>DaemonSets: Cluster-wide agents (logging, monitoring, networking)</li> </ul>"},{"location":"fundamentals/static-pods/#summary","title":"Summary","text":""},{"location":"fundamentals/static-pods/#key-points","title":"Key Points","text":"<p>Static Pods Essentials</p> <p>\u2705 Managed directly by kubelet, not API server \u2705 Manifests stored in <code>/etc/kubernetes/manifests/</code> (default) \u2705 Survive master node failures \u2705 Cannot be deleted via kubectl \u2705 Always run on the same specific node \u2705 Used for control plane components \u2705 Create mirror pods in API server (when available)  </p>"},{"location":"fundamentals/static-pods/#when-to-use-static-pods","title":"When to Use Static Pods","text":"<ol> <li>Control plane components (API server, scheduler, etc.)</li> <li>Critical node-specific services (monitoring, logging)</li> <li>Edge computing scenarios</li> <li>Air-gapped environments</li> <li>Disaster recovery situations</li> <li>Services requiring guaranteed node placement</li> </ol>"},{"location":"fundamentals/static-pods/#quick-reference","title":"Quick Reference","text":"<pre><code># Find static pod path\ncat /var/lib/kubelet/config.yaml | grep staticPodPath\n\n# Create static pod\nsudo vim /etc/kubernetes/manifests/my-pod.yaml\n\n# Delete static pod\nsudo rm /etc/kubernetes/manifests/my-pod.yaml\n\n# View static pods (as mirror pods)\nkubectl get pods -A | grep &lt;node-name&gt;\n\n# Identify static pod\nkubectl get pod &lt;name&gt; -o yaml | grep config.mirror\n\n# Check kubelet logs\nsudo journalctl -u kubelet -f\n</code></pre> <p>Static pods are a powerful mechanism for running critical services independently of the Kubernetes control plane, providing resilience and enabling self-hosted clusters.</p>"},{"location":"fundamentals/pod-scheduling/","title":"Pod Scheduling in Kubernetes","text":"<p>Scheduling is the process of assigning a Pod to a specific Node in the cluster. This is primarily handled by the kube-scheduler.</p>"},{"location":"fundamentals/pod-scheduling/#how-the-kube-scheduler-works","title":"\ud83c\udfd7\ufe0f How the Kube-Scheduler Works","text":"<p>The scheduler follows a two-step process to decide where a pod should live:</p> <ol> <li>Filtering (Predicates): The scheduler finds all nodes that meet the pod's requirements (e.g., enough CPU/Memory, matching <code>nodeSelector</code>).</li> <li>Scoring (Priorities): The scheduler ranks the remaining nodes using a set of scoring rules (e.g., balance load across nodes, favor nodes with existing images) to find the \"best\" fit.</li> </ol> <p>[!NOTE] If no nodes pass the filtering stage, the pod remains in <code>Pending</code> state.</p>"},{"location":"fundamentals/pod-scheduling/#scheduling-methods","title":"\ud83d\udee0\ufe0f Scheduling Methods","text":""},{"location":"fundamentals/pod-scheduling/#1-manual-scheduling-nodename","title":"1. Manual Scheduling (<code>nodeName</code>)","text":"<p>The simplest way to manually schedule a pod is by adding the <code>nodeName</code> field to the pod specification. When this field is present, the kube-scheduler completely ignores the pod, and the kubelet on the specified node attempts to run it.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: manual-pod\nspec:\n  nodeName: worker-1  # Directly assign to this node\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>[!IMPORTANT] Immutability: The <code>nodeName</code> field is immutable. You can only set it at the time of pod creation. If you try to add <code>nodeName</code> to an existing pod using <code>kubectl edit</code>, Kubernetes will reject the change. To move a pod, you must delete and recreate it.</p>"},{"location":"fundamentals/pod-scheduling/#2-under-the-hood-the-binding-object","title":"2. Under the Hood: The Binding Object","text":"<p>When the scheduler assigns a pod to a node, it doesn't actually edit the Pod object. Instead, it creates a Binding object. </p> <p>A Binding object is a temporary resource that tells the API server: \"Link Pod A to Node B.\"</p>"},{"location":"fundamentals/pod-scheduling/#scenario-scheduling-without-a-scheduler","title":"Scenario: Scheduling without a Scheduler","text":"<p>If the <code>kube-scheduler</code> is down or missing, you can manually schedule a pod by mimicking what the scheduler does: creating a Binding object via the API.</p> <ol> <li>Identify Unscheduled Pods: Use <code>kubectl get pods</code> to find pods in the <code>Pending</code> state.</li> <li>Create a Binding JSON:     <pre><code>{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Binding\",\n    \"metadata\": {\n        \"name\": \"manual-pod\"\n    },\n    \"target\": {\n        \"apiVersion\": \"v1\",\n        \"kind\": \"Node\",\n        \"name\": \"worker-1\"\n    }\n}\n</code></pre></li> <li>Submit to the API: Send a POST request to the pod's binding endpoint.     <pre><code>curl -X POST -H \"Content-Type: application/json\" \\\n  --data @binding.json \\\n  http://localhost:8001/api/v1/namespaces/default/pods/manual-pod/binding\n</code></pre></li> </ol>"},{"location":"fundamentals/pod-scheduling/#3-node-selector","title":"3. Node Selector","text":"<p>A simple way to constrain pods to nodes with specific labels.</p> <pre><code>spec:\n  nodeSelector:\n    disk: ssd\n</code></pre>"},{"location":"fundamentals/pod-scheduling/#3-node-affinity","title":"3. Node Affinity","text":"<p>A more powerful and expressive way to handle scheduling logic.</p> <ul> <li>Required (Hard Rule): <code>requiredDuringSchedulingIgnoredDuringExecution</code></li> <li>Preferred (Soft Rule): <code>preferredDuringSchedulingIgnoredDuringExecution</code></li> </ul> <pre><code>spec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values: [\"us-east-1a\"]\n</code></pre>"},{"location":"fundamentals/pod-scheduling/#4-taints-tolerations-repelling-pods","title":"4. Taints &amp; Tolerations (Repelling Pods)","text":"<p>Taints are applied to Nodes to repel pods. Tolerations are applied to Pods to allow them to \"tolerate\" the taint.</p> <p>Read the detailed guide on Taints and Tolerations here \u2192</p> <pre><code>spec:\n  tolerations:\n  - key: \"key\"\n    operator: \"Equal\"\n    value: \"value\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"fundamentals/pod-scheduling/#resource-based-scheduling","title":"\u2696\ufe0f Resource-Based Scheduling","text":"<p>The scheduler heavily relies on Requests and Limits to manage node capacity.</p> <ul> <li>Requests: What the pod is guaranteed to get. The scheduler uses this to find a node with enough space.</li> <li>Limits: The maximum amount of resources a pod can consume.</li> </ul> <pre><code>spec:\n  containers:\n  - name: app\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre>"},{"location":"fundamentals/pod-scheduling/#summary-table","title":"\ud83d\udccb Summary Table","text":"Method Target Logic Use Case nodeName Pod Direct Assignment Testing / Edge cases nodeSelector Pod Label Matching Simple Hardware requirements (SSD) Node Affinity Pod Expressive Rules Multi-zone, complex logic Taints/Tolerations Node/Pod Repelling/Allowing Dedicated nodes, Control Plane isolation"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/","title":"Multiple Schedulers in Kubernetes","text":"<p>Kubernetes allows you to run multiple schedulers simultaneously. This means you can have the default scheduler plus one or more custom schedulers, and different pods can choose which scheduler to use.</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#1-the-restaurant-hosts-analogy","title":"1. The \"Restaurant Hosts\" Analogy","text":"<p>Think of schedulers like different hosts at a restaurant: *   Default Scheduler: The main host who seats most customers using standard rules. *   VIP Scheduler: A special host who only seats VIP guests using custom criteria. *   Express Scheduler: Another host who seats customers with special \"quick service\" needs.</p> <p>Each pod can choose which \"host\" (scheduler) will seat it (place it on a node).</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#2-why-use-multiple-schedulers","title":"2. Why Use Multiple Schedulers?","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#common-use-cases","title":"Common Use Cases","text":"Scenario Why Custom Scheduler? GPU Workloads You need complex GPU affinity logic that the default scheduler doesn't handle well. Machine Learning Custom placement based on model size, training phases, or data locality. High-Performance Computing Specialized scheduling for tightly-coupled parallel jobs. Multi-Tenancy Different scheduling policies for different teams/tenants. Legacy Applications Applications that need specific node selection logic."},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#3-how-it-works","title":"3. How It Works","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#the-default-scheduler","title":"The Default Scheduler","text":"<p>Every Kubernetes cluster starts with one scheduler: *   Name: <code>default-scheduler</code> *   Runs as: A static pod in the <code>kube-system</code> namespace *   Manifest: <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code> (on master node)</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#adding-a-custom-scheduler","title":"Adding a Custom Scheduler","text":"<p>You can deploy additional schedulers as: 1.  Deployment (Recommended): Regular deployment in any namespace 2.  Static Pod: Another static pod on the master node 3.  External Process: Running outside the cluster</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#4-deploying-a-custom-scheduler","title":"4. Deploying a Custom Scheduler","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-1-create-the-custom-scheduler-deployment","title":"Step 1: Create the Custom Scheduler Deployment","text":"<pre><code># custom-scheduler.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-scheduler\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: my-scheduler-as-kube-scheduler\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-scheduler\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: my-scheduler\n  template:\n    metadata:\n      labels:\n        component: my-scheduler\n    spec:\n      serviceAccountName: my-scheduler\n      containers:\n      - name: my-scheduler\n        image: registry.k8s.io/kube-scheduler:v1.28.0\n        command:\n        - kube-scheduler\n        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n        - --kubeconfig=/etc/kubernetes/scheduler.conf\n        - --leader-elect=false\n        - --scheduler-name=my-custom-scheduler\n        volumeMounts:\n        - name: config\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: config\n        hostPath:\n          path: /etc/kubernetes\n</code></pre> <p>Critical Flags</p> <ul> <li><code>--scheduler-name=my-custom-scheduler</code>: This is the unique name pods will use to select this scheduler.</li> <li><code>--leader-elect=false</code>: Disables leader election (only needed for HA setups with multiple replicas).</li> </ul>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-2-apply-the-deployment","title":"Step 2: Apply the Deployment","text":"<pre><code>kubectl apply -f custom-scheduler.yaml\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-3-verify-its-running","title":"Step 3: Verify It's Running","text":"<pre><code># Check deployment\nkubectl get deploy -n kube-system my-scheduler\n\n# Check the pod logs\nkubectl logs -n kube-system -l component=my-scheduler\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#5-using-a-custom-scheduler-in-pods","title":"5. Using a Custom Scheduler in Pods","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#the-schedulername-field","title":"The <code>schedulerName</code> Field","text":"<p>To tell a pod to use a specific scheduler, add the <code>schedulerName</code> field in the pod spec.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-workload\nspec:\n  schedulerName: my-custom-scheduler  # &lt;--- Use custom scheduler\n  containers:\n  - name: gpu-app\n    image: nvidia/cuda:11.0-base\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#default-behavior","title":"Default Behavior","text":"<p>If you don't specify <code>schedulerName</code>, the pod uses the default scheduler: <pre><code>spec:\n  schedulerName: default-scheduler  # This is implicit\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#6-checking-which-scheduler-scheduled-a-pod","title":"6. Checking Which Scheduler Scheduled a Pod","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#method-1-check-the-pod-yaml","title":"Method 1: Check the Pod YAML","text":"<pre><code>kubectl get pod &lt;pod-name&gt; -o yaml | grep schedulerName\n</code></pre> <p>Output: <pre><code>schedulerName: my-custom-scheduler\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#method-2-custom-columns-view","title":"Method 2: Custom Columns View","text":"<pre><code>kubectl get pods -o custom-columns=\"NAME:.metadata.name,SCHEDULER:.spec.schedulerName\"\n</code></pre> <p>Output: <pre><code>NAME              SCHEDULER\nnginx-default     default-scheduler\ngpu-workload      my-custom-scheduler\nweb-app           default-scheduler\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#method-3-check-events","title":"Method 3: Check Events","text":"<pre><code>kubectl describe pod &lt;pod-name&gt; | grep -i scheduled\n</code></pre> <p>Output: <pre><code>Successfully assigned default/gpu-workload to node01 by my-custom-scheduler\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#7-real-world-example-gpu-scheduler","title":"7. Real-World Example: GPU Scheduler","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#scenario","title":"Scenario","text":"<p>You have a cluster with: *   3 nodes: 1 has GPUs, 2 are CPU-only *   Default scheduler: Doesn't understand GPU requirements well *   Custom GPU scheduler: Has logic to prefer GPU nodes</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-1-deploy-custom-scheduler","title":"Step 1: Deploy Custom Scheduler","text":"<pre><code>kubectl apply -f gpu-scheduler.yaml\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-2-deploy-gpu-workload","title":"Step 2: Deploy GPU Workload","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training\nspec:\n  schedulerName: gpu-scheduler  # Uses custom logic\n  containers:\n  - name: trainer\n    image: tensorflow/tensorflow:latest-gpu\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#step-3-verify-placement","title":"Step 3: Verify Placement","text":"<pre><code>kubectl get pod ml-training -o wide\n</code></pre> <p>Output: <pre><code>NAME          READY   STATUS    SCHEDULER         NODE\nml-training   1/1     Running   gpu-scheduler     gpu-node1\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#8-multiple-schedulers-in-the-same-cluster","title":"8. Multiple Schedulers in the Same Cluster","text":"<p>You can have as many schedulers as you want running simultaneously:</p> <pre><code>Cluster State:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 kube-system namespace        \u2502\n\u2502                              \u2502\n\u2502 1. default-scheduler (Pod)  \u2502\u2500\u2500\u2500\u25b6 Most pods\n\u2502 2. gpu-scheduler (Deploy)   \u2502\u2500\u2500\u2500\u25b6 ML workloads\n\u2502 3. batch-scheduler (Deploy) \u2502\u2500\u2500\u2500\u25b6 Batch jobs\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each pod independently chooses which one to use via <code>schedulerName</code>.</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#9-common-issues-and-troubleshooting","title":"9. Common Issues and Troubleshooting","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#issue-1-pod-stuck-in-pending","title":"Issue 1: Pod Stuck in Pending","text":"<p>Symptom: <pre><code>kubectl get pod my-pod\n# NAME     READY   STATUS    RESTARTS   AGE\n# my-pod   0/1     Pending   0          5m\n</code></pre></p> <p>Possible Causes: 1.  Typo in schedulerName: The scheduler doesn't exist. 2.  Scheduler is down: The custom scheduler pod crashed. 3.  RBAC issues: Scheduler doesn't have permissions.</p> <p>Check: <pre><code># Verify scheduler is running\nkubectl get pods -n kube-system -l component=my-scheduler\n\n# Check pod events\nkubectl describe pod my-pod\n</code></pre></p> <p>Look for: <pre><code>Warning  FailedScheduling  default-scheduler  0/3 nodes are available: \n         No scheduler found with name: my-custom-scheduler\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#issue-2-wrong-scheduler-name","title":"Issue 2: Wrong Scheduler Name","text":"<p>Problem: <pre><code>spec:\n  schedulerName: my-costom-scheduler  # Typo: \"costom\" instead of \"custom\"\n</code></pre></p> <p>Solution: <pre><code># Delete and recreate (pods are immutable for schedulerName)\nkubectl delete pod my-pod\n</code></pre></p> <pre><code>spec:\n  schedulerName: my-custom-scheduler  # Fixed\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#issue-3-scheduler-pod-not-scheduling-pods","title":"Issue 3: Scheduler Pod Not Scheduling Pods","text":"<p>Check scheduler logs: <pre><code>kubectl logs -n kube-system -l component=my-scheduler\n</code></pre></p> <p>Common log errors: <pre><code>Failed to get leases.coordination.k8s.io \"my-scheduler\" is forbidden\n</code></pre></p> <p>Solution: Fix RBAC permissions (see deployment YAML above).</p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#10-cka-exam-tips","title":"10. CKA Exam Tips","text":""},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#task-create-a-pod-that-uses-a-custom-scheduler","title":"Task: \"Create a pod that uses a custom scheduler\"","text":"<p>Workflow:</p> <pre><code># Step 1: Generate pod YAML\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Step 2: Edit to add schedulerName\nvim pod.yaml\n</code></pre> <p>Add under <code>spec:</code>: <pre><code>  schedulerName: my-custom-scheduler\n</code></pre></p> <pre><code># Step 3: Apply\nkubectl apply -f pod.yaml\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#task-identify-which-scheduler-scheduled-a-pod","title":"Task: \"Identify which scheduler scheduled a pod\"","text":"<pre><code>kubectl get pod &lt;name&gt; -o jsonpath='{.spec.schedulerName}'\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#task-check-if-a-custom-scheduler-is-running","title":"Task: \"Check if a custom scheduler is running\"","text":"<pre><code>kubectl get pods -n kube-system -l component=&lt;scheduler-name&gt;\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#11-key-differences-scheduler-vs-priorityclass","title":"11. Key Differences: Scheduler vs PriorityClass","text":"<p>Students often confuse these concepts:</p> Concept Purpose Field Scheduler WHO makes the scheduling decision <code>schedulerName: my-scheduler</code> PriorityClass HOW IMPORTANT the pod is <code>priorityClassName: high-priority</code> <p>You can combine both: <pre><code>spec:\n  schedulerName: gpu-scheduler      # Custom logic\n  priorityClassName: critical-tier  # High importance\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#12-quick-reference-commands","title":"12. Quick Reference Commands","text":"<pre><code># List all pods and their schedulers\nkubectl get pods -A -o custom-columns=\"NAME:.metadata.name,SCHEDULER:.spec.schedulerName\"\n\n# Check which scheduler scheduled a specific pod\nkubectl get pod &lt;name&gt; -o jsonpath='{.spec.schedulerName}'\n\n# Verify custom scheduler is running\nkubectl get deploy -n kube-system &lt;scheduler-name&gt;\n\n# Check scheduler pod logs\nkubectl logs -n kube-system -l component=&lt;scheduler-name&gt;\n\n# Describe pod to see scheduling events\nkubectl describe pod &lt;name&gt; | grep -A 5 Events\n</code></pre>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 Kubernetes supports multiple schedulers running simultaneously \u2705 Pods select a scheduler via <code>spec.schedulerName</code> \u2705 Default scheduler name is <code>default-scheduler</code> \u2705 Custom schedulers need RBAC permissions \u2705 Each scheduler must have a unique name \u2705 If <code>schedulerName</code> is wrong or scheduler is down, pod stays Pending \u2705 Use <code>kubectl get pod &lt;name&gt; -o yaml | grep schedulerName</code> to check which scheduler was used  </p>"},{"location":"fundamentals/pod-scheduling/multiple-schedulers/#when-to-use-multiple-schedulers","title":"When to Use Multiple Schedulers","text":"<ul> <li>\u2705 Specialized workloads (GPU, HPC, ML)</li> <li>\u2705 Multi-tenancy with different policies per tenant</li> <li>\u2705 Legacy apps with unique placement needs</li> <li>\u274c NOT for basic priority (use PriorityClasses instead)</li> <li>\u274c NOT for node selection (use nodeSelector/affinity instead)</li> </ul>"},{"location":"fundamentals/pod-scheduling/node-affinity/","title":"Node Affinity","text":"<p>Node Affinity is a more advanced and flexible version of <code>nodeSelector</code>. It allows you to use complex logic and soft requirements to control where your Pods land.</p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#1-why-use-node-affinity","title":"1. Why use Node Affinity?","text":"<p>Unlike Node Selector, Node Affinity supports: *   Logical Operators: <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code> (Greater than), <code>Lt</code> (Less than). *   Soft Rules: You can tell Kubernetes \"I would prefer this node, but if it's full, run it somewhere else.\"</p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#2-the-two-types-of-affinity","title":"2. The Two Types of Affinity","text":"<p>Kubernetes uses long, descriptive names for these rules. Think of them as \"Hard\" vs. \"Soft\".</p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#a-requiredduringschedulingignoredduringexecution-the-hard-rule","title":"A. RequiredDuringSchedulingIgnoredDuringExecution (\"The Hard Rule\")","text":"<ul> <li>Scheduling: The scheduler must find a node that matches the rule. If not, the Pod stays Pending.</li> <li>Execution: If the node labels change while the Pod is already running, the Pod is NOT evicted (it stays running).</li> </ul> <p>Example YAML: <pre><code>spec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - us-east-1a\n            - us-east-1b\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#b-preferredduringschedulingignoredduringexecution-the-soft-rule","title":"B. PreferredDuringSchedulingIgnoredDuringExecution (\"The Soft Rule\")","text":"<ul> <li>Scheduling: The scheduler will try to find a matching node. If none are available, it will schedule the Pod on any available node.</li> <li>Weight: You assign a weight (1-100). The scheduler adds up weights for each node to decide the \"best\" winner.</li> </ul> <p>Example YAML: <pre><code>spec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#3-operators-cheat-sheet","title":"3. Operators Cheat Sheet","text":"Operator Usage In Label value must be in this list. NotIn Label value must NOT be in this list. Exists The key must exist on the node (value doesn't matter). DoesNotExist The key must NOT exist on the node. Gt / Lt Greater than / Less than (for numeric values)."},{"location":"fundamentals/pod-scheduling/node-affinity/#4-the-exam-workflow-dry-run-edit","title":"4. The Exam Workflow (Dry-Run + Edit)","text":"<p>CRITICAL TIP: <code>kubectl</code> does not have a flag for <code>nodeAffinity</code>. To use it in the exam:</p> <ol> <li>Generate the template:     <pre><code>kubectl create deployment blue --image=nginx --dry-run=client -o yaml &gt; blue.yaml\n</code></pre></li> <li>Add the affinity block: Open the file and insert the affinity section inside <code>spec.template.spec</code>.</li> <li>Apply:     <pre><code>kubectl apply -f blue.yaml\n</code></pre></li> </ol>"},{"location":"fundamentals/pod-scheduling/node-affinity/#5-the-duringexecution-clarification","title":"5. The \"DuringExecution\" Clarification","text":"<p>You might notice the suffix <code>IgnoredDuringExecution</code> in the standard affinity names. </p>"},{"location":"fundamentals/pod-scheduling/node-affinity/#ignoredduringexecution-current-standard","title":"IgnoredDuringExecution (Current Standard)","text":"<ul> <li>Meaning: Rules are only checked during the scheduling phase (when the Pod is first being created).</li> <li>Behavior: If a node's labels change while the Pod is already running (making the affinity rule technically \"invalid\"), the Pod continues to run unaffected.</li> </ul>"},{"location":"fundamentals/pod-scheduling/node-affinity/#requiredduringexecution-plannedfuture","title":"RequiredDuringExecution (Planned/Future)","text":"<ul> <li>Meaning: Rules would be checked continuously throughout the Pod's lifecycle.</li> <li>Behavior: If a node's labels change and no longer match the Pod's requirements, the Pod would be evicted (terminated) immediately.</li> <li>Status: This is NOT yet implemented in Kubernetes but is reserved in the API for future functionality.</li> </ul>"},{"location":"fundamentals/pod-scheduling/node-affinity/#5-comparison-table","title":"5. Comparison Table","text":"Feature Node Selector Node Affinity Logic Simple (Equal) Complex (In, NotIn, etc.) Strictness Always Required Required OR Preferred Use Case Simple constraints Production-grade scheduling <p>Note for CKA Exam: You will likely be asked to use Required affinity to move a Pod to a specific zone or node with a specific label. Always use <code>dry-run</code> to generate the base YAML and then carefully paste the <code>affinity</code> block.</p>"},{"location":"fundamentals/pod-scheduling/node-selector/","title":"Node Selector","text":"<p>Node Selector is the simplest form of node selection constraint in Kubernetes. It allows you to restrict a Pod to run only on nodes with specific labels.</p>"},{"location":"fundamentals/pod-scheduling/node-selector/#1-how-it-works","title":"1. How it works","text":"<p>For Node Selector to work, you must: 1.  Label the Node: Give your node a key-value pair. 2.  Add nodeSelector to Pod: Update the Pod spec to look for that exact label.</p>"},{"location":"fundamentals/pod-scheduling/node-selector/#2-practical-example","title":"2. Practical Example","text":""},{"location":"fundamentals/pod-scheduling/node-selector/#step-1-label-your-node","title":"Step 1: Label your node","text":"<pre><code>kubectl label nodes node01 disktype=ssd\n</code></pre>"},{"location":"fundamentals/pod-scheduling/node-selector/#step-2-create-a-pod-with-the-selector","title":"Step 2: Create a Pod with the selector","text":"<p>Exam Tip: Since there is no <code>--node-selector</code> flag in <code>kubectl run</code>, you must generate the YAML first and then edit it.</p> <pre><code># 1. Generate the base YAML\nkubectl run nginx-ssd --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# 2. Add the nodeSelector block to pod.yaml\n# (See YAML structure below)\n\n# 3. Apply the file\nkubectl apply -f pod.yaml\n</code></pre> <p>YAML Structure: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-ssd\nspec:\n  nodeSelector:      # &lt;--- Added section\n    disktype: ssd\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/node-selector/#3-limitations","title":"3. Limitations","text":"<p>Node Selector is very simple but has significant drawbacks: *   Exact Match Only: You cannot say \"SSD OR HDD\" or \"NOT GPU\". *   Hard Requirement: If no node matches the label, the Pod will stay in Pending forever. It has no \"best effort\" mode. *   Simple Logic: You can only use one or more exact matches (AND logic).</p>"},{"location":"fundamentals/pod-scheduling/node-selector/#summary-cheat-sheet","title":"Summary Cheat Sheet","text":"Command Action <code>kubectl label node &lt;node&gt; key=value</code> Add a label to a node <code>kubectl label node &lt;node&gt; key-</code> Remove a label from a node <code>kubectl get nodes --show-labels</code> See all labels on your nodes <p>Since Node Selector is so limited, for more complex logic (Like \"Try to put it on a high-cpu node, but if none are free, anywhere is fine\"), you should use Node Affinity.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/","title":"Pod Priority and Preemption","text":"<p>In a crowded Kubernetes cluster, not all Pods are equal. PriorityClasses allow you to tell the scheduler which Pods are the most important, ensuring they get scheduled even if the cluster is full.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#1-the-vip-seating-analogy","title":"1. The \"VIP Seating\" Analogy","text":"<p>Think of your cluster as a popular restaurant: *   Standard Pods: Customers who show up and wait for a table. *   Priority Pods: Customers with a VIP reservation.</p> <p>If a VIP customer shows up and there are no tables, the manager (Scheduler) might ask a \"Regular\" customer to leave (Preemption) to make room for the VIP.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#2-priorityclass-object","title":"2. PriorityClass Object","text":"<p>A PriorityClass is a cluster-wide (non-namespaced) object that maps a name to a numeric value.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#example-yaml","title":"Example YAML:","text":"<pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority-apps\nvalue: 1000000\nglobalDefault: false\ndescription: \"This priority class should be used for critical service pods.\"\n</code></pre> <ul> <li><code>value</code>: Higher numbers mean higher priority.</li> <li><code>globalDefault</code>: If set to <code>true</code>, any pod without a priority class will automatically get this one. (Only one PriorityClass can be the global default).</li> </ul>"},{"location":"fundamentals/pod-scheduling/priority-classes/#3-how-to-use-it-in-a-pod","title":"3. How to use it in a Pod","text":"<p>Once the PriorityClass is created, you simply reference its name in the Pod spec.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-app\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  priorityClassName: high-priority-apps    # &lt;--- Reference here\n</code></pre>"},{"location":"fundamentals/pod-scheduling/priority-classes/#4-preemption-survival-of-the-fittest","title":"4. Preemption (Survival of the Fittest)","text":"<p>When a Pod with high priority is in the <code>Pending</code> state, the scheduler tries to find a node for it. If no nodes have enough resources, the scheduler looks for lower-priority pods to evict.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#the-preemption-process","title":"The Preemption Process:","text":"<ol> <li>High-priority Pod enters the queue.</li> <li>Scheduler finds no nodes with enough CPU/RAM.</li> <li>Scheduler identifies a node where removing one or more lower-priority pods would create enough space.</li> <li>Lower-priority Pods are gracefully terminated.</li> <li>High-priority Pod is scheduled on that node.</li> </ol>"},{"location":"fundamentals/pod-scheduling/priority-classes/#preemptionpolicy-to-kill-or-not-to-kill","title":"<code>preemptionPolicy</code>: To Kill or Not to Kill?","text":"<p>By default, high priority means the pod can evict others. However, you can change this behavior using the <code>preemptionPolicy</code> field:</p> <ol> <li><code>PreemptLowerPriority</code> (Default): If this pod is pending, the scheduler will kill lower-priority pods to make room.</li> <li><code>Never</code> (Non-Preempting): This pod will still be placed at the \"front of the line\" in the scheduling queue, but it will not kill any existing pods. It will wait until space naturally becomes available.</li> </ol> <p>Real-world use case for <code>Never</code>: Batch jobs that are \"nice to have\" but shouldn't disrupt the live website. They get scheduled as soon as there's a gap, but they never kick anyone out.</p> <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: batch-workload-non-preempting\nvalue: 500000\npreemptionPolicy: Never    # &lt;--- Won't evict other pods\nglobalDefault: false\n</code></pre>"},{"location":"fundamentals/pod-scheduling/priority-classes/#how-value-and-preemptionpolicy-work-together","title":"How <code>value</code> and <code>preemptionPolicy</code> Work Together","text":"<p>Think of this like a Social Ladder: *   <code>value</code> = Rank (High number &gt; Low number). It defines who is more important. *   <code>preemptionPolicy</code> = Permission. It defines how that importance is enforced.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#scenario-the-node-is-full","title":"Scenario: The Node is Full","text":"<p>A new Pod with <code>value: 1000</code> arrives. The node is currently occupied by a Pod with <code>value: 500</code>.</p> If the new Pod (1000) has... Action it can take <code>PreemptLowerPriority</code> The Comparison: 1000 &gt; 500. The Policy: Allowed to evict. Result: The 500-pod is killed, and the 1000-pod takes its place. <code>Never</code> The Comparison: 1000 &gt; 500. The Policy: Refused permission. Result: The 1000-pod stays <code>Pending</code> until space opens up naturally. <p>The Golden Rule of Preemption</p> <p>A pod can NEVER preempt/kill a pod with an equal or higher priority value. You can only kick out those \"below\" you on the ladder.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#5-the-priority-value-spectrum-highest-to-lowest","title":"5. The Priority Value Spectrum (Highest to Lowest)","text":"<p>Priority values are 32-bit integers. The range is technically large, but Kubernetes reserves specific \"neighborhoods\" for different tiers of importance.</p> Priority Tier Value Range Typical Use Case System Critical 2,000,000,001 to 2,147,483,647 Reserved. Used for internal K8s components. System Node Critical 2,000,001,000 Built-in: CNI, kube-proxy. System Cluster Critical 2,000,000,000 Built-in: CoreDNS, API Server. User Critical 1,000,000 to 1,000,000,000 Your most important PROD apps (Databases, Gateways). Standard Apps 1,000 to 999,999 Regular web servers, APIs. Default Priority 0 The baseline for every pod if no class is defined. Best Effort / Batch Negative Numbers Low-priority background jobs, analytics, or research."},{"location":"fundamentals/pod-scheduling/priority-classes/#the-danger-zone-1-billion","title":"The \"Danger Zone\" (&gt; 1 Billion)","text":"<p>You should not create your own priority classes with values greater than 1 billion unless you strictly want to compete with the Kubernetes system components. If your app evicts <code>kube-proxy</code>, your node's networking will break!</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#logic-for-choosing-numbers","title":"Logic for Choosing Numbers:","text":"<ul> <li>Production vs staging: Give Prod a higher range (e.g., 10,000+) and Staging a lower range (e.g., 5,000).</li> <li>Leave Gaps: Don't use 1, 2, 3. Use 1000, 2000, 3000. This allows you to \"squeeze\" a new level in between later without having to update all your YAML files.</li> </ul>"},{"location":"fundamentals/pod-scheduling/priority-classes/#6-pro-tip-group-by-tiers-not-pod-by-pod","title":"6. Pro Tip: Group by Tiers, not Pod-by-Pod","text":"<p>A common mistake is trying to give every single application pod its own unique priority number (e.g., <code>1001</code>, <code>1002</code>, <code>1003</code>). </p> <p>Why you shouldn't do this: 1.  Complexity: You would have to manage hundreds of <code>PriorityClass</code> objects. 2.  No \"Squeeze\" Room: If you use <code>1001</code> and <code>1002</code>, you can't insert a new priority level between them later.  3.  Grouping is Better: In real production environments, we group pods into Tiers.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#recommended-tiering-strategy","title":"Recommended Tiering Strategy:","text":"Category Value Example System 2,000,000,000 K8s Components (Reserved) Tenant Critical 1,000,000 Shared Ingress, Core DBs App Critical 100,000 Customer-facing APIs Standard 10,000 Regular Microservices Default 0 (The global baseline) Best Effort -100 Background workers, logs"},{"location":"fundamentals/pod-scheduling/priority-classes/#7-built-in-priorityclasses","title":"7. Built-in PriorityClasses","text":"<p>Kubernetes comes with two critical priority classes by default. They are at the absolute top of the hierarchy.</p> Name Value Purpose <code>system-node-critical</code> 2000001000 For pods that MUST run on nodes (like CNI plugins). <code>system-cluster-critical</code> 2000000000 For essential cluster pods (like CoreDNS, API Server)."},{"location":"fundamentals/pod-scheduling/priority-classes/#7-real-world-use-cases","title":"7. Real-World Use Cases","text":"<p>How do companies actually use these values?</p> Scenario Tier Rationale Ingress Controllers Cluster-Wide Critical If Nginx-Ingress or Traefik dies, the entire cluster's external traffic stops. These should have very high priority. Payment Gateways Business Critical In an e-commerce cluster, if the \"Search\" pod and \"Payment\" pod are fighting for space, you WANT the Payment pod to win so you don't lose money. Dev/Test Environment Default (0) Regular experimental pods that aren't vital for uptime. ML Training Jobs Negative / Best Effort Machine learning training can take days. It is \"compressible\"\u2014it's better to pause a learning job than to crash the live website."},{"location":"fundamentals/pod-scheduling/priority-classes/#8-full-implementation-example","title":"8. Full Implementation Example","text":"<p>Here is a holistic example of how you would set up a \"Platinum\" tier for your most important production database.</p> <pre><code># 1. Create the PriorityClass\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: platinum-tier\nvalue: 1000000\nglobalDefault: false\ndescription: \"Used for mission-critical production databases.\"\n---\n# 2. Create the Pod referencing it\napiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-db\n  labels:\n    env: prod\nspec:\n  priorityClassName: platinum-tier\n  containers:\n  - name: postgres\n    image: postgres:14\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"1\"\n</code></pre>"},{"location":"fundamentals/pod-scheduling/priority-classes/#9-practical-scenario-managing-4-different-apps","title":"9. Practical Scenario: Managing 4 Different Apps","text":"<p>If you have 4 different applications, you should categorize them by their \"Impact of Failure\" rather than just giving them sequential numbers.</p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#the-apps","title":"The Apps:","text":"<ol> <li>Payment API: Mission Critical (Losing money if it's down).</li> <li>Frontend Web: High Priority (Users see an error page if it's down).</li> <li>Analytics Service: Medium Priority (Business data is delayed, but users are fine).</li> <li>Log Archiver: Low Priority (Housekeeping, can run whenever).</li> </ol>"},{"location":"fundamentals/pod-scheduling/priority-classes/#step-1-define-the-tiers-priorityclasses","title":"Step 1: Define the Tiers (PriorityClasses)","text":"<p>We create 3 reusable tiers instead of 4 unique numbers. The Log Archiver will just use the default (0).</p> <pre><code># cluster-tiers.yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: critical-tier\nvalue: 1000000\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-tier\nvalue: 500000\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: standard-tier\nvalue: 100000\n</code></pre>"},{"location":"fundamentals/pod-scheduling/priority-classes/#step-2-assign-to-app-pods","title":"Step 2: Assign to App Pods","text":"App Assigned PriorityClass Why? Payment API <code>priorityClassName: critical-tier</code> It will evict anything else to stay alive. Frontend Web <code>priorityClassName: high-tier</code> Will evict standard apps, but won't touch Payments. Analytics <code>priorityClassName: standard-tier</code> Will run if there's room, but will get killed for Frontend/Payments. Log Archiver (None) Uses default priority (0). It is the first to be killed."},{"location":"fundamentals/pod-scheduling/priority-classes/#10-summary-cheat-sheet","title":"10. Summary Cheat Sheet","text":"Situation Behavior Higher Value Higher Priority (more important). Preemption The act of killing a low-priority pod to make room for a high-priority one. <code>preemptionPolicy</code> Can be <code>PreemptLowerPriority</code> (Default) or <code>Never</code> (Best-effort only). Namespace PriorityClasses are Cluster-wide (Global)."},{"location":"fundamentals/pod-scheduling/priority-classes/#11-kubectl-commands-workflows","title":"11. kubectl Commands &amp; Workflows","text":""},{"location":"fundamentals/pod-scheduling/priority-classes/#creating-a-priorityclass","title":"Creating a PriorityClass","text":"<p>There is NO shorthand <code>kubectl create priorityclass</code> with flags for value/globalDefault.</p> <p>You must write the YAML manually:</p> <pre><code># Step 1: Create the YAML file\ncat &lt;&lt;EOF &gt; high-priority.yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false\ndescription: \"For critical production workloads\"\nEOF\n\n# Step 2: Apply it\nkubectl apply -f high-priority.yaml\n</code></pre>"},{"location":"fundamentals/pod-scheduling/priority-classes/#assigning-priorityclass-to-a-pod","title":"Assigning PriorityClass to a Pod","text":"<p>There is NO flag for <code>--priority-class-name</code> in <code>kubectl run</code>.</p> <p>You must use the dry-run workflow:</p> <pre><code># Step 1: Generate the pod YAML\nkubectl run low-prio-pod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Step 2: Edit the YAML to add priorityClassName\nvim pod.yaml\n# Add under spec:\n#   priorityClassName: high-priority\n\n# Step 3: Apply\nkubectl apply -f pod.yaml\n</code></pre> <p>Or use a one-liner with yq/sed: <pre><code>kubectl run low-prio-pod --image=nginx --dry-run=client -o yaml | \\\n  sed '/spec:/a\\  priorityClassName: high-priority' | \\\n  kubectl apply -f -\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#inspecting-priorityclasses","title":"Inspecting PriorityClasses","text":"<pre><code># List all priority classes (shorthand: pc)\nkubectl get priorityclass\nkubectl get pc\n\n# Describe a specific PriorityClass\nkubectl describe pc high-priority\n\n# View as YAML\nkubectl get pc high-priority -o yaml\n\n# See just the value\nkubectl get pc high-priority -o jsonpath='{.value}'\n</code></pre> <p>Output: <pre><code>NAME                      VALUE        GLOBAL-DEFAULT   AGE\nhigh-priority             1000000      false            5m\nsystem-cluster-critical   2000000000   false            30d\nsystem-node-critical      2000001000   false            30d\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#inspecting-pods-with-priority","title":"Inspecting Pods with Priority","text":"<p>Check which pods are using priority classes:</p> <pre><code># Basic view - shows priority in YAML\nkubectl get pod &lt;pod-name&gt; -o yaml | grep -A 2 priority\n\n# Custom columns - Clean table view\nkubectl get pods -o custom-columns=\"NAME:.metadata.name,PRIORITY:.spec.priorityClassName\"\n</code></pre> <p>Output: <pre><code>NAME              PRIORITY\nnginx-critical    high-priority\nweb-app           &lt;none&gt;\ndb-backup         low-priority\n</code></pre></p> <p>Check the numeric priority value: <pre><code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.priority}'\n</code></pre></p> <p>See all priority-related fields: <pre><code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.priorityClassName}{\" -&gt; \"}{.spec.priority}{\"\\n\"}'\n</code></pre></p> <p>Output: <pre><code>high-priority -&gt; 1000000\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#filtering-pods-by-priority","title":"Filtering Pods by Priority","text":"<p>Find all pods with a specific PriorityClass: <pre><code>kubectl get pods -A -o json | \\\n  jq -r '.items[] | select(.spec.priorityClassName==\"high-priority\") | .metadata.name'\n</code></pre></p> <p>Find all pods without any priority: <pre><code>kubectl get pods -A -o json | \\\n  jq -r '.items[] | select(.spec.priorityClassName==null) | .metadata.name'\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#advanced-inspection","title":"Advanced Inspection","text":"<p>See priorities across all namespaces: <pre><code>kubectl get pods -A -o custom-columns=\\\n\"NAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nPRIORITY_CLASS:.spec.priorityClassName,\\\nPRIORITY_VALUE:.spec.priority,\\\nNODE:.spec.nodeName\"\n</code></pre></p> <p>Check preemption policy of a PriorityClass: <pre><code>kubectl get pc high-priority -o jsonpath='{.preemptionPolicy}'\n</code></pre></p> <p>List pods sorted by priority (requires jq): <pre><code>kubectl get pods -A -o json | \\\n  jq -r '.items | sort_by(.spec.priority // 0) | reverse | \n  .[] | \"\\(.spec.priority // 0)\\t\\(.metadata.name)\"'\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<p>Check if a pending pod is waiting due to preemption: <pre><code>kubectl describe pod &lt;pod-name&gt; | grep -A 10 Events\n</code></pre></p> <p>Look for messages like: <pre><code>Normal   Preempted   Pod preempted to make room for higher priority pod\n</code></pre></p> <p>Check scheduler logs for preemption events: <pre><code>kubectl logs -n kube-system -l component=kube-scheduler | grep -i preempt\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/priority-classes/#quick-reference-table","title":"Quick Reference Table","text":"Task Command List all PriorityClasses <code>kubectl get pc</code> Create PriorityClass Must use YAML file (no imperative command) Delete PriorityClass <code>kubectl delete pc &lt;name&gt;</code> Check pod's priority class <code>kubectl get pod &lt;name&gt; -o jsonpath='{.spec.priorityClassName}'</code> Check pod's numeric priority <code>kubectl get pod &lt;name&gt; -o jsonpath='{.spec.priority}'</code> Custom columns view <code>kubectl get pods -o custom-columns=\"NAME:.metadata.name,PRIORITY:.spec.priorityClassName\"</code> Check preemption policy <code>kubectl get pc &lt;name&gt; -o jsonpath='{.preemptionPolicy}'</code>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/","title":"Scheduler Profiles in Kubernetes","text":"<p>Scheduler Profiles allow you to configure different scheduling behaviors within a single scheduler instance, rather than deploying multiple separate schedulers.</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#1-the-restaurant-with-multiple-sections-analogy","title":"1. The \"Restaurant with Multiple Sections\" Analogy","text":"<p>Think of scheduler profiles like a restaurant with different sections:</p> <p>Without Profiles (Multiple Schedulers): <pre><code>Restaurant A: Fine dining (slow, careful seating)\nRestaurant B: Fast food (quick seating)\nRestaurant C: VIP section (special rules)\n\nEach has a separate manager (scheduler process).\n</code></pre></p> <p>With Profiles (Single Scheduler): <pre><code>One Restaurant, One Manager, Three Sections:\n  - Section 1: Fine dining rules\n  - Section 2: Fast food rules  \n  - Section 3: VIP rules\n\nSame manager handles all sections with different rule books.\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#2-what-are-scheduler-profiles","title":"2. What are Scheduler Profiles?","text":"<p>A Scheduler Profile is a named configuration within the Kubernetes scheduler that defines which plugins to enable and how they should behave during the scheduling process.</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#key-concepts","title":"Key Concepts:","text":"<ul> <li>One scheduler binary can have multiple profiles</li> <li>Each profile has a unique name</li> <li>Pods select a profile via <code>spec.schedulerName</code></li> <li>Profiles share the same process but use different plugin configurations</li> </ul>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#3-why-use-scheduler-profiles","title":"3. Why Use Scheduler Profiles?","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#problem-with-multiple-schedulers","title":"Problem with Multiple Schedulers","text":"<p>Resource Overhead: <pre><code>3 Custom Schedulers = 3 Separate Processes\n  - my-scheduler-1:  ~100MB RAM\n  - my-scheduler-2:  ~100MB RAM\n  - my-scheduler-3:  ~100MB RAM\n  Total: ~300MB RAM\n</code></pre></p> <p>With Profiles: <pre><code>1 Scheduler with 3 Profiles = 1 Process\n  - default-scheduler (3 profiles): ~120MB RAM\n  Total: ~120MB RAM \u2705 Much more efficient!\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#benefits","title":"Benefits","text":"Benefit Explanation Resource Efficient Single process vs multiple scheduler pods Easier Management One deployment to configure and monitor Shared Cache All profiles share the same cluster state cache Simpler RBAC One ServiceAccount instead of many"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#4-the-scheduling-framework-phases-extension-points","title":"4. The Scheduling Framework: Phases &amp; Extension Points","text":"<p>The scheduler operates in two main cycles:</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#the-two-cycles","title":"The Two Cycles","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SCHEDULING CYCLE (Decide which node)               \u2502\n\u2502 QueueSort \u2192 PreFilter \u2192 Filter \u2192 PostFilter \u2192      \u2502\n\u2502 PreScore \u2192 Score \u2192 NormalizeScore \u2192 Reserve \u2192      \u2502\n\u2502 Permit                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BINDING CYCLE (Bind pod to node)                   \u2502\n\u2502 WaitOnPermit \u2192 PreBind \u2192 Bind \u2192 PostBind           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#extension-points-in-order-of-execution","title":"Extension Points (in Order of Execution)","text":"# Extension Point Phase Purpose Can Reject Pod? 1 QueueSort Scheduling Sort pods waiting to be scheduled No 2 PreFilter Scheduling Pre-process pod info or check cluster state Yes 3 Filter Scheduling Filter nodes that cannot run the pod Yes 4 PostFilter Scheduling Called only when no nodes pass Filter (for preemption) No 5 PreScore Scheduling Pre-scoring work (run once per pod) No 6 Score Scheduling Rank each node (0-100) No 7 NormalizeScore Scheduling Normalize scores from different plugins No 8 Reserve Scheduling Reserve resources before binding Yes 9 Permit Scheduling Approve, deny, or wait for pod binding Yes 10 WaitOnPermit Binding Wait for all Permit plugins No 11 PreBind Binding Pre-binding tasks (e.g., provision volumes) Yes 12 Bind Binding Bind pod to node Yes 13 PostBind Binding Informational hook (cannot fail) No"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#built-in-plugins-by-extension-point","title":"Built-in Plugins by Extension Point","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#queuesort-plugins","title":"QueueSort Plugins","text":"<p>Determine the order pods are scheduled.</p> Plugin What It Does PrioritySort Sort by pod priority (higher priority first)"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#prefilter-plugins","title":"PreFilter Plugins","text":"<p>Pre-process information before filtering nodes.</p> Plugin What It Does Can Skip Filter? NodeResourcesFit Calculate resource requests No NodePorts Check if required ports are available No PodTopologySpread Prepare topology spread constraints No InterPodAffinity Pre-calculate pod affinity/anti-affinity No VolumeBinding Check which volumes need binding No"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#filter-plugins","title":"Filter Plugins","text":"<p>Eliminate nodes that cannot run the pod.</p> Plugin What It Filters Reason for Rejection NodeUnschedulable Unschedulable nodes Node has <code>unschedulable: true</code> NodeName Non-matching nodes Pod specifies <code>nodeName</code>, node doesn't match TaintToleration Tainted nodes Pod doesn't tolerate node's taints NodeAffinity Nodes without labels Node labels don't match pod's <code>nodeAffinity</code> NodePorts Nodes without free ports Required port already in use NodeResourcesFit Nodes without resources Not enough CPU/Memory/GPU VolumeRestrictions Incompatible volumes Volume zone doesn't match node VolumeBinding Volume unavailable PVC cannot be bound to this node PodTopologySpread Unbalanced topology Would violate spread constraints InterPodAffinity Affinity violations Pod affinity/anti-affinity not satisfied"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#postfilter-plugins","title":"PostFilter Plugins","text":"<p>Called when no nodes pass the Filter phase (usually for preemption).</p> Plugin What It Does DefaultPreemption Find lower-priority pods to evict"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#prescore-plugins","title":"PreScore Plugins","text":"<p>Prepare for scoring (runs once per scheduling cycle).</p> Plugin What It Does InterPodAffinity Calculate affinity terms for scoring PodTopologySpread Calculate current topology distribution"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#score-plugins","title":"Score Plugins","text":"<p>Rank nodes (0-100, higher is better).</p> Plugin Scoring Logic Weight (Default) NodeResourcesBalancedAllocation Prefer nodes with balanced CPU/Memory usage 1 NodeResourcesMostAllocated Prefer nodes that are already full (bin-packing) 1 NodeResourcesLeastAllocated Prefer nodes with most free resources 1 ImageLocality Prefer nodes that already have the image cached 1 InterPodAffinity Score based on pod affinity preferences 1 NodeAffinity Score based on preferred node affinity 1 PodTopologySpread Score to achieve even spread 2 TaintToleration Slight preference for nodes without taints 1"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#reserve-plugins","title":"Reserve Plugins","text":"<p>Reserve resources before binding (can be rolled back if binding fails).</p> Plugin What It Does VolumeBinding Reserve volumes for the pod"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#permit-plugins","title":"Permit Plugins","text":"<p>Approve or deny binding (can wait for external conditions).</p> Plugin What It Does (Usually custom) Can approve, deny, or wait for external signal"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#prebind-plugins","title":"PreBind Plugins","text":"<p>Prepare for binding (e.g., attach volumes).</p> Plugin What It Does VolumeBinding Attach volumes to the node"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#bind-plugins","title":"Bind Plugins","text":"<p>Actually bind the pod to the node.</p> Plugin What It Does DefaultBinder Create the Binding object in API server"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#postbind-plugins","title":"PostBind Plugins","text":"<p>Informational hooks (cannot fail the scheduling).</p> Plugin What It Does (Usually none) Can be used for logging/metrics"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#plugin-behavior-summary","title":"Plugin Behavior Summary","text":"<p>Filtering vs Scoring:</p> Type Purpose Returns Effect Filter Eliminate nodes Pass/Fail Node removed from consideration if fails Score Rank nodes 0-100 score Higher score = more likely to be chosen <p>Example Flow for a Single Pod:</p> <pre><code>Pod arrives \u2192 QueueSort (place in queue by priority)\n           \u2193\n         PreFilter (all plugins prepare data)\n           \u2193\n         Filter on Node1: \u2705 Pass (has resources)\n         Filter on Node2: \u274c Fail (no CPU)\n         Filter on Node3: \u2705 Pass (has resources)\n           \u2193\n         Remaining nodes: Node1, Node3\n           \u2193\n         Score Node1: 75 (balanced allocation)\n         Score Node3: 90 (better image locality)\n           \u2193\n         Selected: Node3 (highest score)\n           \u2193\n         Reserve \u2192 Permit \u2192 PreBind \u2192 Bind \u2192 PostBind\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#visual-diagram-complete-scheduling-flow-with-all-extension-points","title":"Visual Diagram: Complete Scheduling Flow with All Extension Points","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    POD ENTERS SCHEDULING QUEUE                          \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 1. QUEUESORT EXTENSION POINT                                 \u2502      \u2502\n\u2502  \u2502    Plugin: PrioritySort                                      \u2502      \u2502\n\u2502  \u2502    Action: Order pods by priority (high \u2192 low)               \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      SCHEDULING CYCLE BEGINS                            \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 2. PREFILTER EXTENSION POINT                                 \u2502      \u2502\n\u2502  \u2502    Plugins:                                                  \u2502      \u2502\n\u2502  \u2502    \u2022 NodeResourcesFit      (calculate requests)              \u2502      \u2502\n\u2502  \u2502    \u2022 NodePorts            (check port requirements)          \u2502      \u2502\n\u2502  \u2502    \u2022 PodTopologySpread    (prepare topology data)            \u2502      \u2502\n\u2502  \u2502    \u2022 InterPodAffinity     (pre-calc affinity)                \u2502      \u2502\n\u2502  \u2502    \u2022 VolumeBinding        (check volume needs)               \u2502      \u2502\n\u2502  \u2502    Result: \u2705 Continue  OR  \u274c Reject Pod                     \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 3. FILTER EXTENSION POINT (For Each Node)                   \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Node1: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502      \u2502\n\u2502  \u2502           \u2502 NodeUnschedulable?    \u2705 Pass       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2502 NodeName?             \u2705 Pass       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2502 TaintToleration?      \u2705 Pass       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2502 NodeResourcesFit?     \u2705 Pass       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2502 VolumeBinding?        \u2705 Pass       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502      \u2502\n\u2502  \u2502           Result: \u2705 Node1 is FEASIBLE                       \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Node2: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502      \u2502\n\u2502  \u2502           \u2502 NodeResourcesFit?     \u274c FAIL       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502      \u2502\n\u2502  \u2502           Result: \u274c Node2 ELIMINATED                        \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Node3: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502      \u2502\n\u2502  \u2502           \u2502 TaintToleration?      \u274c FAIL       \u2502            \u2502      \u2502\n\u2502  \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502      \u2502\n\u2502  \u2502           Result: \u274c Node3 ELIMINATED                        \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Feasible Nodes: [Node1]                                  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502         \u2502  IF NO NODES PASS FILTER             \u2502                       \u2502\n\u2502         \u2502      \u2193                                \u2502                       \u2502\n\u2502         \u2502  4. POSTFILTER EXTENSION POINT        \u2502                       \u2502\n\u2502         \u2502     Plugin: DefaultPreemption         \u2502                       \u2502\n\u2502         \u2502     Action: Find pods to evict        \u2502                       \u2502\n\u2502         \u2502     Result: Retry scheduling          \u2502                       \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 5. PRESCORE EXTENSION POINT                                  \u2502      \u2502\n\u2502  \u2502    Plugins:                                                  \u2502      \u2502\n\u2502  \u2502    \u2022 InterPodAffinity     (prepare affinity data)            \u2502      \u2502\n\u2502  \u2502    \u2022 PodTopologySpread    (get current distribution)         \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 6. SCORE EXTENSION POINT (For Each Feasible Node)           \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Node1 Scoring:                                            \u2502      \u2502\n\u2502  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502      \u2502\n\u2502  \u2502    \u2502 NodeResourcesBalancedAllocation: 80 \u00d7 1 = 80 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 ImageLocality:                   70 \u00d7 1 = 70 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 NodeAffinity:                    60 \u00d7 1 = 60 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 PodTopologySpread:               85 \u00d7 2 = 170\u2502         \u2502      \u2502\n\u2502  \u2502    \u2502                                   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 TOTAL:                           380         \u2502         \u2502      \u2502\n\u2502  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502      \u2502\n\u2502  \u2502                                                              \u2502      \u2502\n\u2502  \u2502    Node4 Scoring:                                            \u2502      \u2502\n\u2502  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502      \u2502\n\u2502  \u2502    \u2502 NodeResourcesBalancedAllocation: 90 \u00d7 1 = 90 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 ImageLocality:                   50 \u00d7 1 = 50 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 NodeAffinity:                    80 \u00d7 1 = 80 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 PodTopologySpread:               75 \u00d7 2 = 150\u2502         \u2502      \u2502\n\u2502  \u2502    \u2502                                   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502         \u2502      \u2502\n\u2502  \u2502    \u2502 TOTAL:                           370         \u2502         \u2502      \u2502\n\u2502  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 7. NORMALIZESCORE EXTENSION POINT                            \u2502      \u2502\n\u2502  \u2502    Normalize scores to 0-100 range                           \u2502      \u2502\n\u2502  \u2502    Node1: 380/380 \u00d7 100 = 100                                 \u2502      \u2502\n\u2502  \u2502    Node4: 370/380 \u00d7 100 = 97                                  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502           WINNER: Node1 (Score: 100)                         \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 8. RESERVE EXTENSION POINT                                   \u2502      \u2502\n\u2502  \u2502    Plugin: VolumeBinding                                     \u2502      \u2502\n\u2502  \u2502    Action: Reserve PVCs for this pod                         \u2502      \u2502\n\u2502  \u2502    Result: \u2705 Resources Reserved                              \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 9. PERMIT EXTENSION POINT                                    \u2502      \u2502\n\u2502  \u2502    Plugins can:                                              \u2502      \u2502\n\u2502  \u2502    \u2022 Approve (continue)                                      \u2502      \u2502\n\u2502  \u2502    \u2022 Deny (reject)                                           \u2502      \u2502\n\u2502  \u2502    \u2022 Wait (external approval needed)                         \u2502      \u2502\n\u2502  \u2502    Result: \u2705 Approved                                        \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       BINDING CYCLE BEGINS                              \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 10. WAITONPERMIT EXTENSION POINT                             \u2502      \u2502\n\u2502  \u2502     Wait for all Permit plugins to approve                   \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 11. PREBIND EXTENSION POINT                                  \u2502      \u2502\n\u2502  \u2502     Plugin: VolumeBinding                                    \u2502      \u2502\n\u2502  \u2502     Action: Attach volumes to Node1                          \u2502      \u2502\n\u2502  \u2502     Result: \u2705 Volumes Attached                               \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 12. BIND EXTENSION POINT                                     \u2502      \u2502\n\u2502  \u2502     Plugin: DefaultBinder                                    \u2502      \u2502\n\u2502  \u2502     Action: Create Binding object                            \u2502      \u2502\n\u2502  \u2502             POST /api/v1/.../pods/nginx/binding              \u2502      \u2502\n\u2502  \u2502     Result: \u2705 Pod Bound to Node1                             \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 13. POSTBIND EXTENSION POINT                                 \u2502      \u2502\n\u2502  \u2502     Informational only (logging, metrics)                    \u2502      \u2502\n\u2502  \u2502     Cannot fail the binding                                  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                             \u2193                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502           \u2705 POD SUCCESSFULLY SCHEDULED TO NODE1              \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#decision-points-flowchart","title":"Decision Points Flowchart","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  What Happens at Each Extension Point?                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPreFilter (Pod Level):\n   \u251c\u2500 \u2705 Pass \u2192 Continue to Filter\n   \u2514\u2500 \u274c Fail \u2192 Pod Unschedulable (Error)\n\nFilter (Per Node):\n   \u251c\u2500 All nodes fail \u2192 PostFilter (Try Preemption)\n   \u251c\u2500 Some nodes pass \u2192 Continue to Score\n   \u2514\u2500 One node passes \u2192 Score it anyway\n\nReserve:\n   \u251c\u2500 \u2705 Success \u2192 Continue to Permit\n   \u2514\u2500 \u274c Fail \u2192 Unreserve, Retry scheduling\n\nPermit:\n   \u251c\u2500 \u2705 Approve \u2192 Continue to Bind Cycle\n   \u251c\u2500 \u23f8 Wait \u2192 Hold until external approval\n   \u2514\u2500 \u274c Deny \u2192 Unreserve, Retry scheduling\n\nPreBind:\n   \u251c\u2500 \u2705 Success \u2192 Continue to Bind\n   \u2514\u2500 \u274c Fail \u2192 Unreserve, Retry scheduling\n\nBind:\n   \u251c\u2500 \u2705 Success \u2192 PostBind \u2192 Done!\n   \u2514\u2500 \u274c Fail \u2192 Unreserve, Retry scheduling\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#5-built-in-scheduler-profiles","title":"5. Built-in Scheduler Profiles","text":"<p>Kubernetes 1.23+ comes with two default profiles:</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#profile-1-default-scheduler-balanced","title":"Profile 1: <code>default-scheduler</code> (Balanced)","text":"<p>Purpose: General-purpose scheduling for most workloads</p> <p>Enabled Plugins: - NodeResourcesFit (CPU/RAM check) - NodeResourcesBalancedAllocation (spread load evenly) - ImageLocality (prefer nodes with cached images) - TaintToleration - NodeAffinity - PodTopologySpread - InterPodAffinity</p> <p>Use for: 99% of your workloads</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#profile-2-bin-packing-scheduler-experimental","title":"Profile 2: <code>bin-packing-scheduler</code> (Experimental)","text":"<p>Purpose: Pack pods densely to minimize the number of nodes used</p> <p>Difference: Uses NodeResourcesMostAllocated instead of BalancedAllocation</p> <p>Effect: - Tries to fill up nodes completely before moving to next node - Good for cost optimization (fewer nodes) - Bad for high availability (all eggs in few baskets)</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#6-configuring-scheduler-profiles","title":"6. Configuring Scheduler Profiles","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#kubeschedulerconfiguration","title":"KubeSchedulerConfiguration","text":"<p>Profiles are configured via the <code>KubeSchedulerConfiguration</code> file.</p> <p>Example: Adding a custom profile</p> <pre><code># scheduler-config.yaml\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  # Profile 1: Default balanced scheduling\n  - schedulerName: default-scheduler\n    plugins:\n      score:\n        enabled:\n        - name: NodeResourcesBalancedAllocation\n          weight: 1\n        - name: ImageLocality\n          weight: 1\n\n  # Profile 2: GPU-optimized scheduling\n  - schedulerName: gpu-scheduler\n    plugins:\n      filter:\n        enabled:\n        - name: NodeResourcesFit\n        - name: TaintToleration\n      score:\n        enabled:\n        - name: NodeResourcesFit\n          weight: 10  # Strongly prefer nodes with resources\n        disabled:\n        - name: ImageLocality  # Don't care about image locality\n\n  # Profile 3: High-density bin packing\n  - schedulerName: bin-packing\n    plugins:\n      score:\n        enabled:\n        - name: NodeResourcesMostAllocated\n          weight: 1\n        disabled:\n        - name: NodeResourcesBalancedAllocation\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#applying-the-configuration","title":"Applying the Configuration","text":"<p>Step 1: Create the ConfigMap <pre><code>kubectl create configmap scheduler-config \\\n  --from-file=scheduler-config.yaml \\\n  -n kube-system\n</code></pre></p> <p>Step 2: Update the scheduler manifest</p> <p>Edit <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-scheduler\n    image: registry.k8s.io/kube-scheduler:v1.28.0\n    command:\n    - kube-scheduler\n    - --config=/etc/kubernetes/scheduler-config.yaml  # &lt;--- Add this\n    volumeMounts:\n    - name: scheduler-config\n      mountPath: /etc/kubernetes/scheduler-config.yaml\n      readOnly: true\n  volumes:\n  - name: scheduler-config\n    configMap:\n      name: scheduler-config\n</code></pre> <p>Step 3: Wait for scheduler to restart</p> <p>The kubelet will automatically restart the scheduler pod with the new config.</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#7-using-scheduler-profiles-in-pods","title":"7. Using Scheduler Profiles in Pods","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#select-a-profile","title":"Select a Profile","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-workload\nspec:\n  schedulerName: gpu-scheduler  # &lt;--- Same as profile name\n  containers:\n  - name: trainer\n    image: tensorflow/tensorflow:latest-gpu\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#default-profile","title":"Default Profile","text":"<p>If you don't specify <code>schedulerName</code>, it uses the first profile (usually <code>default-scheduler</code>):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  # schedulerName: default-scheduler  # Implicit\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#8-real-world-example-multi-tenant-cluster","title":"8. Real-World Example: Multi-Tenant Cluster","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#scenario","title":"Scenario","text":"<p>You have one cluster serving three teams: 1. ML Team: Needs GPU-aware scheduling 2. Web Team: Needs balanced scheduling 3. Batch Team: Wants dense packing to save costs</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#solution-three-profiles","title":"Solution: Three Profiles","text":"<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  # Default for Web Team\n  - schedulerName: default-scheduler\n    plugins:\n      score:\n        enabled:\n        - name: NodeResourcesBalancedAllocation\n\n  # GPU-optimized for ML Team\n  - schedulerName: ml-scheduler\n    plugins:\n      score:\n        enabled:\n        - name: NodeResourcesFit\n          weight: 10\n        - name: NodeAffinity\n          weight: 5\n\n  # Bin-packing for Batch Team\n  - schedulerName: batch-scheduler\n    plugins:\n      score:\n        enabled:\n        - name: NodeResourcesMostAllocated\n          weight: 1\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#team-usage","title":"Team Usage","text":"<p>ML Team: <pre><code>spec:\n  schedulerName: ml-scheduler\n  containers:\n  - name: training\n    image: pytorch/pytorch\n    resources:\n      limits:\n        nvidia.com/gpu: 2\n</code></pre></p> <p>Web Team: <pre><code>spec:\n  # Uses default-scheduler (implicit)\n  containers:\n  - name: web\n    image: nginx\n</code></pre></p> <p>Batch Team: <pre><code>spec:\n  schedulerName: batch-scheduler\n  containers:\n  - name: job\n    image: busybox\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#9-scheduler-profiles-vs-multiple-schedulers","title":"9. Scheduler Profiles vs Multiple Schedulers","text":"Aspect Scheduler Profiles Multiple Schedulers Processes 1 scheduler, N profiles N separate scheduler processes Memory ~120MB total ~100MB \u00d7 N Configuration One config file N config files RBAC One ServiceAccount N ServiceAccounts Cluster Cache Shared Separate caches Complexity Lower Higher Isolation Shared process Separate processes Use Case Different plugin configs Completely different scheduling logic"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#when-to-use-which","title":"When to Use Which?","text":"<p>Use Scheduler Profiles when: - \u2705 You need different plugin configurations - \u2705 You want resource efficiency - \u2705 You're running on Kubernetes 1.23+</p> <p>Use Multiple Schedulers when: - \u2705 You need completely custom scheduling algorithms - \u2705 You're using third-party schedulers (e.g., Volcano, Kube-batch) - \u2705 You need strong process isolation</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#10-checking-active-profiles","title":"10. Checking Active Profiles","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#view-scheduler-configuration","title":"View Scheduler Configuration","text":"<pre><code># Get the scheduler pod\nkubectl get pod -n kube-system -l component=kube-scheduler\n\n# Check logs for loaded profiles\nkubectl logs -n kube-system kube-scheduler-&lt;node-name&gt; | grep -i profile\n</code></pre> <p>Output: <pre><code>Registered scheduling profiles: default-scheduler, gpu-scheduler, batch-scheduler\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#check-which-profile-scheduled-a-pod","title":"Check Which Profile Scheduled a Pod","text":"<pre><code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.schedulerName}'\n</code></pre> <p>Custom columns view: <pre><code>kubectl get pods -A -o custom-columns=\\\n\"NAME:.metadata.name,\\\nNAMESPACE:.metadata.namespace,\\\nSCHEDULER:.spec.schedulerName,\\\nNODE:.spec.nodeName\"\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#11-common-pitfalls","title":"11. Common Pitfalls","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#issue-1-profile-name-typo","title":"Issue 1: Profile Name Typo","text":"<p>Problem: <pre><code>spec:\n  schedulerName: gpu-schedular  # Typo: \"schedular\" instead of \"scheduler\"\n</code></pre></p> <p>Result: Pod stays <code>Pending</code> forever</p> <p>Check: <pre><code>kubectl describe pod &lt;name&gt;\n# Events:\n#   Warning  FailedScheduling  No scheduler named \"gpu-schedular\" found\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#issue-2-scheduler-not-restarted-after-config-change","title":"Issue 2: Scheduler Not Restarted After Config Change","text":"<p>Problem: Updated <code>scheduler-config.yaml</code> but scheduler didn't reload</p> <p>Solution: <pre><code># For static pod, edit the manifest to force restart\n# Or delete the scheduler pod\nkubectl delete pod -n kube-system kube-scheduler-&lt;node-name&gt;\n</code></pre></p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#issue-3-conflicting-plugin-configs","title":"Issue 3: Conflicting Plugin Configs","text":"<p>Problem: <pre><code>plugins:\n  score:\n    enabled:\n    - name: NodeResourcesBalancedAllocation\n    - name: NodeResourcesMostAllocated  # Conflicts with above!\n</code></pre></p> <p>Solution: Use one OR the other, not both</p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#12-cka-exam-relevance","title":"12. CKA Exam Relevance","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#exam-tasks","title":"Exam Tasks","text":"<p>Task: \"Configure a scheduler with two profiles\"</p> <p>Not commonly tested directly, but you should know: 1. Profiles live in <code>KubeSchedulerConfiguration</code> 2. Pods select profiles via <code>schedulerName</code> 3. How to check which profile scheduled a pod</p> <p>More Common: \"Create a pod using a specific scheduler\"</p> <pre><code># Step 1: Generate pod\nkubectl run app --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Step 2: Edit to add schedulerName\nvim pod.yaml\n</code></pre> <p>Add: <pre><code>spec:\n  schedulerName: my-scheduler\n</code></pre></p> <pre><code># Step 3: Apply\nkubectl apply -f pod.yaml\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#13-quick-reference","title":"13. Quick Reference","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#pod-selection","title":"Pod Selection","text":"<pre><code>spec:\n  schedulerName: &lt;profile-name&gt;\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#check-profile","title":"Check Profile","text":"<pre><code># Which scheduler/profile was used?\nkubectl get pod &lt;name&gt; -o jsonpath='{.spec.schedulerName}'\n\n# View scheduler logs\nkubectl logs -n kube-system kube-scheduler-&lt;node&gt;\n</code></pre>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#common-profile-names","title":"Common Profile Names","text":"Profile Purpose <code>default-scheduler</code> Standard balanced scheduling <code>bin-packing</code> Dense node packing Custom names Your configured profiles"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#summary","title":"Summary","text":"<p>Key Takeaways</p> <p>\u2705 Scheduler Profiles = Multiple scheduling behaviors in ONE scheduler process \u2705 More efficient than running multiple separate schedulers \u2705 Configured via KubeSchedulerConfiguration \u2705 Pods select a profile using <code>spec.schedulerName</code> \u2705 Each profile can enable/disable different plugins \u2705 Built-in profiles: <code>default-scheduler</code> (balanced), <code>bin-packing</code> (dense) \u2705 Use profiles for different plugin configs, use multiple schedulers for custom algorithms \u2705 Profiles share the same cluster state cache (efficiency!)  </p>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#when-to-use-scheduler-profiles","title":"When to Use Scheduler Profiles","text":"<ul> <li>\u2705 Need different scoring/filtering logic</li> <li>\u2705 Want resource efficiency (one process)</li> <li>\u2705 Running Kubernetes 1.23+</li> <li>\u2705 Multi-tenant scenarios with different scheduling needs</li> <li>\u274c Don't use if you need completely custom scheduling logic (use multiple schedulers instead)</li> </ul>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#further-reading-references","title":"Further Reading &amp; References","text":""},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#official-kubernetes-documentation","title":"Official Kubernetes Documentation","text":"<ul> <li> <p>Advanced Scheduling in Kubernetes (Official Blog)   In-depth article from the Kubernetes team explaining advanced scheduling features and the scheduler framework.</p> </li> <li> <p>Scheduling Code Hierarchy Overview (GitHub)   Deep dive into the scheduler codebase structure - essential for understanding the internal implementation.</p> </li> </ul>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#community-resources","title":"Community Resources","text":"<ul> <li> <p>How Does the Kubernetes Scheduler Work? (Julia Evans)   Excellent visual explanation with clear diagrams showing how the scheduler makes decisions.</p> </li> <li> <p>How Does Kubernetes Scheduler Work? (Stack Overflow)   Detailed Q&amp;A covering common scheduling questions and edge cases.</p> </li> </ul>"},{"location":"fundamentals/pod-scheduling/scheduler-profiles/#recommended-reading-order","title":"Recommended Reading Order","text":"<p>For CKA exam preparation: 1. Start with Julia Evans' blog for a high-level visual understanding 2. Read the official Kubernetes blog for advanced concepts 3. Refer to Stack Overflow for specific troubleshooting scenarios 4. Deep dive into the GitHub code hierarchy only if you want to understand internals</p> <p>Exam Focus</p> <p>For the CKA exam, focus on practical usage (how to configure profiles, use schedulerName, debug scheduling issues) rather than deep implementation details.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/","title":"Taints and Tolerations","text":"<p>Taints and Tolerations work together to ensure that Pods are not scheduled onto inappropriate Nodes. While Node Affinity attracts Pods to a certain set of Nodes, Taints allow a Node to repel a set of Pods.</p> <p>[!CAUTION]</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#the-golden-rule","title":"\ud83d\udd11 The Golden Rule","text":"<ul> <li>Taints are set on Nodes.</li> <li>Tolerations are set on Pods.</li> </ul>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#the-real-world-analogy","title":"\ud83c\udfad The Real-World Analogy","text":"<p>Think of a Taint as a Lock on a door (the Node) and a Toleration as the Key carried by the Pod.</p> <ol> <li>The Taint (The Lock): You put a lock on a room that says \"Only authorized personnel\". Anyone without a key is kept out.</li> <li>The Toleration (The Key): A person (the Pod) has a special key. They can enter the locked room, but they don't have to. They can still choose to sleep in the common area (an untainted node).</li> </ol> <p>[!IMPORTANT] A Toleration allows a Pod to schedule on a Tainted node, but it does not force it there. To force a Pod to a specific node, you use Node Affinity.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#how-to-taint-a-node","title":"\ud83d\udee0\ufe0f How to Taint a Node","text":"<p>A Taint consists of a Key, a Value, and an Effect.</p> <pre><code>kubectl taint nodes node1 dedicated=finance:NoSchedule\n</code></pre>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#the-three-effects","title":"The Three Effects","text":"Effect Behavior NoSchedule New pods will not be scheduled unless they have a matching toleration. Existing pods remain. PreferNoSchedule The scheduler will try to avoid placing untolerated pods here, but it's not a hard requirement. NoExecute New pods are blocked, AND existing pods without a toleration are immediately evicted from the node."},{"location":"fundamentals/pod-scheduling/taints-tolerations/#adding-a-toleration-to-a-pod","title":"\ud83e\ude79 Adding a Toleration to a Pod","text":"<p>To allow a Pod to \"pass through\" the taint, you add a <code>tolerations</code> section to the Pod spec.</p> <pre><code>spec:\n  containers:\n  - name: nginx\n    image: nginx\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"finance\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#operators","title":"Operators","text":"<ul> <li>Equal: The key, value, and effect must match exactly.</li> <li>Exists: The key must exist (the value is ignored). Useful for matching any pod to a specific \"maintenance\" taint.</li> </ul>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#real-life-examples","title":"\ud83c\udf0d Real-Life Examples","text":""},{"location":"fundamentals/pod-scheduling/taints-tolerations/#1-the-control-plane-isolation","title":"1. The \"Control Plane\" Isolation","text":"<p>By default, Kubernetes taints the Control Plane nodes so that your application pods don't take up resources meant for the API server or ETCD.  - Taint: <code>node-role.kubernetes.io/control-plane:NoSchedule</code> - Toleration: Critical system pods (like CoreDNS or Kube-Proxy) have a toleration for this so they can run on the masters.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#2-dedicated-hardware-gpu-nodes","title":"2. Dedicated Hardware (GPU Nodes)","text":"<p>If you have expensive GPU nodes, you don't want a simple \"Hello World\" app taking up space there. - Action: Taint the GPU nodes with <code>hardware=gpu:NoSchedule</code>. - Result: Only your Machine Learning pods (with the matching toleration) will land there.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#3-maintenance-evacuation","title":"3. Maintenance / Evacuation","text":"<p>You need to perform hardware maintenance on <code>worker-3</code>. - Action: Apply a <code>NoExecute</code> taint. - Result: All pods currently running on <code>worker-3</code> are kicked off and rescheduled elsewhere, clearing the path for your maintenance.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#4-edge-computing-cloud-bursting","title":"4. Edge Computing / Cloud Bursting","text":"<p>You have some local nodes and some expensive nodes in the cloud. - Taint: <code>location=cloud:PreferNoSchedule</code> - Result: Kubernetes will fill up your local nodes first and only use the cloud nodes when the local ones are full.</p>"},{"location":"fundamentals/pod-scheduling/taints-tolerations/#useful-commands","title":"\u2328\ufe0f Useful Commands","text":"Goal Command Apply Taint <code>kubectl taint nodes node-name key=value:Effect</code> Remove Taint <code>kubectl taint nodes node-name key:Effect-</code> (Suffix with dash) Verify Taints <code>kubectl describe node node-name | grep Taints</code>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/","title":"Taints &amp; Tolerations vs. Node Affinity","text":"<p>In Kubernetes scheduling, these are the two main ways to control where Pods land. The easiest way to remember the difference is the \"Push vs. Pull\" concept.</p>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#1-core-logic-the-push-vs-pull","title":"1. Core Logic: The \"Push\" vs. \"Pull\"","text":""},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#taints-tolerations-the-push","title":"Taints &amp; Tolerations (The \"Push\")","text":"<ul> <li>Concept: Nodes use Taints to repel Pods.</li> <li>Analogy: A \"No Entry\" sign on a door. Only people with a specific key (Toleration) can enter.</li> <li>Primary Goal: To prevent certain Pods from landing on certain Nodes.</li> </ul>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#node-affinity-the-pull","title":"Node Affinity (The \"Pull\")","text":"<ul> <li>Concept: Pods use Affinity to be attracted to Nodes.</li> <li>Analogy: A \"Wanted\" poster. The Pod is looking for a node that matches its requirements.</li> <li>Primary Goal: To force or prefer certain Pods to land on specific Nodes.</li> </ul>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#2-practical-examples","title":"2. Practical Examples","text":""},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#scenario-a-dedicated-gpu-node","title":"Scenario A: Dedicated GPU Node","text":"<p>You have a node with a expensive GPU. You don't want \"regular\" web apps wasting its resources. *   Taint the Node: <code>gpu=true:NoSchedule</code> *   Effect: Every normal Pod in your cluster is now \"pushed\" away from this node. *   Add Toleration: Only your AI/ML Pods get the <code>gpu=true</code> toleration, so they are the only ones allowed in.</p>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#scenario-b-spreading-pods-across-zones","title":"Scenario B: Spreading Pods across Zones","text":"<p>You want your app to run in the <code>us-east-1</code> zone for low latency. *   Node Affinity: <code>requiredDuringScheduling...</code> with <code>zone in [us-east-1]</code>. *   Effect: The Pod \"pulls\" itself toward nodes in that zone.</p>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#3-pros-and-cons","title":"3. Pros and Cons","text":"Feature Taints &amp; Tolerations Node Affinity Logic Exclusive (Keep others out) Inclusive (Get me in) Strength Guaranteed Exclusion. Non-tolerating pods will NEVER land here. Flexible. Can be Hard (Required) or Soft (Preferred). Complexity Simple key/value/effect. Complex expressions (In, NotIn, Exists). Con It only allows a pod to enter; it doesn't force it to go there. It doesn't stop other pods from landing on your node if they have no preference."},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#4-when-to-use-both-the-dedicated-node","title":"4. When to use BOTH? (The \"Dedicated Node\")","text":"<p>If you want a node to be EXCLUSIVELY for a specific team (e.g., The \"Finance\" team), one tool is not enough.</p> <ol> <li>If you only use Taints: Other pods stay out (Good), but the Finance pods might accidentally land on a \"General\" node because they are allowed everywhere else (Bad).</li> <li>If you only use Affinity: Finance pods land on the Finance node (Good), but General pods might also land on the Finance node because they don't have any restrictions (Bad).</li> </ol> <p>The Solution: Use Both! *   Taint the Node: Keeps everyone else out. *   Toleration on Pod: Allows the Finance pod to enter. *   Affinity on Pod: Forces the Finance pod to choose that node specifically.</p>"},{"location":"fundamentals/pod-scheduling/taints-vs-affinity/#5-which-is-most-important","title":"5. Which is Most Important?","text":"<ul> <li>Node Affinity is the most important for application developers. It ensures high availability and correct resource placement.</li> <li>Taints &amp; Tolerations are most important for Cluster Administrators. It's used to \"reserve\" master nodes, handle hardware maintenance (drain), and manage specialized hardware.</li> </ul> <p>Summary for CKA: *   Use Taints to \"Lock\" a node. *   Use Affinity to \"Direct\" a pod. *   Use Both to \"Dedicate\" a node.</p>"},{"location":"security/auth-vs-authz-vs-tls/","title":"AuthN vs. AuthZ vs. TLS","text":"<p>Understanding the distinction between these three concepts is critical for the CKA exam. They form the \"Three-Layer Defense\" for any request entering the cluster.</p>"},{"location":"security/auth-vs-authz-vs-tls/#1-tls-the-secure-tunnel-foundation","title":"1. TLS: The Secure Tunnel (Foundation)","text":"<p>Before any identification happens, the connection itself must be secure. TLS ensures that the data is encrypted and that the parties are who they say they are.</p> <ul> <li>Primary Goal: Encryption &amp; Identity Verification (at the transport level).</li> <li>When it happens: At the moment of connection (Layer 4/7).</li> </ul>"},{"location":"security/auth-vs-authz-vs-tls/#methods-in-kubernetes","title":"Methods in Kubernetes:","text":"<ol> <li>Certificate Authority (CA): The root of trust. The cluster usually has its own internal CA.</li> <li>Server Certificates: Used by the API server so clients know they aren't talking to an imposter.</li> <li>Client Certificates: Used by components (like <code>admin</code>, <code>kube-proxy</code>, <code>kubelet</code>) to prove their identity to the API server.</li> <li>Mutual TLS (mTLS): Both the server and the client verify each other's certificates (standard for internal K8s communication).</li> </ol>"},{"location":"security/auth-vs-authz-vs-tls/#2-authentication-authn-who-are-you","title":"2. Authentication (AuthN): \"Who are you?\"","text":"<p>Once the secure tunnel is established, the API server needs to know your \"Identity\".</p> <ul> <li>Primary Goal: Identity Verification.</li> <li>Success: The user is logged in.</li> <li>Failure: <code>401 Unauthorized</code>.</li> </ul>"},{"location":"security/auth-vs-authz-vs-tls/#methods-in-kubernetes_1","title":"Methods in Kubernetes:","text":"Method Usage in CKA Description X.509 Client Certs High Files like <code>user.crt</code> and <code>user.key</code>. Common for admins. ServiceAccount Tokens High JWT tokens mounted into Pods for automated access. Static Token File Low A CSV file on the API server disk. (Requires restart to change). OIDC (Connect) Med Integrating with external providers like Google, GitHub, or Okta. Webhook Token Med Outsourcing authentication to an external service."},{"location":"security/auth-vs-authz-vs-tls/#3-authorization-authz-what-can-you-do","title":"3. Authorization (AuthZ): \"What can you do?\"","text":"<p>Now that we know who you are, do you have the permission to do what you're asking?</p> <ul> <li>Primary Goal: Permission Enforcement.</li> <li>Success: The action is performed.</li> <li>Failure: <code>403 Forbidden</code>.</li> </ul>"},{"location":"security/auth-vs-authz-vs-tls/#methods-in-kubernetes_2","title":"Methods in Kubernetes:","text":"<ol> <li>RBAC (Role-Based Access Control):<ul> <li>Roles / ClusterRoles: Define what can be done (verbs: get, list, watch, create, delete).</li> <li>RoleBindings / ClusterRoleBindings: Link the user to the Role.</li> <li>This is the main focus of the CKA exam.</li> </ul> </li> <li>Node Authorization: A specialized authorizer that limits Kubelets to only modifying their own Node and Pods on that Node.</li> <li>ABAC (Attribute-Based Access Control): Uses policies based on attributes. Hard to manage and requires API server restarts for changes.</li> <li>Webhook: Calls an external API to ask \"Is this allowed?\". Usually used by security tools (e.g., OPA Gatekeeper).</li> <li>AlwaysDeny / AlwaysAllow: Used for testing; usually not seen in production.</li> </ol>"},{"location":"security/auth-vs-authz-vs-tls/#at-a-glance-comparison","title":"At a Glance Comparison","text":"Feature TLS Authentication (AuthN) Authorization (AuthZ) Question Asked \"Is this connection safe?\" \"Who are you?\" \"Are you allowed to do this?\" Error Code SSL/TLS Handshake Error <code>401 Unauthorized</code> <code>403 Forbidden</code> Example Tool OpenSSL / CFSSL Certificates / Bearer Tokens RBAC (Roles/Bindings) Dependency Happens First Happens Second Happens Third <p>[!IMPORTANT] In the CKA exam, if you get a 403 Forbidden, your first thought should be RBAC. If you get an SSL error or 401, check your certificates and kubeconfig.</p>"},{"location":"security/overview/","title":"Security in Kubernetes (CKA)","text":"<p>The Security section of the CKA exam covers approximately 25% of the exam weightage. This section focuses on securing the cluster, managing access, and ensuring network security.</p>"},{"location":"security/overview/#core-topics","title":"Core Topics","text":""},{"location":"security/overview/#1-cluster-hardening","title":"1. Cluster Hardening","text":"<ul> <li>Restricting access to the API server</li> <li>Securing Kubelet</li> <li>Using Network Policies to isolate traffic</li> <li>Securing etcd</li> </ul>"},{"location":"security/overview/#2-identity-and-access-management-iam","title":"2. Identity and Access Management (IAM)","text":"<ul> <li>Role-Based Access Control (RBAC): Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings.</li> <li>Service Accounts: Managing non-human identities.</li> <li>Certificates: Managing TLS certificates for cluster components and users.</li> </ul>"},{"location":"security/overview/#3-application-security","title":"3. Application Security","text":"<ul> <li>Security Contexts: Defining privilege and access control for Pods and Containers.</li> <li>Secrets: Storing sensitive information like passwords, tokens, and keys.</li> <li>Admission Controllers: Using ImagePolicyWebhook or other pluggable controllers.</li> </ul>"},{"location":"security/overview/#4-network-security","title":"4. Network Security","text":"<ul> <li>Implementing Network Policies for Pod communication.</li> </ul> <p>[!TIP] Focus heavily on RBAC and Network Policies, as they are frequently tested in practical scenarios.</p>"},{"location":"security/security-primitives/","title":"Kubernetes Security Primitives","text":"<p>Kubernetes security is built on multiple layers of protection. Understanding how a request gets from a user (or service) to the API server and eventually to a resource is fundamental for the CKA exam.</p>"},{"location":"security/security-primitives/#1-the-request-lifecycle-the-three-pillars","title":"1. The Request Lifecycle (The Three Pillars)","text":"<p>Every request to the Kubernetes API server goes through three major stages:</p>"},{"location":"security/security-primitives/#a-authentication-who-are-you","title":"A. Authentication (Who are you?)","text":"<p>Kubernetes does not have a built-in \"User\" database. It relies on internal ServiceAccounts or external methods for human users: - Files/Tokens: Static Token File, Bootstrap Tokens. - Certificates: Client Certificates (X.509). - External Providers: LDAP, OIDC (Google, GitHub), Keystone.</p> <p>[!NOTE] If a request fails authentication, it returns a 401 Unauthorized error.</p>"},{"location":"security/security-primitives/#b-authorization-what-can-you-do","title":"B. Authorization (What can you do?)","text":"<p>Once identified, Kubernetes checks if the user has permission to perform the specific action (verb) on the resource. - RBAC (Role-Based Access Control): The most common method (used in CKA). - ABAC (Attribute-Based Access Control): Policy-based, less common. - Node Authorization: Specifically for Kubelets. - Webhook: External authorization service.</p> <p>[!NOTE] If a request fails authorization, it returns a 403 Forbidden error.</p>"},{"location":"security/security-primitives/#c-admission-control-policy-enforcement","title":"C. Admission Control (Policy Enforcement)","text":"<p>Even if you are authorized, an Admission Controller can still block or modify your request based on cluster-wide policies. - Mutating Admission Controllers: Modify the request (e.g., inject a sidecar). - Validating Admission Controllers: Check the request against rules (e.g., check if the image comes from a trusted registry).</p>"},{"location":"security/security-primitives/#2-communication-security-tls","title":"2. Communication Security (TLS)","text":"<p>By default, all communication between Kubernetes components is encrypted using TLS. - Control Plane components (API Server, Scheduler, Controller Manager) use TLS to talk to each other. - Kubelet uses TLS to talk to the API Server. - etcd should be secured with its own set of certificates.</p>"},{"location":"security/security-primitives/#3-pod-security-primitives","title":"3. Pod Security Primitives","text":"<p>Beyond the API server, security applies to the workloads themselves: - Security Contexts: Define user IDs, group IDs, and capabilities at the Pod or Container level. - Secrets: Store sensitive data (though they are only Base64 encoded by default\u2014not encrypted at rest unless configured). - Network Policies: Control the flow of traffic between Pods (Layer 3/4 Firewall).</p>"},{"location":"security/security-primitives/#4-host-level-security","title":"4. Host Level Security","text":"<ul> <li>Node Hardening: Disabling unused services.</li> <li>SSH Access: Restricting access to the underlying nodes.</li> <li>Kubelet Security: Disabling anonymous access and ensuring only authorized API requests are processed.</li> </ul>"},{"location":"setup-guides/kops-aws/","title":"Kubernetes on AWS with kOps","text":"<p>kOps (Kubernetes Operations) is an open-source tool used to create, destroy, upgrade, and maintain production-grade, highly available Kubernetes clusters on AWS and other cloud providers.</p> <p>Often referred to as \"The kubectl for clusters,\" kOps manages the entire cluster lifecycle, from infrastructure provisioning to Kubernetes software installation.</p>"},{"location":"setup-guides/kops-aws/#key-features","title":"Key Features","text":"<ul> <li>Full Automation: Provisions all AWS resources (EC2, VPC, ASG, IAM, ELB).</li> <li>High Availability: Supports multi-master setups across multiple Availability Zones.</li> <li>State-Sync Model: Uses an S3 bucket to store the cluster configuration/state.</li> <li>Dry-runs: Preview infrastructure changes before applying them (<code>kops update cluster</code>).</li> <li>Terraform Integration: Can output Terraform manifests for Infrastructure-as-Code (IaC) workflows.</li> </ul>"},{"location":"setup-guides/kops-aws/#step-by-step-aws-setup","title":"Step-by-Step AWS Setup","text":""},{"location":"setup-guides/kops-aws/#1-prerequisites-iam-s3","title":"1. Prerequisites (IAM &amp; S3)","text":"<p>kOps requires an IAM user with appropriate permissions and an S3 bucket to store the cluster state.</p> <pre><code># 1. Create S3 Bucket for state storage\naws s3 mb s3://clusters.example.com --region us-east-1\n\n# 2. Versioning is highly recommended\naws s3api put-bucket-versioning --bucket clusters.example.com --versioning-configuration Status=Enabled\n\n# 3. Export the state store environment variable\nexport KOPS_STATE_STORE=s3://clusters.example.com\n</code></pre>"},{"location":"setup-guides/kops-aws/#2-dns-configuration","title":"2. DNS Configuration","text":"<p>kOps uses DNS for cluster discovery. You can use: - Public/Private Route53 Hosted Zone: Recommended for production. - Gossip DNS: Uses <code>.k8s.local</code> suffix for simpler setups without a real domain.</p>"},{"location":"setup-guides/kops-aws/#3-create-cluster-configuration","title":"3. Create Cluster Configuration","text":"<p>This command generates the cluster specification but doesn't create the resources yet.</p> <pre><code>kops create cluster \\\n    --name=mycluster.k8s.local \\\n    --zones=us-east-1a,us-east-1b \\\n    --node-count=2 \\\n    --node-size=t3.medium \\\n    --master-size=t3.medium \\\n    --dns gossip\n</code></pre>"},{"location":"setup-guides/kops-aws/#4-build-the-cluster","title":"4. Build the Cluster","text":"<p>Review the plan and then apply it to provision AWS resources.</p> <pre><code># Preview the changes\nkops update cluster --name mycluster.k8s.local\n\n# Apply and build\nkops update cluster --name mycluster.k8s.local --yes --admin\n</code></pre> <p>It usually takes 5-10 minutes for the cluster to become ready.</p>"},{"location":"setup-guides/kops-aws/#lifecycle-management-commands","title":"Lifecycle Management Commands","text":"Action Command Validate Cluster <code>kops validate cluster</code> List Clusters <code>kops get clusters</code> Edit Configuration <code>kops edit cluster &lt;name&gt;</code> Rolling Update <code>kops rolling-update cluster --yes</code> Delete Cluster <code>kops delete cluster --name &lt;name&gt; --yes</code>"},{"location":"setup-guides/kops-aws/#kops-vs-eks-vs-kubeadm","title":"kOps vs. EKS vs. Kubeadm","text":"Feature kOps AWS EKS Kubeadm Management Self-managed AWS-managed Control Plane Self-managed Infrastructure Automated (AWS) Automated (AWS) Manual / Any Control Plane You manage Nodes AWS manages Nodes You manage Nodes Customization Extremely High Moderate High Best For Custom production Cloud-native production Learning / On-prem"},{"location":"setup-guides/kubeadm-mac-multipass/","title":"CKA Lab Setup: Kubeadm on macOS (via Multipass)","text":"<p>Minikube is great for deploying apps, but it cannot be used to practice cluster bootstrapping or upgrades (<code>kubeadm</code>). </p> <p>This guide allows you to run a full Linux VM on your Mac to practice the \"Cluster Installation\" portion of the CKA exam.</p>"},{"location":"setup-guides/kubeadm-mac-multipass/#1-what-is-multipass","title":"1. What is Multipass?","text":"<p>Multipass is an official tool from Canonical (the makers of Ubuntu) designed to create Ubuntu virtual machines instantly.</p> <p>Think of it as \"Docker for Virtual Machines.\" *   Docker gives you a container (processes sharing a kernel). *   Multipass gives you a full VM (its own kernel, systemd, and IP stack).</p> <p>Why use it for CKA? 1.  Native Hypervisor: It uses <code>HyperKit</code> (on Intel Macs) or <code>Virtualization.framework</code> (on Apple Silicon M1/M2/M3), making it extremely fast compared to VirtualBox. 2.  Clean Slate: You get a fresh \"server\" in seconds. If you break the cluster (common when learning <code>kubeadm</code>), you just delete the VM and start a new one. 3.  Real Linux: <code>kubeadm</code> only runs on Linux. Multipass gives you that Linux environment on your Mac transparently.</p>"},{"location":"setup-guides/kubeadm-mac-multipass/#2-install-multipass","title":"2. Install Multipass","text":"<p>Multipass behaves like a lightweight cloud on your local machine. It creates Ubuntu VMs instantly.</p> <pre><code>brew install --cask multipass\n</code></pre>"},{"location":"setup-guides/kubeadm-mac-multipass/#2-launch-a-master-node","title":"2. Launch a Master Node","text":"<p>Create a VM named <code>k8s-master</code> with enough resources (2 CPUs, 2GB RAM).</p> <pre><code>multipass launch --name k8s-master --cpus 2 --memory 2G --disk 10G\n</code></pre> <p>Log into the VM: <pre><code>multipass shell k8s-master\n</code></pre></p>"},{"location":"setup-guides/kubeadm-mac-multipass/#what-to-expect-inside","title":"What to Expect Inside","text":"<p>When you first log in, the directory (<code>/home/ubuntu</code>) will be empty. <pre><code>ubuntu@k8s-master:~$ ls -ltr\ntotal 0\n</code></pre> This is normal! You have launched a minimal \"Cloud Image\" (similar to an AWS EC2 instance). It is a clean slate without any pre-installed tools or GUI folders.</p> <p>To see the Linux system files, check the root: <pre><code>ls -F /\n# Output: bin/ etc/ var/ usr/ ...\n</code></pre></p> <p>(All subsequent commands are run INSIDE this shell)</p>"},{"location":"setup-guides/kubeadm-mac-multipass/#3-the-standard-installation-cka-workflow","title":"3. The \"Standard\" Installation (CKA Workflow)","text":"<p>In the exam, you must install a Container Runtime and the Kubernetes tools.</p>"},{"location":"setup-guides/kubeadm-mac-multipass/#step-a-configure-system-prerequisites","title":"Step A: Configure System Prerequisites","text":"<p>Disabling swap and loading kernel modules is mandatory.</p> <pre><code># 1. Forward IPv4 and let iptables see bridged traffic\ncat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# 2. Sysctl params required by setup\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\nsudo sysctl --system\n</code></pre>"},{"location":"setup-guides/kubeadm-mac-multipass/#step-b-install-container-runtime-containerd","title":"Step B: Install Container Runtime (Containerd)","text":"<pre><code># Update and install containerd\nsudo apt-get update\nsudo apt-get install -y containerd\n\n# Create default config\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# IMPORTANT for CKA: Set SystemdCgroup = true\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml\n\n# Restart containerd\nsudo systemctl restart containerd\n</code></pre>"},{"location":"setup-guides/kubeadm-mac-multipass/#step-c-install-kubeadm-kubelet-kubectl","title":"Step C: Install Kubeadm, Kubelet, Kubectl","text":"<pre><code># 1. Install packages needed to use the Kubernetes apt repository\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n\n# 2. Download the public signing key for the Kubernetes packet repositories\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# 3. Add the K8s apt repository\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# 4. Install the tools\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre>"},{"location":"setup-guides/kubeadm-mac-multipass/#4-initialize-the-cluster-bootstrapping","title":"4. Initialize the Cluster (Bootstrapping)","text":"<p>Now you are ready to run the command you tried earlier.</p> <pre><code># Using a specific pod network CIDR is usually required for CNI plugins\nsudo kubeadm init --pod-network-cidr=192.168.0.0/16\n</code></pre> <p>Once this finishes, Read the Output! It tells you exactly what to do next:</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"setup-guides/kubeadm-mac-multipass/#5-install-network-plugin-cni","title":"5. Install Network Plugin (CNI)","text":"<p>Nodes will remain <code>NotReady</code> until you install networking. We will use Calico.</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/custom-resources.yaml\n</code></pre> <p>Check your work: <pre><code>kubectl get nodes\n</code></pre></p>"},{"location":"setup-guides/kubeadm-mac-multipass/#6-cleanup-when-finished","title":"6. Cleanup (When finished)","text":"<p>To delete the VM from your Mac: <pre><code># On your Mac terminal\nmultipass delete k8s-master\nmultipass purge\n</code></pre></p>"},{"location":"setup-guides/kubeadm-operations/join-command/","title":"Kubeadm Join Command","text":"<p>This document contains the critical join command generated after the cluster was initialized on the control plane. This is needed to add worker nodes to the cluster.</p>"},{"location":"setup-guides/kubeadm-operations/join-command/#the-join-command","title":"\ud83d\ude80 The Join Command","text":"<p>Control Plane IP: <code>192.168.1.187</code></p> <p>Run this command on your worker nodes (node01, node02) with <code>sudo</code> to join them to the cluster:</p> <pre><code>sudo kubeadm join 192.168.1.187:6443 --token nzo4d6.0tt0bv01najwjkov \\\n    --discovery-token-ca-cert-hash sha256:415d9742be98a0b5f00a94d665578a480d90efb69f5858915282617c4b78d733 \n</code></pre>"},{"location":"setup-guides/kubeadm-operations/join-command/#important-notes-for-cka","title":"\ud83d\udca1 Important Notes for CKA","text":""},{"location":"setup-guides/kubeadm-operations/join-command/#1-token-lifespan","title":"1. Token Lifespan","text":"<p>By default, this token is only valid for 24 hours. If you try to join a node tomorrow and it fails, you'll need to generate a new token.</p>"},{"location":"setup-guides/kubeadm-operations/join-command/#2-how-to-recover-a-lost-join-command","title":"2. How to recover a lost join command","text":"<p>If you did not note down the join command on the controlplane node after running <code>kubeadm init</code>, you can recover it by running the following on controlplane:</p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>This command generates a new token and prints the full <code>kubeadm join</code> string for you.</p>"},{"location":"setup-guides/kubeadm-operations/join-command/#3-verification","title":"3. Verification","text":"<p>After running the join command on the workers, verify they have joined by running this on the control plane: <pre><code>kubectl get nodes\n</code></pre></p>"},{"location":"setup-guides/kubeadm-operations/join-command/#4-what-if-i-lose-the-ca-hash","title":"4. What if I lose the CA Hash?","text":"<p>If you have a token but lost the hash, you can find it with this command: <pre><code>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \\\n   openssl dgst -sha256 -hex | sed 's/^.* //'\n</code></pre></p>"},{"location":"setup-guides/kubeadm-operations/join-command/#troubleshooting-join-failures","title":"Troubleshooting \"Join\" Failures","text":"<ul> <li>Connectivity: Ensure the worker node can ping <code>192.168.1.187</code>.</li> <li>Firewall: Port <code>6443</code> must be open on the control plane.</li> <li>Container Runtime: <code>containerd</code> must be running on the worker node before joining.</li> <li>Swap: Swap must be disabled on the worker node (<code>sudo swapoff -a</code>).</li> </ul>"}]}